I"j#<p>[CS231n] Lecture-4-Introduction-to-Neural-Networks<br />
<a href="https://www.youtube.com/watch?v=d14TUNcbn1k&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=4">4강 Video 바로가기</a></p>

<ul>
  <li>Contents
    <ul>
      <li>Backpropagation</li>
      <li>Multi-layer Perceptrons</li>
      <li>The neural viewpoint</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="review">Review</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/125886711-42556497-bc4f-4a31-80bf-7c1c4126071a.png" alt="image" /></p>

<p>지난 시간에 우리는 Loss function으로 <em>‘우리가 만든 스코어가 얼마나 잘 만들어졌는지’</em> 정량화하는 것에 대해 배웠다. 우리는 간단한 모델을 선호하기 때문에 더 나은 모델의 일반화를 위해 Regularization을 추가한다.</p>

<p>그리고 우리는 이 Loss를 낮추기 위해 가장 가파른 내리막 방향으로 내려가는데 이것을 gradient descent method라고 한다. w의 gradient를 찾아서 Loss 풍경의 가장 낮은 지점으로 단계적으로 내려간다.</p>

<p>또한 이 gradient를 계산하는 방법에 대해 배웠다. 오늘은 Computational Graphs를 사용해서 Loss fucntion의 gradient를 계산하는 방법에 대해 배울 것이다.</p>

<hr />

<h2 id="backpropagation">Backpropagation</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/125887845-74520c0b-5298-4a07-8482-d3f2c4ea3c0b.png" alt="image" /></p>

<p>이 Computational Graphs는 반복적으로 chain rule을 사용해서 gradient를 구하는 Backpropagation 방법에 유용한데, 특히 아주 복잡한 함수에 적용하는데에 매우 유용하다. 예를들어 아주 많은 레이어를 거치는 Convolutional Network에 적용하는데에 유용하다.</p>

<p>chain rule을 사용해서 gradient를 구하는데 기본적으로 <strong>[local gradient] X [upstream gradient]</strong> 의 방법으로 gradient를 구한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/126234709-170d0ec8-bcce-4c40-a24e-722e06c64b00.png" alt="image" /></p>

<p>가장 마지막 graph node부터 구하는데, gradient를 구하는 예시를 들겠다. 마지막 gradient는 당연히 1이다.
$$
f(x)=\frac{1}{x}
$$
일때
$$
\frac{df(x)}{dx} = - \frac{1}{x^2}
$$
이므로 
$$
\frac{df(x)}{dx} \frac{dx}{dx} = [LocalGradient][UpstreamGradient] <br />
= (\frac{-1}{1.37^2})(1)=-0.53
$$
 이다.</p>

<p>그 다음 node의 gradient를 구하면
$$
g(x)=x+1
$$</p>

<p>$$
\frac {dg(x)}{dx} = 1
$$</p>

<p>이므로 chain rule을 이용하면
$$
\frac {df(g(x))}{dx} = \frac{dg(x)}{dx} \frac {df(x)}{dg(x)} <br />
= [LocalGradient][UpstreamGradient] <br />
= (1)(-0.53) <br />
= -0.53
$$
이다.</p>

<p>이러한 방법으로 가장 끝에 있는 node(w)에 관한 gradient를 구할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/126234856-6bd41b3e-7f1d-46d3-80f6-5eb3d90ec27c.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/126234888-588b05e2-c482-409e-86f8-e21c83275572.png" alt="image" /></p>

<p>하나하나 다 쪼개서 가장 간단하게 그래프를 나타내고 gradient를 구할 수도 있지만 표현할 수 있다면 묶어서 하나의 node로 나타내도 된다. 예를들어 sigmoid 함수는 특별히 아래처럼 미분이 되기 때문에 한번에 <strong>sigmoid gate</strong>로 표현해서 구할 수 있다.</p>

<p>$$
\frac {d \sigma(x)} {dx} = (1 - \sigma(x)) \sigma(x)
$$
이를 이용해서</p>

<p>$$
\sigma(x)=0.73
$$</p>

<p>의 값을 넣고 구하면
$$
(1-0.73) * (0.73) = 0.2
$$
으로 gradient를 구할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/126235688-b5f2b82b-e4d8-4209-9694-842c901dcffb.png" alt="image" /></p>

<p>덧셈, 최대값, 곱셈 연산이 가지는 패턴은 위와 같다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/126236356-8e61730a-29b8-40f5-861b-46b18eafabe5.png" alt="image" /></p>

<p>$$
\frac {df} {dx} = \sum_{i} \frac {df} {dq_{i}} \frac {dq_{i}} {dx}
$$
특히 덧셈 연산은 x의 각각의 local gradient를 더한 값으로 표현할 수 있다.  이렇게 함수에서 각 변수에 대해서 gradient를 계산하는 방법인 backpropagation에 대해 살펴보았다.</p>

<h2 id="backpropagation---vectorized-gradients">Backpropagation - vectorized gradients</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/126237011-537e0ac0-3b4f-46e4-91e0-69c23cc6e1db.png" alt="image" /></p>

<p>이번에는 vector의 gradient를 살펴볼 것이다. 이때의 유일한 차이점은 <em>gradient가 Jacobian matrix</em>라는 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127001319-580a9843-6dde-4b5d-b766-9aad241e1646.png" alt="image" />
먼저 computational graph를 그리고 시작한다</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127002183-ab73fff6-6738-4e2e-81f1-4c8050863b0c.png" alt="image" /></p>

<p>먼저 마지막 gradient는 당연히 1이다.
그리고 
$$
f(q)=q_1^2+q_2^2
$$
이므로
$$
\frac{df}{dq_{i}} = 2q_{i}
$$
의 벡터이다. 이를 이용해서 W의 gradient를 구하면</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127002874-ffbcef65-862d-49fb-96e0-adc6810249d9.png" alt="image" />
$$
\frac{dq}{dw}=x <br />
\frac{df}{dw}=\frac{df}{dq}<em>\frac{dq}{dw}=2q_{i}</em>x_{j}
$$
<img src="https://user-images.githubusercontent.com/62828866/127091519-5ab80400-f676-47af-815e-c3ef0df0ddbd.png" alt="image" /></p>

<p>예를들어 많은 딥 러닝 프레임워크를 보면 그 안에 backpropagation을 구현해놨다. Caffe의 코드를 보면 레이어가 computational node로 되어있고 안에는 하나씩 forward backward 연산을 해서 쌓는것이다. 시그모이드 레이어를 보면 gradient를 구하는 부분이 위처럼 생겼다</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127091636-7a2136c1-474c-47bd-a195-96cfe9cd3e80.png" alt="image" /></p>

<p>정리하면 위와 같다. 그 중 핵심은 backpropagation을 적용한 forward, backwar API를 사용해서 레이어의 gradient를 구한다!</p>

<hr />

<h2 id="multi-layer-perceptrons">Multi-layer Perceptrons</h2>

<p>신경망은 사실 오래된 기술이다. 1980~1990년대에 연구되었다. 그런데 현대에 여러가지 문제를 해결하기 위해 쓰이는 CNN, LSTM 등의 기법들을 위해하기 위해서는 이 신경망을 먼저 이해해야한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127185959-1f04b066-2414-44a6-8767-e839e41518be.png" alt="image" /></p>

<p>1번째 linear function에 또 linear function을 쌓아서 non linear하게 만든다. 이러한 계층적 다중 레이어 아키텍쳐의 기본이다. DNN(Deep Neural Network)의 뜻은 이러한 Neural network를 깊게 쌓았다는 원초적 의미이다.</p>

<hr />

<h2 id="the-neural-viewpoint">The neural viewpoint</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/127186697-e794c9f0-de13-404b-a28b-51c722ae85c9.png" alt="image" /></p>

<p>인체의 뉴런과 비슷하다. x0이라는 데이터가 들어와서 가중치w0과 시냅스 반응이 나고 이것을 cell body에서 선형 합연산을해서 activation function에 의해 비선형으로 내보낸다. 그리고 이를 다음 뉴런으로 전달해준다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187235-91d68d5b-6856-4ca9-b544-46f17e034da3.png" alt="image" /></p>

<p>그러나 인공신경망이 인체의 뉴런과 아예 동일하다고 볼 수 없으므로 주의해야한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187331-95ea3eca-60b7-46e8-bb27-bc6af229045a.png" alt="image" /></p>

<p>activation function이 여러개 존재한다. 전통적으로 0~1사이로 리턴해주는 sigmoid가 가장 많이 사용됐으며, 2012년에 등장한 <strong>ReLU</strong>가 최근에 가장 많이 쓰인다. Leaky ReLU나 ELU등 좀 더 수정된 function도 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187516-b977a277-7783-476a-9de8-c07cb57de9ee.png" alt="image" /></p>

<p>fully-connected layer는 벡터로 표현이 가능하고 행렬의 곱인 W*x로 계산이 효율적이다. 따라서 fully-connected layer로 많이 쓰고 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187718-98fb296c-be91-41b6-b8d4-83a33fdd46da.png" alt="image" /></p>

<p>또한 fully-connected layer로 쓰면 hidden layer 하나를 한줄로 간단하게 f() function을 통해 표현이 가능하다는 장점이 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187823-1851e3c7-a274-4e91-a172-481d944e93a4.png" alt="image" /></p>

<p>여기까지가 Neural Network에 대한 기본 개괄이었다. 다음 시간에는 <strong>드디어</strong> Convolutional Neural Network에 대해서 배울 것이다.</p>

:ET