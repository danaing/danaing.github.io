I"2<p>이번 글은 <strong>차원 축소</strong>에 대해 교내 학회에서 강의했던 내용을 바탕으로 올립니다. 이 글의 수식은 연세대학교 응용통계학과 전용호 교수님의 ‘회귀분석’ 강의안의 수식을 참고하여 제 식으로 설명하기 쉽게 정리했음을 먼저 밝힙니다. 강의한 내용은 <a href="/assets/images/pca/Dimension-Reduction.pdf" target="_blank">PDF</a>를 참고하시면 될 것 같습니다. 오늘은 차원 축소의 대표적인 통계 기법인 <strong>PCA</strong>에 대해 알아보겠습니다.</p>

<h2 id="1-introduction">1. Introduction</h2>

<h3 id="11-what-is-dimension-in-data">1.1. What is Dimension in data?</h3>

<p><img src="/assets/images/pca/pca-01.png" alt="data in table" style="zoom:50%;" /></p>

<p>들어가기에 앞서서 데이터에서 차원이란 무엇일까요? 특히 테이블로 정형화된 데이터에서 생각해봅시다. 위와 같이 키, 몸무게, 머리 길이라는 변수가 있다면 간단히, 차원이란 <strong>변수의 개수</strong>라고 생각할 수 있습니다. 그러나 현실에서는 항상 변수들이 독립적(orthogonal)이지 않습니다. 예를들어 키와 몸무게는 강한 상관관계가 있다고 할 수 있죠.</p>

<h3 id="12-the-curse-of-dimensionality">1.2. The Curse of Dimensionality</h3>

<p><img src="/assets/images/pca/pca-02.png" alt="" style="zoom:40%;" /></p>

<p>Machine Learning에서 feature가 하나 늘어나면 차원이 하나 증가합니다. 특히 Machine Learning에서 <strong>차원의 저주</strong>는 차원이 커질수록 역설적으로 성능 저하를 가져오는 것을 의미합니다. 왼쪽 그림을 보면 축의 0.5 거리에서 1차원에서는 데이터가 모여있지만 차원이 늘어날수록 점점 퍼지게 됩니다. 이렇듯 표본인 학습 데이터는 제한적임에 비해 <strong>차원이 계속 늘어난다면 학습 데이터가 그 공간을 충분히 설명하기 어렵습니다.</strong></p>

<p>오른쪽 그림을 보면, 같은 공간을 설명하기 위해 필요한 차원의 수와 거리가 나타나있습니다. 1차원에서 10%의 공간을 채우기 위해서는 10%의 데이터가 필요합니다($0.1=0.1$). 그러나 10차원에서 10%의 공간을 설명하기 위해서는 각각의 축에 대해 80%의 데이터가 필요합니다($(0.8)^{10} = 0.1$). <strong>즉, 공간은 기하급수적으로 증가하는 것에 비해 한정된 학습 데이터로 공간을 설명하기 때문에 과적합(overfitting)이 일어나고, 모델의 성능이 저하되는 것입니다.</strong> 이 뿐만 아니라 학습 속도도 상당히 느려집니다.</p>

<h3 id="13-dimension-reduction">1.3. Dimension Reduction</h3>

<p>따라서 차원의 저주 문제를 해결하기 위해 <strong>차원 축소</strong>가 필요합니다. 예를들어 MNIST 데이터셋이 있다고 할 때 이미지 한 장은 28*28=784차원의 데이터입니다. 이 데이터를 그대로 사용한다면 784차원을 모두 학습해야겠지만, 그렇다면 위에서 말한 문제가 발생할 수 있습니다.</p>

<p><img src="/assets/images/pca/pca-03.png" alt="img" style="zoom:50%;" /></p>

<p>따라서 모든 feature를 사용하는 것이 아니라 비슷한 픽셀 덩어리를 하나로 묶어서 보거나, 중요한 픽셀만 뽑을 수 있도록 하는 것이 차원 축소입니다. 즉, 고차원 데이터를 통째로 다 쓰는 것이 아니라 <strong>데이터를 압축하고 요약하여 큰 특징들만 사용하는 것입니다.</strong> 예를들어 획이 길쭉하거나, 획이 동그랗다는 2개의 큰 특징을 뽑을 수도 있습니다. 이는 Convolution layer를 사용하여 저차원의 feature map으로 주요 특징을 추출하는 것 또한 이미지의 차원 축소의 예라고 할 수 있습니다.</p>

<h2 id="2-principal-component-analysispca">2. Principal Component Analysis(PCA)</h2>

<h3 id="21-basic-idea-of-pca">2.1. Basic Idea of PCA</h3>

<p><img src="/assets/images/pca/pca-04.png" alt="img" style="zoom:20%;" /></p>

<p>위의 그림을 운동장에 놀고 있는 아이들이라고 합시다. 만약 선생님이 최대한 한눈에 아이들을 보려면 어디서 봐야할까요? 왼쪽에서 보면 오른쪽에 있는 아이들이 가려져서 잘 보이지 않을 것입니다. 그러나 아래쪽에서 본다면 아이들이 최소한으로 가려지기 때문에 최대한 많이 볼 수 있습니다.</p>

<p><img src="/assets/images/pca/pca-05.png" alt="img" style="zoom:20%;" /></p>

<p>가로로 멀리 분산되어 있기 때문에 가장 많이 볼 수 있는것이죠! 즉, <strong>퍼짐의 정도는 정보량에 비례합니다.</strong> 많은 정보를 담고 있는 소수의 새로운 축으로 데이터를 설명하는 것이 주성분분석(PCA)의 기본적 아이디어입니다.</p>

<h3 id="22-what-is-pca">2.2. What is PCA?</h3>

<p><img src="/assets/images/pca/pca-06.png" alt="img" style="zoom:90%;" /></p>

<p>PCA는 <strong>최대 분산을 가지도록 기존 변수의 선형 결합을 찾습니다.</strong> 위처럼 Feature1과 Feature2를 조합하여 새로운 하늘색 축인 주성분(ex. PC = 0.5*Feature1 + 0.5*Feature2)으로 표현하는 것이죠.</p>

<p><strong>아이디어</strong></p>

<ul>
  <li>몇 개의 작은 주성분이 원본 데이터에서 사용할 수 있는 거의 모든 정보를 포함할 수 있습니다.</li>
</ul>

<p><strong>목표</strong></p>

<ul>
  <li>데이터의 차원을 줄이고 실제 데이터의 차원을 발견합니다.</li>
  <li>의미 있는 새로운 변수를 발견합니다.</li>
</ul>

<h3 id="23-linear-combination">2.3. Linear Combination</h3>

<p>위에서 말한 <strong>선형 결합</strong>에 대해서 잠깐 더 살펴보겠습니다. 변수가 $p$개, 관측치가 $n$개 의 변수가 있는 원 데이터 $\mathbf{X}$(p x n) 행렬이 평균을 중심(centered data)으로 되어있다고 가정합니다. 여기서 $x_{i}$는 데이터 행렬 $\mathbf{X}$의  행벡터(1 x n)입니다.</p>

<p>$$
\mathbf{X} = \begin{pmatrix} x_{11} &amp; x_{12} &amp; \ldots \\ x_{21} &amp; x_{22} &amp; \ldots \\ \vdots &amp; \vdots &amp; \ddots \end{pmatrix} = \begin{pmatrix} x_{1} \\ x_{2} \\ \vdots \\ \end{pmatrix}
$$</p>

<p>우선 이 $\mathbf{X}$ 가 아니라, $X_{i}$를 선형 조합한 새로운 $y_{i}$(1 x n)으로 표현하겠습니다.</p>

<p>$$
\left{\begin{matrix} y_{1} = a_{11}x_{1} + a_{12}x_{2} + \ldots + a_{1p}x_{p} = a_{1}^{T}\mathbf{X} \\ \vdots  \\ y_{p} = a_{p1}x_{1} + a_{p2}x_{2} + \ldots + a_{pp}x_{p} = a_{p}^{T}\mathbf{X}  \end{matrix} \right.
$$</p>

<p>이러한 선형 결합은 $y_{i}$는 $\mathbf{X}$를 새로운 $a_{i}$(p x 1)의 축으로 사영(projection)한 것입니다. 만들어진 $y_{i}$로 구성된 행렬 $\mathbf{Y}$ (p x n)은 아래처럼 나타낼 수 있습니다.</p>

<p>$$
\mathbf{Y} =\left( \begin{array}{cccc} y_{1} \\ y_{2} \\ \vdots \\ y_{p} \end{array} \right)  =\left( \begin{array}{cccc} a_{1}^{T} \mathbf{X} \\ a_{2}^{T} \mathbf{X} \\ \vdots \\ a_{p}^{T} \mathbf{X} \end{array} \right) = A^{T} \mathbf{X}
$$</p>

<h3 id="24-pca-on-covariance">2.4. PCA on Covariance</h3>

<p>위에서 보았든 우리는 새로운 축으로 선형 결합된 $\mathbf{Y}$의 분산을 최대화하고 싶습니다.</p>

<p>$$
Var(\mathbf{Y})=Var(A^{T}\mathbf{X})=A^{T}Cov(\mathbf{X})A
$$</p>

<p>여기서 $a_{i}$의 norm은 1로 제약하고 분산을 가장 크게하는 $a_{i}$를 라그랑지안을 사용하여 구합니다.</p>

<p>$$
\begin{aligned} \underset{a_{i}}{max}{Var (\mathbf{Y})} &amp;= \underset{a_{i}}{max}{ Var (a_{i} \mathbf{X}) } \\ s&amp;= \underset{a_{i}}{max} {  a_{i}^{T} Cov (\mathbf{X}) a_{i} } \end{aligned}
$$</p>

<p>제약식을 $\lambda$ 안에 적어줍니다.
$$
L = a_{i}^{T}Cov(\mathbf{X})a_{i} - \lambda (a_{i}^{T}a_{i}-1)
$$
$a_{i}$로 미분한 식을 0으로 두고 최대값을 찾으면 아래와 같습니다.</p>

<p>$$
\begin{aligned}
\frac{\partial L}{\partial a_{i}} = Cov(\mathbf{X})a_{i} - \lambda a_{i} &amp;= 0 \\ 
(Cov(\mathbf{X})-\lambda) a_{i} &amp; = 0
\end{aligned}
$$</p>

<p><strong>고유벡터(eigenvector)</strong>의 정의에 의해 $a$는 $Cov(\mathbf{X})$의 고유벡터, <strong>고유값(eigenvalue)</strong>의 정의에 의해 $\lambda$는 $Cov(\mathbf{X})$의 고유값이 됩니다. 즉 선형 조합의 계수 벡터인 $a_{i}$가 $\mathbf{X}$의 고유벡터(eigen vector)일 때 $Y_{i}$의 <strong>분산이 최대</strong>가 됩니다.</p>

<p>$Cov(\mathbf{X})$는 $XX^{T}$에 비례하며 이렇게 표현된 공분산은 Spectral Decomposition에 의해 아래처럼 분해됩니다.</p>

<p>$$
\begin{gathered}
\mathbf{XX^{T}} = \Sigma : \text{Covariance matrix (p x p)} \\ 
\text{By Spectral Decomposition, } \mathbf{XX^{T}} = \mathbf{VDV^{T}} \\ 
\text{where } \mathbf{V^{T}V}=\mathbf{VV^{T}}= \mathbf{I} \\ 
\text{with } \mathbf{V} { v_{1}, v_{2}, …, v_{p} } \text{ (eigenvectors)} \\ 
\text{ and }\mathbf{D}=diag {\lambda_{1}, \lambda_{2}, \ldots \lambda_{p} } \\ 
\text{with }\lambda_{1} \ge \lambda_{2} \ge \ldots \ge \lambda_{p} \ge 0. \text{ (eigenvalues)}
\end{gathered}
$$</p>

<p>따라서 $y_{i}$의 분산은 아래와 같이 쓸 수 있습니다.</p>

<p>$$
\begin{aligned}
Var(y_{i}) &amp;= v_{i}^{T}Cov(\mathbf{X})v_{i} \\ 
&amp;= v_{i}^{T}\mathbf{VDV^{T}}v_{i} \\ 
&amp;=\lambda_{i}
\end{aligned}
$$</p>

<p>분산은 그에 대응한 고유값(eigen value)가 됩니다.</p>

<p><img src="/assets/images/pca/pca-07.PNG" alt="PCA3" style="zoom:80%;" /></p>

<p>$$
\begin{aligned}
max \big( var(Y_{i}) \big) &amp;= \lambda_{i} \textrm{ : eigen value} \\ 
 \textrm{when } a_{i} &amp;= v_{i} \textrm{ : eigen vector}
\end{aligned}
$$</p>

<p>따라서 새로 만들어진 X의 선형 조합 $V^{T}X$는 주성분 축이 되며, 아래처럼 나타낼 수 있습니다. <strong>가장 큰 고유값 순으로 그에 대응하는 고유벡터에 사영한 데이터들이 가장 큰 정보를 가지고 있습니다.</strong> 따라서PC1, PC2, … 순으로 그 축이 데이터를 가장 많이 설명합니다. 또한 주성분 축은 <strong>선형 독립(orthogonal)</strong>한 고유벡터에 mapping하기 때문에 <strong>다중공선성(multicollinearity)을 해결</strong>합니다.</p>

<p><img src="/assets/images/pca/pca-08.PNG" alt="" style="zoom:50%;" /></p>

<p>위의 예시를 보면, 원 데이터를 0을 중심으로 이동한 뒤 고유벡터 2개로 이루어진 2개의 PC로 재구성된 것을 확인할 수 있습니다.</p>

<p>$$
\begin{aligned}
PC1 &amp;= v_{1}^{T}X \\ 
PC2 &amp;= v_{2}^{T}X
\end{aligned}
$$</p>

<h3 id="25-result-of-pca">2.5. Result of PCA</h3>

<p>주성분을 이용해서 표현하는 것은 소위 noise라 불리는 필요 없는 정보를 줄일 수 있습니다. 그러나 당연히 데이터의 정보를 잃어버리게 되겠죠. 그렇다면 몇개의 주성분을 사용해야 원래의 데이터를 얼만큼 설명할지 정해야합니다.</p>

<p>주성분의 수는 보통 Scree plot을 통해 정합니다. 통상적으로 고유값이 크게 꺽이는 ‘‘팔꿈치’‘부분에서 주성분의 개수를 정합니다. 아래의 경우는 주성분 3~4개 정도를 통상적으로 사용합니다.</p>

<p><img src="/assets/images/pca/pca-09.PNG" alt="sreeplot" style="zoom:50%;" /></p>

<p><strong>주성분이 원래의 데이터에서 얼만큼을 설명하고 있는지</strong>는 고유값을 통해서 알 수 있습니다. 따라서 얼만큼 원 데이터를 잘 설명하고 있는지는 아래 고유값의 비율을 더한 누적 비율로 설명할 수 있습니다.</p>

<p>$$
\frac{\lambda_{k}}{\sum^{p}_{i=1}\lambda_{i}}
$$</p>

<p>PCA는 순차적으로 가장 큰 정보량을 먼저 담기 떄문에 가장 최소의 정보 손실을 보장할 수 있다는 장점이 있습니다. 그러나 이렇게 만들어진 PC는 원 변수의 조합이기 때문에, 그 변수를 해석해석하기 적절하지 않습니다.</p>

<h2 id="3-sparse-pca">3. Sparse PCA</h2>

<p>전통적인 PCA에 sparsity 제약 조건을 더하여 확장해준 개념입니다. 차원이 매우 크면 PCA는 일관적이지 않고 해석하기에 어렵습니다. Sparse PCA는 특히 아래처럼 Sparse한 데이터에서 차원을 축소해주는 동시에 변수 선택을 동시에 할 수 있습니다.</p>

<p><img src="/assets/images/pca/pca-10.PNG" alt="sparsedata" style="zoom:80%;" /></p>

<p>Lasso, Ridge, Elastic net과 같은 방법들과 같이 L-norm과 $\alpha$를 수정하여 피팅합니다. 아래처럼 L1-norm이면 sparse한 변수들이 0으로 효과적으로 제거되어 변수 선택의 효과가 있습니다.</p>

<p>$$
\begin{gathered}
\min_{U, V}||X-UV||^{2}_2 + \alpha ||V||_{1} \\ 
\text{subject to } ||U_{k}||_{2} = 1 \text{ for all } 0 \le k &lt; n
\end{gathered}
$$</p>
:ET