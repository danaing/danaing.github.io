I"ùP<p>ì¡¸ì—… ë…¼ë¬¸ì´ ë³¸ì‹¬ì„ í†µê³¼í•˜ë©´ì„œ 2ë…„ì˜ ì„ì‚¬ ê³¼ì •ì„ ë§ˆì¹˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì œê°€ ì—°êµ¬í•œ ì£¼ì œ <strong>â€˜Change-point detection using Nonparametric methodsâ€™</strong>ì— ëŒ€í•´ ì ì–´ë³´ë ¤ í•©ë‹ˆë‹¤.</p>

<ul>
  <li>
    <p><strong>ë…¼ë¬¸</strong><br />
â€˜Empirical Likelihood for Change-point Detection using Double Quantileâ€™ Danah Kim, Master thesis, Graduate School, Yonsei University, 2020<br />
â†’ <a href="https://library.yonsei.ac.kr/search/detail/CAT000001994068?briefLink=/main/searchBrief?q=%EA%B9%80%EB%8B%A8%EC%95%84">URL</a></p>
  </li>
  <li>
    <p><strong>í†µê³„í•™íšŒ ì°¸ì—¬ ë° ìˆ˜ìƒ í›„ê¸°</strong><br />
â†’ <a href="https://danaing.github.io/statistics/2019/11/30/19-fall-statistics-conference.html">ë°”ë¡œê°€ê¸°</a></p>
  </li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<h3 id="11-backgrounds-of-change-point-problem">1.1. Backgrounds of Change-point Problem</h3>

<p><img src="/assets/images/master-thesis/2020-03-01-masters-thesis-nonpara-change-point-fde636c9.png" width="80%" /></p>
<center> <small> ë‚˜ì¼ ê°•ì˜ ì—°ê°„ ìœ ëŸ‰ì˜ ë°ì´í„°ë¡œ ì ì„ ì´ change-pointë¥¼ ì˜ë¯¸í•¨.</small> </center>
<p><br /></p>

<p>ë¨¼ì € <strong>Change-point</strong>ë€ ë‹¨ì–´ ëœ» ê·¸ëŒ€ë¡œ ë³€í™”ê°€ ìƒê¸°ëŠ” ì§€ì , ì¦‰ <strong>ë³€ë™ì </strong>ìœ¼ë¡œ ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ì¼ë ¨ì˜ ê³¼ì •ì—ì„œ <strong>ê·¼ë³¸ì ì¸ í”„ë¡œì„¸ìŠ¤ì˜ í†µê³„ì ì¸ ì†ì„±ì´ ë³€í•œ ì§€ì </strong>ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.</p>

<p><img src="/assets/images/master-thesis/2020-03-01-masters-thesis-nonpara-change-point-controlchart.svg" width="70%" /></p>
<center> <small> sampleì˜ í‰ê· ê°’ìœ¼ë¡œ ê·¸ë¦° control chart ì˜ˆì‹œ (ì¶œì²˜ 1) </small> </center>
<p><br /></p>

<p>ê¸ˆìœµì´ë‚˜ ì œì¡°ì—…, ì—­í•™ ë“±ì˜ ë‹¤ì–‘í•œ ë¶„ì•¼ì™€ ë§ì€ ì‹¤ì œì ì¸ ìƒí™©ì—ì„œ í†µê³„í•™ìëŠ” <strong>change-pointê°€ ë°œìƒí–ˆëŠ”ì§€, ê·¸ë¦¬ê³  ë°œìƒí–ˆë‹¤ë©´ ì–´ë””ì— ë°œìƒí–ˆëŠ”ì§€</strong>ì™€ ê°™ì€ ë¬¸ì œì— ë´‰ì°©í•˜ê³¤ í–ˆìŠµë‹ˆë‹¤. <a href="https://en.wikipedia.org/wiki/Statistical_process_control">í†µê³„ì  ê³µì • ê´€ë¦¬(Statistical Process Control)</a>ì˜ ì•„ë²„ì§€ë¡œ ì•Œë ¤ì§„ <a href="https://en.wikipedia.org/wiki/Walter_A._Shewhart">W. A. Shewhart</a>ëŠ” 1931ë…„ì— ì²˜ìŒìœ¼ë¡œ ë¶ˆëŸ‰ë¥  ê´€ë¦¬ì˜ ì¸¡ë©´ì˜ Control Chartë¥¼ ê°œë°œí•˜ì˜€ê³ , E. S. PageëŠ” 1954ë…„ì— ë³€í™”ë¥¼ íƒì§€í•˜ê¸° ìœ„í•´ <a href="https://en.wikipedia.org/wiki/CUSUM">CUSUM chart</a>ë¥¼ ê³ ì•ˆí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê³ ë¯¼ê³¼ í•´ê²°ì˜ ì‹œë„ëŠ” ì˜¤ë˜ì „ë¶€í„° ìˆì—ˆìœ¼ë©° ì´í›„ ë§ì€ ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.</p>

<p><img src="/assets/images/master-thesis/2020-03-01-masters-thesis-nonpara-change-point-cusum.png" width="70%" /></p>
<center> <small> Thresholdì™€ CUSUM chart ì˜ˆì‹œ (ì¶œì²˜ 2) </small> </center>
<p><br /></p>

<h3 id="12-change-point-problem">1.2. Change-point Problem</h3>

<p>Change-point Probelmì— ëŒ€í•œ ì •ì˜ëŠ” ë‹¤ì–‘í•©ë‹ˆë‹¤ë§Œ, ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” Change-point problemì— ëŒ€í•´ ì´ë ‡ê²Œ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.</p>

<p>Consider a sequence of
observations $x_{1}, x_{2}, \ldots, x_{n}$ drawn from independent random variables $X_{1}, X_{2}, \ldots, X_{n}$.<br />
Multiple $m$ change-points $\tau_{1}, \tau_{2}, \ldots , \tau_{m}$ exist in the data. Then there are $(m+1)$ segments.<br />
The distributions of the sequence can be written as</p>

<p>$$
X_{i} \sim
\begin{cases}
&amp; F_{1} \; \; \; \; \text{ if $i \le \tau_{1}$} \\ 
&amp; F_{2} \; \; \; \; \text{ if $\tau_{1} &lt; i \le \tau_{2}$} \\ 
&amp; \vdots \\ 
&amp; F_{m+1} \; \text{ if $\tau_{m} &lt; i$}
\end{cases}
$$</p>

<p>ì¦‰ Change-pointë¬¸ì œëŠ” Change-point $\tau$ê°€ $\tau_{1}$ ë¶€í„° $\tau_{m}$ê¹Œì§€ $m$ê°œ ì¡´ì¬í•  ë•Œ, $F_{1}$ë¶€í„° $F_{m+1}$ê¹Œì§€ $m+1$ê°œì˜ distributionìœ¼ë¡œ ë‚˜ëˆ ì§„ë‹¤ëŠ” ê°€ì • í•˜ì— ì‹œì‘í•©ë‹ˆë‹¤. ì´ ë•Œ distributionì˜ structure ë³€í™”ëŠ” ì£¼ë¡œ <strong>í‰ê· </strong>ì´ë‚˜ <strong>ë¶„ì‚°</strong>ì˜ ë³€í™” í˜¹ì€ <strong>ë¶„í¬</strong>ì˜ ìì²´ì˜ ë³€í™”ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ Change-point problemì—ì„œ <strong>$\tau$ì˜ ì ì ˆí•œ ìœ„ì¹˜ì™€ ê°œìˆ˜ë¥¼ detect</strong>í•´ì•¼ í•©ë‹ˆë‹¤.</p>

<h3 id="13-change-point-model">1.3. Change-point Model</h3>

<p>Change-point problemì—ì„œ <strong>$\tau$</strong> <strong>ì˜ ì ì ˆí•œ ìœ„ì¹˜ì™€ ê°œìˆ˜ë¥¼ detectí•˜ê¸° ìœ„í•œ í†µê³„ì  ëª¨ë¸</strong>ì„ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.  ë¨¼ì € $\tau$ì—ì„œ 1ê°œì˜ ë³€í™”ê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ single change-pointë¥¼ êµ¬í•˜ê³ , ë‚˜ë‰˜ì–´ì§„ ì§€ì ì„ ê¸°ì¤€ìœ¼ë¡œ ì• ë’¤ë¡œ <strong>binaryí•œ segmentation</strong>ì„ ì—°ì†ì ì´ê³  ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ë©´ ëª¨ë“  change-pointë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ single change-pointë¥¼ detectí•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ë‹¨ìˆœí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>Consider independent random variables $X_{1} \sim G_{1},\ldots,X_{n}\sim G_{n}$.<br />
Assume that there is at most one change $\tau$ in the sequence of distributions above. We want to test the null hypothesis of no change</p>

<p>$$
\label{eq1}\tag{1}
\mathbf{H_{0}} : G_{1} = G_{2} = \ldots = G_{n} = F,
$$</p>

<p>against the following alternative of one change</p>

<p>$$
\label{eq2}\tag{2}
\mathbf{H_{a}} : F_{1} = G_{1} = G_{2} = \ldots = G_{\tau} \ne G_{\tau+1} = \ldots = G_{n} = F_{2}.
$$</p>

<p>where $1 \le \tau &lt; n$ and neither F nor G is degenerate.
Using binary segmentation, it suffices to test and estimate the position of a single change point at each stage sequentially.</p>

<h2 id="2-proposed-methodology">2. Proposed Methodology</h2>

<h3 id="21-methods-on-change-point-analysis">2.1. Methods on change-point analysis</h3>

<p>ìœ„ì—ì„œ ì„¤ëª…í•œ Modelì—ì„œ change-pointë¥¼ íƒì§€í•˜ëŠ” ë°©ë²•ì„ í¬ê²Œ í†µê³„ì ìœ¼ë¡œ <strong>ëª¨ìˆ˜ì  ë°©ë²•</strong>ê³¼ <strong>ë¹„ëª¨ìˆ˜ì  ë°©ë²•</strong>ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ìˆ˜ì  ë°©ë²• ì¤‘ ì—°êµ¬ê°€ ê°€ì¥ ë§ì´ëœ ë¶„ì•¼ëŠ” ì •ê·œë¶„í¬ì—ì„œ í‰ê· ì˜ ë³€í™”ì— ëŒ€í•œ íƒì§€ì´ë©° ë˜í•œ ë¶„ì‚°ì˜ ë³€í™”ì— ëŒ€í•´ì„œë„ ì—°êµ¬ê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤.<br />
ê·¸ëŸ¬ë‚˜ ëª¨ìˆ˜ì  ë°©ë²•ì—ëŠ” <strong>(1)</strong> <u>ëª¨ìˆ˜ì  ê°€ì •ì€ ë•Œë•Œë¡œ í˜„ì‹¤ì—ì„œ ë§Œì¡±ë˜ì§€ ì•Šìœ¼ë©°</u> <strong>(2)</strong> <u>ì•„ì›ƒë¼ì´ì–´ì— ë¯¼ê°í•˜ê³ ,</u> <strong>(3)</strong> <u>ê·¹ë‹¨ì— ìœ„ì¹˜í•œ ê°’ì€ ë¶„í¬ì— ë”°ë¼ í¬ê²Œ ë‹¤ë¥´ë‹¤</u>ëŠ” í•œê³„ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤.</p>

<p>ê·¸ì— ë¹„í•´ ë¹„ëª¨ìˆ˜ì  ë°©ë²•ì€ ê°€ì •ì´ í›¨ì”¬ ì ìœ¼ë©° ë‹¤ì–‘í•œ ìƒí™©ì— ë” ì ì ˆí•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¹„ëª¨ìˆ˜ì  ë°©ë²•ìœ¼ë¡œë„ change-pointì— ëŒ€í•´ ë§ì€ ì—°êµ¬ê°€ ë˜ì—ˆìœ¼ë©°, 2007ë…„ì˜ Empirical likelihood ratioë¥¼ ì´ìš©í•œ change-pointë¬¸ì œì— ëŒ€í•œ ë…¼ë¬¸[^1]ê³¼ 2017ë…„ì— ì œì•ˆëœ Quantile empirical likelihoodì„ ë°©ë²•[^2]ì„ ë°œì „ì‹œì¼œ, ì´ë²ˆ ë…¼ë¬¸ì€ ì–‘ ë°©í–¥ìœ¼ë¡œì˜ Double quantile empirical likelihoodë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.</p>

<h3 id="22-empirical-likelihood">2.2. Empirical likelihood</h3>

<p>ì´ ë…¼ë¬¸ì—ì„œ ì´ìš©í•œ ë¹„ëª¨ìˆ˜ì  ë°©ë²•ë¡ ì€ <strong>Empirical likelihood</strong>ì…ë‹ˆë‹¤. <a href="https://statistics.stanford.edu/people/art-b-owen">Art B. Owen</a>ì´ 1988ë…„ì— ë¹„ëª¨ìˆ˜ì  ë°©ë²•ì˜ empirical likelihoodë¥¼ ì²˜ìŒ ì œì‹œí•˜ì˜€ëŠ”ë°, ì£¼ìš” ì•„ì´ë””ì–´ëŠ” ê°ê°ì˜ ê´€ì¸¡ì¹˜ì— unknown probability massë¥¼ ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.</p>

<p>Assume that independently and identically distributed observation $x_{1}, â€¦ ,x_{n}$ are from an unknown population distribution $F$. Let $p_{i} = P(X=x_{i})$. Empirical likelihood function of $ { p_{i} }_{i=1}^{n}$ is defined as</p>

<p>$$
L(F) = \prod_{i=1}^{n} p_{i},
$$</p>

<p>where $p_{i}$ satisfy the constraints $p_{i} \ge 0$ and $\sum_{i=1}^{n} p_{i}=1$</p>

<p>ê·¸ëŸ¬ë©´ $L(F)$ëŠ” $p_{i}=1/n$ ì¼ë•Œ ê°€ì¥ ìµœëŒ€ê°€ ë˜ë©°, Full modelì—ì„œ $n^{-n}$ë¥¼ ìµœëŒ€ê°’ìœ¼ë¡œ ê°€ì§‘ë‹ˆë‹¤.</p>

<p>When a population parameter $\theta$ identified by $E[m(X;\theta)]=0$ is of interest, the empirical likelihood maximum when $\theta$ has the true value $\theta_{0}$ is obtained subject to the additional constraint</p>

<p>$$
\sum_{i=1}^{n} p_{i} m(x_{i},\theta_{0}) = 0.
$$</p>

<p>ê´€ì‹¬ìˆëŠ” ëª¨ìˆ˜($\theta$)ì— ëŒ€í•œ ì œì•½ì‹ì„ ì¶”ê°€í•˜ì—¬ likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê° observationì— ëŒ€í•œ probabilityë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°ê°ì˜ ì œì•½ì‹ì„ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•(Lagrange multiplier)ìœ¼ë¡œ í’€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>To find ${p_{i}}_{i=1}^{n}$ under the restrictions, solve the Laglange Multiplier</p>

<p>$$
\sum_{i=1}^{n} \log p_{i} + \lambda_{0} ( \sum_{i=1}^{n} p_{i} - 1) +  \lambda_{1} (\sum_{i=1}^{n} p_{i} m(x_{i},\theta_{0})).
$$</p>

<p>ë”°ë¼ì„œ ì•„ë˜ì²˜ëŸ¼ Empirical Likelihood Ratio(ELR)ë¥¼ êµ¬í•  ìˆ˜ ìˆìœ¼ë©°, ì´ê²ƒì€ ê·¼ì‚¬ì ìœ¼ë¡œ Chi-squared distributionì„ ë”°ë¥¸ë‹¤ê³  ì•Œë ¤ì ¸ìˆìŠµë‹ˆë‹¤. Empirical Likelihood ë°©ë²•ì€ Lagrange Multiplier Methodë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ ì œì•½ì‹ìœ¼ë¡œ í™•ì¥í•˜ì—¬ probabilityë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>The Empirical Likelihood Ratio(ELR) statistic to test $\theta = \theta_{0}$ is given by</p>

<p>$$
\mathbf{R(\theta_{0})} = \frac{L(F)}{L(F_{n})} = \max \left ( \prod_{i=1}^{n} np_{i} | \sum_{i=1}^{n} p_{i}m(x_{i}, \theta_{0})=0, p_{i} \ge 0, \sum_{i=1}^{n} p_{i} =1 \right )
$$</p>

<p>Under the null model $\theta = \theta_{0}$ with mild regular conditions, $-2 \log \mathbf{R(\theta_{0})} \to \chi_{r}^{2}$ in distribution as $n \to \infty$, where $r$ is dimension of $m(x, \theta)$ (Owen, 1988). The empirical likelihood method can be extended to other constraints using Lagrange multiplier method to find $\{p_{i}\}_{i=1}^{n}$.</p>

<h3 id="23-empirical-likelihood-for-two-groups-comparison">2.3. Empirical Likelihood for Two groups comparison</h3>

<p>ìœ„ ê²€ì • ë°©ë²•ì„ ë‘ ê°œì˜ ê·¸ë£¹ ë¹„êµ ê²€ì •(Two groups comparison test)ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. $F_{1}$ê³¼ $F_{2}$ì˜ probabiityë¥¼ ìµœëŒ€ë¡œí•˜ëŠ” empirical likelihoodë¥¼ ì •ì˜í•˜ë©´, ì•„ë˜ì™€ ê°™ì´ two sample testë¡œ change-pointë¥¼ binaryí•˜ê²Œ 2ê°œì˜ ë¶„í¬ë¡œ ë‚˜ëˆ„ëŠ” testì™€ ë™ì¼í•˜ê²Œ ë©ë‹ˆë‹¤.</p>

<p>Two samples: $X_{1}, X_{2}, \ldots, X_{n} \sim F_{1}$ and $Y_{1}, Y_{2}, \ldots, Y_{m} \sim F_{2}$ and let $p_{i} = P(X=x_{i})$ and $q_{j} = P(Y=y_{j})$.
Empirical likelihood function of $p_{i}$ and $q_{j}$ is defined as</p>

<p>$$
L(F) = \prod_{i=1}^{n} p_{i}\prod_{j=1}^{m} q_{j},
$$</p>

<p>where $p_{i}$ and $q_{j}$ satisfy the constraints $p_{i} \ge 0, q_{j} \ge 0$ and $\sum_{i=1}^{n} p_{i}=1$, $\sum_{j=1}^{m} q_{j}=1$</p>

<p>This hypothesis (\ref{eq1}) and (\ref{eq2}) is equivalent to</p>

<p>$$
\mathbf{H_{0}} : F_{1} = F_{2},
$$</p>

<p>against</p>

<p>$$
\mathbf{H_{a}} : F_{1} \ne F_{2}.
$$</p>

<p>Thus, the hypothesis becomes two sample test.</p>

<h3 id="24-quantile-llikelihood-ratio-for-two-sample">2.4. Quantile Llikelihood Ratio for Two Sample</h3>

<p>Zhou, Y., Fu, L., and Zhang, B.(2017) ë…¼ë¬¸ì— ë”°ë¥´ë©´ ì—¬ê¸°ì— Quantileì„ ì´ìš©í•œ constraintsë¥¼ empirical likelihoodì— ì¶”ê°€í•˜ì—¬ probabilityë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>Under the null hypothesis, for any given $x$, we have $F_{1}(x)=F_{2}(x)=F(x)$. Let $p=F(\xi_{p})$; hence, $\xi_{p}$ is the $p$ quantile of $F$ and $\xi_{p}$ needs to satisfy</p>

<p>$$
\label{eq3}\tag{3}
    E[I(X_{i} \le \xi_{p})-p]=0, \quad \text{for } 1 \le i \le n+m,
$$
We can construct the following quantile empirical likelihood test statistic under restriction,</p>

<p>$$
\mathbf{R(\xi_{p})} = \max \left ( \prod_{i=1}^{n} np_{i} \prod_{j=1}^{m} mq_{j} | \sum_{i=1}^{n} p_{i} I(X_{i} \le \xi_{p}) ) = p,  \right .
$$
$$
\left . 
\sum_{j=1}^{m} q_{j} I(Y_{j} \le \xi_{p}) ) = p, p_{i}, q_{j} \ge 0, \sum_{i=1}^{n} p_{i} = \sum_{j=1}^{m} q_{j} =1 \right )
$$</p>

<p>ì´ë¥¼ í™•ì¥ì‹œì¼œ ì œê°€ ì œì•ˆí•˜ëŠ” ë°©ë²•ì´ <strong>Double quantile likelihood</strong>ì…ë‹ˆë‹¤. ì•ì„œë³´ì•˜ë˜ empirical likelihoodì— left sideì™€ right sideì˜ quantileì„ constraintsë¡œ ì‚¬ìš©í•˜ì—¬ ì¶”ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. 2ê°œì˜ quantileì„ ì‚¬ìš©í•˜ì—¬ Empirical likelihoodë¥¼ maximizeí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.</p>

<p>Double quantile likelihood expand (\ref{eq3}) to $\textbf{Double quantile likelihood}$ for the both extreme side.
Let $p=F(\xi_{p})$ and $1-q=F(\xi_{1-q})$ ; hence, $\xi_{p}$ is the $p$ quantile of $F$ and $\xi_{1-q}$ is the $1-q$ quantile of $F$. This satisfies</p>

<p>$$
\label{eq4}\tag{4}
E[I(X_{i} \le \xi_{p})-p]=0, \quad E[I(X_{i} \ge \xi_{1-q})-q)]=0
$$</p>

<p>where $0 &lt; p &lt; 1-q &lt; 0$ for $1 \le i \le n+m$.</p>

<p>Using (\ref{eq4}), double quantile empirical likelihood test statistic under restriction is</p>

<p>$$
\label{eq5}\tag{5}
\mathbf{R(\xi_{p}, \xi_{1-q})} = \max \left ( \prod_{i=1}^{n} np_{i} \prod_{j=1}^{m} mq_{j} | \sum_{i=1}^{n} p_{i} I(X_{i} \le \xi_{p}) )=p, \right .
$$
$$
\sum_{j=1}^{m} q_{j} I(Y_{j} \le \xi_{p}) )=p, \sum_{i=1}^{m} q_{i} I(X_{i} \le \xi_{1- q}) )=1-q, 
$$
$$
\left . \sum_{j=1}^{m} q_{j} I(Y_{j} \le \xi_{1-q}) )=1-q, p_{i}, q_{j} \ge 0, \sum_{i=1}^{n} p_{i} = \sum_{j=1}^{m} q_{j} =1  \right )
$$</p>

<p>ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•(Lagrange multiplier)ì„ í†µí•´ ìœ ë‹ˆí¬í•œ ëŒë‹¤ë¥¼ êµ¬í•˜ê³  ì´ë¡œ í™•ë¥ ì„ ì¶”ì •í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ìœ ë„ë˜ëŠ” DLRëŠ” ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤. ì´ë•Œ ì´ test statisticì„ ìµœëŒ€í™”ì‹œí‚¤ëŠ” ì‚¬ì´pì™€ ì‚¬ì´1-që¥¼ íƒí•˜ê³ , í° test statistic Dnì€ ê°€ì¥ ê°€ëŠ¥ì„±ì´ í° ì ì–´ë„ í•˜ë‚˜ì˜ change-pointê°€ ìˆë‹¤ëŠ” ëœ»ìœ¼ë¡œ ê·€ë¬´ê°€ì„¤ì˜ ê¸°ê°ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦ëª…ì€ Appendix.Aì— ìˆ˜ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</p>

<p>Using Lagrange multipliers to solve (\ref{eq5}), we can get following unique $\lambdaâ€™s$ and $p_{i}$ ,  $q_{j}$. (Proof in Appendix.A) This leads to double quantile likelihood ratio(DLR) test statistic.</p>

<p>$$
\begin{aligned}
\mathbf{R(\xi_{p}, \xi_{1-q})} = \left ( \frac{np}{n_{1}} \right )^{n_{1}} \left ( \frac{nq}{n_{2}} \right )^{n_{2}} \left ( \frac{n(1-p-q)}{n-n_{1}-n_{2}} \right )^{n-n_{1}-n_{2}} \\ 
 \left ( \frac{mp}{m_{1}} \right )^{m_{1}} \left ( \frac{mq}{m_{2}} \right )^{m_{2}} \left ( \frac{m(1-p-q)}{m-m_{1}-m_{2}} \right )^{m-m_{1}-m_{2}}
\end{aligned}
$$</p>

<p>where $\sum_{i=1}^{n} I(X_{i} \le \xi_{p}) = n_{1}$, $\sum_{i=1}^{n}I( X_{i} &gt; \xi_{1-q}) = n_{2}$ and $\sum_{j=1}^{m} I(Y_{j} \le \xi_{p}) = m_{1}$, $\sum_{j=1}^{m}I( Y_{j} &gt; \xi_{1-q}) = m_{2}$</p>

<p>$$
\therefore D_{n} = \sup_{\xi_{p}&lt;\xi_{1-q}} { -2\log \mathbf{R(\xi_{p}, \xi_{1-q})} }<br />
$$</p>

<p>Large values of $D_{n}$ indicate that there is at least one change-point.</p>

<h2 id="3-simulations">3. Simulations</h2>

<h3 id="31-algorithm-for-change-point-detection">3.1. Algorithm for change-point detection</h3>

<p>ì†Œê°œí•´ë“œë¦° DLRë¡œ change-pointë¥¼ detectioní•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. Change-point $\tau$ëŠ” two sample testì¸ DLRë¥¼ í†µí•´ ì¶”ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ë•Œ F1ê³¼ F2ì˜ sample sizeê°€ ì‘ë‹¤ë©´ ELì˜ estimatorì¸ ëŒë‹¤ëŠ” ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ, $n$ê³¼ $m$ì˜ ìƒ˜í”Œì‚¬ì´ì¦ˆëŠ” 2007ëŠ” Zouì˜ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ trimming ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì¡°ì •ëœ $D_{n}^{*}$ ë¥¼ êµ¬í•˜ê³ , ê³„ì‚°ëœ $D_{n}^{*}$ ë¥¼ ê°€ì¥ maximizeì‹œí‚¤ëŠ” $\tau$ë¥¼  $\tau$ìœ¼ë¡œ ì¶”ì •í•©ë‹ˆë‹¤.</p>

<p>Change-point detection problem is to detect $\tau$ where</p>

<p>$$
F_{1} = G_{1} = G_{2} = \ldots = G_{\tau} \ne G_{\tau+1} = \ldots = G_{n} = F_{2}
$$</p>

<p>Two sample test: $X_{1}, \ldots, X_{n} \sim F_{1}$ and $Y_{1}, \ldots, Y_{m} \sim F_{2}$. When $n$ or $m$ is too small, the empirical likelihood estimators of $\lambdaâ€™s$ may not exist. Therefore, use a trimmed statistic (Zou, C. (2007))</p>

<p>$$
D_{n}^{*} = \sup_{ c(n+m)^{-1/9}&lt;\xi_{p}&lt;\xi_{1-q}&lt;1-c(n+m)^{-1/9} } { -2\log \mathbf{R(\xi_{p}, \xi_{1-q})} }
$$</p>

<p>where $c$ is a positive constant.
The location $\tau$ can be estimated by</p>

<p>$$
\hat\tau = \arg_{\tau} \max { D_{n}^{*} }
$$</p>

<h3 id="32-simulation">3.2. Simulation</h3>

<p>ë‹¤ìŒì€ ì‹œë®¬ë ˆì´ì…˜ì…ë‹ˆë‹¤. Single change-pointì— ëŒ€í•´ ì‹œë®¬ë ˆì´ì…˜ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. í‰ê· ì˜ ì°¨ì´ë¥¼ ë¸íƒ€ë¡œ ê³ ì •ì‹œí‚¤ê³  ë‘ ë¶„í¬ì—ì„œ observationê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  change-pointì˜ ìœ„ì¹˜ëŠ” 25%, 50%, 75%, 95%ë¡œ 4ê°œì˜ ìë¦¬ì— ìœ„ì¹˜ì‹œì¼°ìŠµë‹ˆë‹¤. ì»´í“¨íŒ…ì„ ê³ ë ¤í•˜ì—¬ ì‚¬ì´ $p$ì™€ ì‚¬ì´ $1-q$ì˜ rankëŠ” ì „ì²´ ìƒ˜í”Œì‚¬ì´ì¦ˆì˜ ì ˆë°˜ì´ìƒì˜ ì°¨ì´ê°€ ë‚˜ë„ë¡ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤. $D_{n}^{*}$ë¥¼ ê°€ì¥ í¬ê²Œ í•˜ëŠ” $\tau$ë¥¼ change-pointë¡œ íƒí•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ì„ 100ë²ˆ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.</p>

<ol>
  <li>Assume that $X_{1}, â€¦ ,X_{n}$ from $F_{1}$, and $Y_{1}, â€¦ ,Y_{m}$ from $F_{2}$ with different distributions by setting $\delta$ satisfying $\delta=E_{F_{1}}(X)-E_{F_{2}}(X)$</li>
  <li>Change location $m$ takes 25%, 50%, 75%, and 95% quantiles of the number of samples.</li>
  <li>$\xi_{p}$ and $\xi_{1-q}$ are the value of $xâ€™s$ satisfying the rank($\xi_{p}$)-rank($\xi_{1-q}$) $\ge 0.5(n+m)$ for computation.</li>
  <li>Calculate $D_{n}^{*}$ and detect change-point $\hat\tau$.</li>
  <li>For each case, 100 simulations are carried out.</li>
  <li>Calculate the accuracy rate.</li>
</ol>

<p><img src="/assets/images/master-thesis/2020-03-01-masters-thesis-nonpara-change-point-d2fd887e.png" width="70%" /></p>

<p><img src="/assets/images/master-thesis/2020-03-01-masters-thesis-nonpara-change-point-143a683c.png" width="70%" /></p>
<center> <small> Simulation Results </small> </center>
<p><br /></p>

<p>ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ DLRì´ ì „ë°˜ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë¶„í¬ì™€ ì—¬ëŸ¬ ìœ„ì¹˜ì— ëŒ€í•´ ì „ë°˜ì ìœ¼ë¡œ ê´œì°®ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p><img src="/assets/images/master-thesis/2020-03-01-masters-thesis-nonpara-change-point-12890a3a.png" width="100%" /></p>
<center> <small> DLR result: $\tau$=28, change-point=1898. </small> </center>
<p><br /></p>

<p>Change-point ë¶„ì„ì— ë„ë¦¬ ì—°êµ¬ëœ Nile ë°ì´í„°ì— ì‹¤ì œ ì ìš©í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ì™¼ìª½ plotì€ DLRì˜ -2LLRì…ë‹ˆë‹¤. maxê°’ì¸ 28ì´ $\tau$ë¡œ ì¶”ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜¤ë¥¸ìª½ì˜ plotì„ ë³´ì•˜ì„ ë•Œ, 1899ë…„ê¹Œì§€ì™€ ê·¸ ì´í›„ì˜ ìœ ëŸ‰ì€ ëˆˆìœ¼ë¡œë„ ì°¨ì´ê°€ ë³´ì´ë©° change-pointë¥¼ ì˜ detectí•˜ê³  ìˆìŠµë‹ˆë‹¤. (ë…¼ë¬¸ì— ë”°ë¥´ë©´ 1898ë…„ì— ê¸°í›„ ë³€í™”ì™€ nileê°• ì£¼ë³€ aswan damì˜ ê°œì…ìœ¼ë¡œ ë‹¬ë¼ì§„ê²ƒìœ¼ë¡œ ë³´ê³  ìˆìŠµë‹ˆë‹¤.) ì¶”ê°€ì ìœ¼ë¡œ Quantileì´ 1ê°œì¼ë•Œì™€ 2ê°œì¼ë•Œì˜ Empirical CDFì™€ p-valueì— ëŒ€í•œ ë¹„êµëŠ” ì•„ë˜ ì²¨ë¶€ìë£Œ Appendix.Bë¥¼ ì°¸ê³ í•˜ì—¬ ì£¼ì„¸ìš”.</p>

<p>ë³¸ ì—°êµ¬ëŠ” ì˜ˆë¥¼ ë“¤ì–´ ì œì¡°ì—… ê³µì •, ê¸°ìƒ ì¸¡ì •, í†µí™”, ì „ë ¥ ë“±ì˜ ìˆ˜ìš”ê°€ ì´ìƒì´ ë°œìƒë˜ì–´ í›„ì— ë¶„í¬ê°€ ë‹¬ë¼ì§„ë‹¤ë©´ ê·¸ change-pointë¥¼ ì°¾ì„ ìˆ˜ ìˆê³  í†µê³„ì  ê´€ì ì—ì„œ í•´ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‘ ë¶„í¬ì˜ ë‹¤ë¦„ì„ í†µê³„ì  ê²€ì • ë°©ë²•ìœ¼ë¡œ ì ‘ê·¼í•˜ê³  ì´ë¥¼ change-pointë¡œ ë°”ë¼ë³´ëŠ” ê²ƒì— ì˜ì˜ê°€ ìˆìœ¼ë©°, ì¶”í›„ ì•Œê³ ë¦¬ì¦˜ì— ì ìš©í•˜ì—¬ ì—°êµ¬ ë° í™œìš©í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ê¸´ê¸€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.</p>

<h2 id="references">References</h2>

<ul>
  <li>Chen, J. and Gupta, A. K. (2011). Parametric statistical change point analysis: with applicationsto genetics, medicine, and finance. Springer Science Business Media.</li>
  <li>Jing, B.-Y. (1995). Two-sample empirical likelihood method.Statistics &amp; probability letters, 24(4):315-319.</li>
  <li>Owen, A. B. (1988). Empirical likelihood ratio confidence intervals for a single functional.Biometrika, 75(2):237-249.</li>
  <li>Owen, A. B. (2001). Empirical likelihood. Chapman and Hall/CRC. * Ross, G. J. and Adams, N. M. (2012). Two nonparametric control charts for detecting arbitrary distribution changes. Journal of Quality Technology, 44(2):102-116.</li>
  <li>Zhang, J. (2002). Powerful goodness-of-fit tests based on the likelihood ratio.Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(2):281-294.</li>
  <li>Zhang, J. (2006). Powerful two-sample tests based on the likelihood ratio.Technometrics, 48(1):95-103.</li>
  <li>Zhou, Y., Fu, L., and Zhang, B. (2017). Two non parametric methods for change-point detection in distribution.Communications in Statistics-Theory and Methods, 46(6):2801-2815.</li>
  <li>
    <p>Zou, C., Liu, Y., Qin, P., and Wang, Z. (2007). Empirical likelihood ratio test for the change-point problem.Statistics probability letters, 77(4):374-382.</p>
  </li>
  <li><strong>ì´ë¯¸ì§€ ì¶œì²˜</strong>
    <ul>
      <li>1: https://en.wikipedia.org/wiki/Control_chart</li>
      <li>2: https://community.jmp.com/t5/JMPer-Cable/New-features-in-CUSUM-control-charts-for-JMP-16/ba-p/382920</li>
    </ul>
  </li>
</ul>

<hr />
<p>2019ë…„ 10ì›” 22ì¼ ì—°ì„¸ëŒ€í•™êµ ëŒ€ìš°ê´€ ë³¸ê´€ì—ì„œ ì—°êµ¬ ë…¼ë¬¸ì˜ ì˜ˆë¹„ì‹¬ì‚¬ê°€ ì§„í–‰ë˜ì—ˆìœ¼ë©° ë‹¹ì‹œ ë°œí‘œì— ì‚¬ìš©í•œ ìë£Œë¥¼ ì²¨ë¶€í•©ë‹ˆë‹¤.</p>

<embed src="/assets/images/master-thesis/NP_cpt_Preliminary_Evaluation.pdf" type="application/pdf" width="600px" height="450px" />

<hr />

:ET