I"Õ*<p>ëŒ€í•™ì› â€˜ë¹„ëª¨ìˆ˜í†µê³„í•™â€™ ìˆ˜ì—…ì—ì„œ Short Articleë¡œ ë°œí‘œí•œ <strong>Resampling using Multivariate Entropy in Classification</strong>ì— ëŒ€í•´ ì˜¬ë ¤ë´…ë‹ˆë‹¤.</p>

<hr />

<ul>
  <li>
    <dl>
      <dt><strong>ì£¼ì œ</strong></dt>
      <dd>Imbalanced dataì˜ Entropyë¥¼ ê³ ë ¤í•œ Resamplingë°©ë²•ì˜ Classification ì„±ëŠ¥ ë¹„êµ</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong>ì£¼ì œ ì„¤ëª…</strong></dt>
      <dd>Imbalanced dataëŠ” íŠ¹íˆ 2%ì´í•˜ì˜ ë‚®ì€ eventë¥¼ ë³´ì´ëŠ” ë°ì´í„°ë¡œ resamplingì´ í•„ìˆ˜ì ì´ë‹¤. ê¸°ì¡´ì˜ Imbalanced dataë¥¼ Resamplingë°©ë²• ì¤‘ Synthetic ë°©ë²•ì€ ë²”ì£¼í˜• ë³€ìˆ˜ì— ì‚¬ìš©í•˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. Entropyê°€ ë°ì´í„°ì˜ ì •ë³´ëŸ‰ì„ ë‹´ê³  ìˆìœ¼ë¯€ë¡œ, Random resamplingë³´ë‹¤ Classì˜ Entropyì„ ê³ ë ¤í•œ Resamplingì´ ë”ìš± ì¢‹ì€ íš¨ê³¼ë¥¼ ë³´ì¼ ê²ƒì´ë¼ ê¸°ëŒ€í•˜ì˜€ë‹¤. ë”°ë¼ì„œ Imbalanced dataì˜ Classificationì—ì„œ Random samplingê³¼ Entropyë¥¼ ê³ ë ¤í•œ Samplingê³¼ì˜ ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ê³ ì í•œë‹¤.</dd>
    </dl>
  </li>
</ul>

<hr />

<h2 id="1-entropy-and-empirical-entropy">1. Entropy and Empirical Entropy</h2>

<p>$$
H(X) = - \int p(x)\log p(x)
$$</p>

<p>EntropyëŠ” eventì— ëŒ€í•œ surprise, í˜¹ì€ rarenessì„ ë‹´ê³  ìˆë‹¤. ë™ì „ë˜ì§€ê¸°ë¥¼ ì˜ˆë¡œ ë“¤ë©´ ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì´ 0.5ë¡œ ê²°ê³¼ë¥¼ ê°€ì¥ ì•Œ ìˆ˜ ì—†ì„ ë•Œì˜ entropyê°€ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜¨ë‹¤.</p>

<p>$$
\begin{gather}
\widehat{ H(X) } = - \sum_{i} f_{i} \log f_{i} \\ \text{where } f_{i} \text{ is empirical frequency}
\end{gather}
$$</p>

<ul>
  <li>$ \text {Observed counts for each bin (i=1,â€¦ ,11)} $</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: left">i</th>
      <th style="text-align: right">1</th>
      <th style="text-align: right">2</th>
      <th style="text-align: right">3</th>
      <th style="text-align: right">4</th>
      <th style="text-align: right">5</th>
      <th style="text-align: right">6</th>
      <th style="text-align: right">7</th>
      <th style="text-align: right">8</th>
      <th style="text-align: right">9</th>
      <th style="text-align: right">10</th>
      <th style="text-align: right">11</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">counts</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>$ \text{Empirical frequency} f_{i} $</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: left">i</th>
      <th style="text-align: right">1</th>
      <th style="text-align: right">2</th>
      <th style="text-align: right">3</th>
      <th style="text-align: right">4</th>
      <th style="text-align: right">5</th>
      <th style="text-align: right">6</th>
      <th style="text-align: right">7</th>
      <th style="text-align: right">8</th>
      <th style="text-align: right">9</th>
      <th style="text-align: right">10</th>
      <th style="text-align: right">11</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">$freq(f_{i})$</td>
      <td style="text-align: right">0.2105</td>
      <td style="text-align: right">0.1052</td>
      <td style="text-align: right">0.1578</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.1052</td>
      <td style="text-align: right">0.2105</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.105</td>
      <td style="text-align: right">0.0526</td>
      <td style="text-align: right">0.0526</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>$ \text{Empirical estimate of Entropy} $</li>
</ul>

<p>$$
\widehat{ H(X) } = 1.968382
$$</p>

<p>Entropyë¥¼ empiricalí•˜ê²Œ ì¶”ì •í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ Maximum likelihoodì¸ $p_i=1/n$ì„ ì‚¬ìš©í•˜ì—¬ histogramìœ¼ë¡œ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. ì¦‰ ê° binì—ì„œ frequencyì— ë”°ë¼ entropyê°€ ë¶€ì—¬ë˜ëŠ” ê²ƒì´ë‹¤. imbalanced datasetì˜ resamplingì‹œì— majority classì™€ minority classì˜ empirical entropyë¥¼ ê³ ë ¤í•´ì¤€ë‹¤ë©´ ë‘ classì˜ ì •ë³´ëŸ‰ì´ ê³ ë¥´ê²Œ ê³ ë ¤ë˜ì–´ classificationì˜ ì„±ëŠ¥ì´ ê°œì„ ë  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•˜ì˜€ë‹¤.</p>

<h2 id="2-limitations-of-resampling-methods">2. Limitations of Resampling Methods</h2>

<p>ê¸°ì¡´ì˜ resamplingë°©ë²• ì¤‘ SMOTEëŠ” ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” Synthetic dataë¥¼ ìƒì„±í•˜ë¯€ë¡œì¨ categorical variableì´ë‚˜ ë²”ìœ„ê°€ ì •í•´ì§„ variableì—ì„œ ì‚¬ìš©í•˜ê¸° ì–´ë µë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì„±ë³„ì´ ë‚¨(0), ì—¬(1)ì¼ ë•Œ, 0.5ë¡œ ìƒì„±ëœ Synthetic dataëŠ” ê·¸ ì˜ë¯¸ë¥¼ ì•Œê¸° ì–´ë µë‹¤. ë”°ë¼ì„œ categorical variableì´ ì¡´ì¬í•˜ëŠ” dataì—ì„œ SMOTEë¥¼ ì‚¬ìš©í•œ oversamplingë³´ë‹¤ entropyë¥¼ ê³ ë ¤í•œ samplingì´ íš¨ê³¼ì ì¼ ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œë‹¤. random samplingì´ë‚˜ SMOTEì—ì„œ borderlineì„ ê³ ë ¤í•œ samplingë³´ë‹¤ entropyë¥¼ í™œìš©í•œ samplingì´ ë” íš¨ê³¼ì ì¸ì§€ ì¦ëª…í•´ë³´ê³ ì í•œë‹¤.</p>

<h2 id="3-proposed-resampling-methodology-using-entropy">3. Proposed Resampling Methodology using Entropy</h2>

<p>Xì™€ Yì˜ ë³€ìˆ˜ê°€ ë…ë¦½ì´ë¼ë©´ ì•„ë˜ì™€ ê°™ì€ joint entropy $H(X,Y)$ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.</p>

<p><img src="/assets/images/nonpara-resampling/2019-11-25-nonpara-resampling-entropy-570565e6.png" width="70%" /></p>
<center> <small> joint entropy $H(X,Y)$ </small> </center>
<p><br /></p>

<p>EntropyëŠ” ë…ë¦½ì¼ ê²½ìš° Additiveí•œ ì„±ì§ˆì„ ê°€ì§„ë‹¤. ì´ articleì—ì„œëŠ” nê°œì˜ ëª¨ë“  ë…ë¦½ë³€ìˆ˜ê°€ ë…ë¦½ì´ë¼ëŠ” ê°€ì • í•˜ì— Empirical entropyë¥¼ additiveí•˜ê²Œ ì‚¬ìš©í•˜ì˜€ë‹¤. ë”°ë¼ì„œ entropy ê¸°ë°˜ì˜ algorithmì€ ì•„ë˜ì™€ ê°™ë‹¤.</p>

<h3 id="31-algorithm-1---over-sampling-of-minority-class">3.1. Algorithm 1 - Over-sampling of minority class</h3>

<p><strong>Step 1.</strong> Calculate Empirical entropy of majority class</p>

<p>$$
\widehat{H_{major}} (X_1,â€¦,X_n ) \simeq \widehat{H_{major}} (X_1) + â‹¯ + \widehat{H_{major}} (X_n ) 
$$</p>

<p><strong>Step 2.</strong> Over-sample with replacement of minority class and calculate $\widehat{H_{resampled}}$ until $\widehat{H_{resampled}} \simeq \widehat{H_{major}}$</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Resampled class : Sample N(Number of majority class samples) from minority class samples
k = 0
While(k =&lt; max_iter) {
k = k+1
(H_resamp(X_1,â€¦,X_n)) = H_resamp(X_1)+â‹¯+H_resamp(X_n)
If (|H_resamp-H_major|&lt;Ïµ)  break  
}
</code></pre></div></div>

<h3 id="32-algorithm-2---under-sampling-of-majority-class">3.2. Algorithm 2 - Under-sampling of majority class</h3>

<p><strong>Step 1.</strong> Calculate Empirical entropy of minority class</p>

<p>$$
\widehat{H_{minor}} (X_1,â€¦,X_n ) \simeq \widehat{H_{minor}} (X_1) + â‹¯ + \widehat{H_{minor}} (X_n ) 
$$</p>

<p><strong>Step 2.</strong> Under-sample without replacement of minority class and calculate $\widehat{H_{resampled}}$ until $\widehat{H_{resampled}} \simeq \widehat{H_{minor}}$</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Resampled class : Sample M(Number of minority class samples) from majority class samples
k = 0
While(k =&lt; max_iter) {
k = k+1
(H_resamp(X_1,â€¦,X_n)) = H_resamp(X_1)+â‹¯+H_resamp(X_n)
If (|H_resamp-H_minor|&lt;Ïµ)  break  
}
</code></pre></div></div>

<h2 id="4-simulation-and-results">4. Simulation and Results</h2>

<p>ì‹œë®¬ë ˆì´ì…˜ì— ì‚¬ìš©í•œ ë°ì´í„°ëŠ” 3ê°€ì§€ì˜ imbalanced dataë¡œ numericê³¼ categorical variableì— ëŒ€í•˜ì—¬ ìˆ˜í–‰í•˜ì˜€ë‹¤. Binary classificationë§Œ ê³ ë ¤í–ˆìœ¼ë©°, ì‚¬ìš©í•œ ë¶„ì„ ë°©ë²•ì€ Decision Treeì´ë‹¤. Random samplingê³¼ SMOTEë¥¼ ì´ë²ˆì— ì œì•ˆí•˜ëŠ” entropyë¥¼ ê³ ë ¤í•œ sampling ë°©ë²•ê³¼ AUCë¡œ ë¹„êµí•˜ì˜€ë‹¤.</p>

<h3 id="hacide-dataset">hacide dataset</h3>

<p><strong>Xë³€ìˆ˜: 2ê°œ(numeric)</strong><br />
<strong>Yë³€ìˆ˜: class 1(2%), class 0(98%)</strong></p>

<p>2%ì˜ imbalanced dataì´ë©° ë³€ìˆ˜ê°„ correlationì´ 0.05ì •ë„ë¡œ ë‚®ì€ hacide datasetìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ì„ ìˆ˜í–‰í•œ ê²°ê³¼ì´ë‹¤.</p>

<p><img src="/assets/images/nonpara-resampling/2019-11-25-nonpara-resampling-entropy-19d06dcf.png" width="70%" /></p>
<center> <small> (train set : test  set = 80% : 20%) </small> </center>
<p><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Sampling method</th>
      <th style="text-align: right">AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">non-sampling(back)</td>
      <td style="text-align: right">0.600</td>
    </tr>
    <tr>
      <td style="text-align: left">SMOTE(green)</td>
      <td style="text-align: right">0.989</td>
    </tr>
    <tr>
      <td style="text-align: left">random oversampling(orange)</td>
      <td style="text-align: right">0.798</td>
    </tr>
    <tr>
      <td style="text-align: left">random undersampling(skyblue)</td>
      <td style="text-align: right">0.785</td>
    </tr>
    <tr>
      <td style="text-align: left">oversampling using entropy(blue)</td>
      <td style="text-align: right">0.798</td>
    </tr>
    <tr>
      <td style="text-align: left">**undersampling using entropy(red)**</td>
      <td style="text-align: right">**0.927**</td>
    </tr>
  </tbody>
</table>

<p>undersampling using entropy(red)ì´ SMOTE(green)ì— ë¹„ê²¬í• ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤. ì´ëŠ” 98%ì˜ ë‹¤ìˆ˜ ë°ì´í„°ë¥¼ 2%ë¡œ ì¤„ì¼ ë•Œ, entropyë¥¼ ê³ ë ¤í•˜ì—¬ íš¨ê³¼ì ìœ¼ë¡œ samplingì„ í–ˆìŒì„ ë°˜ì¦í•œë‹¤.</p>

<p>ì´ì— ë°˜í•´ oversampling using entropy(blue)ì€ random oversampling(orange)ê³¼ ê°™ì€ AUCë¥¼ ë³´ì´ê³  ìˆë‹¤. ì´ëŠ” ë³µì›ì¶”ì¶œë¡œ 2%ì˜ ì†Œìˆ˜ ë°ì´í„°ë¥¼ ë³µì œí•˜ë©´ entropyê°€ ë” ì´ìƒ ì˜¤ë¥´ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì¸ë° ë¹„ìŠ·í•œ ë°ì´í„°ë¥¼ ê°€ì§€ê²Œ ë˜ê¸° ë•Œë¬¸ì´ë¼ ì¶”ì¸¡ëœë‹¤.</p>

<p>(ë‹¤ë¥¸ ì‹œë®¬ë ˆì´ì…˜ ì—…ë°ì´íŠ¸ ì¤‘)</p>

<h2 id="5-discussion">5. Discussion</h2>
<p>ë°ì´í„°ì—ì„œ randomí•˜ê²Œ samplingí•˜ëŠ” ë°©ë²•ë³´ë‹¤ entropyë¥¼ ê³ ë ¤í•˜ëŠ” ë°©ë²•ì€ íŠ¹íˆ numerical variableì„ ë‹¤ìˆ˜ ê°€ì§„ ë°ì´í„°ì—ì„œ undersamplingì„ í•  ì‹œì— íš¨ê³¼ì ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ë°ì´í„°ì—ì„œ ì£¼ì–´ì§„ ëª¨ë“  ë³€ìˆ˜ê°€ ë…ë¦½ì¸ ê²ƒì€ ì•„ë‹ˆë¯€ë¡œ additiveí•˜ê²Œ entropyë¥¼ ë”í•˜ëŠ” ê²ƒì€ biasê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ â€˜Efficient Multivariate Entropy Estimation via K-nearest Neighbour distancesâ€™(TB Berett, 2016)ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì„ ì´ìš©í•˜ì—¬ ë³´ì™„í•œë‹¤ë©´ categorical variableì—ì„œë„ ì„±ëŠ¥ì´ ë” ê°œì„ ë  ê²ƒì´ë¼ ì˜ˆìƒí•œë‹¤.</p>

<h2 id="6-references">6. References</h2>
<ol>
  <li>â€˜Efficient Multivariate Entropy Estimation via K-nearest Neighbour distancesâ€™ (TB Berett, 2016)</li>
</ol>
:ET