I"°<p>[CS231n] Lecture-2-Image-Classification<br />
<a href="https://www.youtube.com/watch?v=OoUX-nOEjG0&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=2">2ê°• Video ë°”ë¡œê°€ê¸°</a></p>

<ul>
  <li>Contents
    <ul>
      <li>The data-driven approach</li>
      <li>K-nearest neighbor</li>
      <li>Linear classification I</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="image-classification-the-data-driven-approach">Image Classification: The data-driven approach</h2>

<p>ì§€ë‚œë²ˆì— ë§í–ˆë˜ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ëŒ€í•´ì„œ ë§í•´ë³´ë ¤ í•œë‹¤. dog, cat, truck, planeìœ¼ë¡œ fixed categories labeled ì‘ì—…ì˜ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ëŒ€í•´ì„œì´ë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125463362-e3573d2b-5966-4001-9988-1c80e68d604f.png" alt="image" /></p>

<p>ì´ë¯¸ì§€ ì¸ì‹ì€ ì‚¬ëŒì—ê²ŒëŠ” ì‰¬ìš´ ì¼ì´ì§€ë§Œ ì»´í“¨í„°ì—ê²ŒëŠ” êµ‰ì¥íˆ ì–´ë ¤ìš´ ì¼ì´ë‹¤. ì»´í“¨í„°ëŠ” ë‹¨ìˆœíˆ RGBì˜0 3ê°œì˜ layerì— 0~255 ì‚¬ì´ì˜ ìˆ«ìë¡œ ì¸ì‹í•˜ê¸° ë•Œë¬¸ì— ì‚¬ëŒì´ ì¸ì‹í•˜ëŠ” ê²ƒê³¼ëŠ” ì˜ë¯¸ì  ì°¨ì´(Semantic gap)ê°€ ì¡´ì¬í•œë‹¤. Viewpoint variation(ê°ë„), Illumination(ì¡°ëª…), deformation(ìì„¸), occlusion(ê°€ë ¤ì§), Background Clutter(ë°°ê²½ê³¼ì˜ ë¹„ìŠ·í•¨), Intraclass variation(ì¢…ì˜ ë‹¤ì–‘í•¨) ë“±ì˜ ë¬¸ì œë¥¼ ì‚¬ëŒì€ ì‰½ê²Œ êµ¬ë¶„í•˜ì§€ë§Œ ì»´í“¨í„°ì—ê²ŒëŠ” ì–´ë µë‹¤.</p>

<p>ì´ë¯¸ì§€ ë¶„ë¥˜ëŠ” ì–´ë–»ê²Œ í•  ê²ƒì¸ê°€?</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125211719-5c5d3d00-e2e3-11eb-863c-d29d9d96a7a1.png" alt="image" /></p>

<p>ë§Œì•½ â€˜ê³ ì–‘ì´ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜â€™ì„ ë§Œë“ ë‹¤ë©´, ì˜ˆì „ì²˜ëŸ¼ ì´ë¯¸ì§€ì˜ Edgeë¥¼ ì°¾ê³ , ê·¸ ì½”ë„ˆë¥¼ ì°¾ê³ , ê³ ì–‘ì´ë¼ë©´ ë¾°ì¡±í•œ ê·€ê°€ ë‘ ê°œ ìˆë‹¤ëŠ” ì´ëŸ¬í•œ <em>ëª…ì‹œì ì¸ ê·œì¹™</em>ì„ ì°¾ì•„ì„œ ì•Œê³ ë¦¬ì¦˜ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ í•´ë³´ì§€ ì•Šì•„ë„ ë‹¹ì—°íˆ í˜„ì‹¤ì— ì˜ ì‘ë™í•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125211733-6a12c280-e2e3-11eb-83fc-c372ddc47cd2.png" alt="image" /></p>

<p><strong>Data-Driven Approach</strong>ëŠ” ë§ê³  ë‹¤ì–‘í•œ classì˜ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ labelê³¼ í•¨ê»˜ í•™ìŠµí•´ì„œ ê°ê°ì˜ classì˜ ì‹œê°ì  ëª¨ìŠµì„ í•™ìŠµí•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ëª…ì‹œì ì¸ ê·œì¹™(í˜¹ì€ hard-code algorithm)ì´ ìˆëŠ”ê²Œ ì•„ë‹ˆë¼ ì£¼ì–´ì§„ training datasetì„ ë”°ë¡œ í•™ìŠµí•˜ëŠ” trainê³¼ ì´ë¯¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” predictì˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ ë§Œë“ ë‹¤.</p>

<hr />

<h2 id="k-nearest-neighbor-classifier">K-Nearest Neighbor classifier</h2>

<p>ê·¸ Data-driven approachì˜ í•œ ê¸°ë²•ì´ K-Nearest Neighbor Classifierì´ë‹¤. <em>ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í•™ìŠµí•´ì„œ</em> ë¶„ë¥˜í•˜ëŠ” ëª¨í˜•ì„ ë§Œë“ ë‹¤.</p>

<pre><code class="language-Python">import numpy as np

class NearestNeighbor(object):
  def __init__(self):
    pass

  def train(self, X, y):
    """ X is N x D where each row is an example. Y is 1-dimension of size N """
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """ X is N x D where each row is an example we wish to predict label for """
    num_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in range(num_test):
      # find the nearest training image to the i'th test image
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example

    return Ypred
</code></pre>

<p><img src="https://user-images.githubusercontent.com/62828866/125298318-fd3f0d00-e362-11eb-83cc-4cdc055d808e.png" alt="image" /></p>

<p>ìœ„ì²˜ëŸ¼ Kì™€ distance metric(L1, L2)ì— ë”°ë¼ trainingì´ ë‹¬ë¼ì§„ë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125298480-22338000-e363-11eb-84d1-b5633e4eb0e6.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/125465042-ba9d9df0-f558-40d3-ac52-7217b5f1b2d5.png" alt="image" /></p>

<p>ì—¬ê¸°ì„œ Kì™€ distance metricì€ <strong>Hyperparameter</strong>ë¼ê³  í•œë‹¤. dataì— ì¡´ì¬í•˜ëŠ” parameterê°€ ì•„ë‹ˆë¼ dataë¥¼ ë– ë‚˜ ë¬¸ì œ ì˜ì¡´ì ì¸ ê²°ì •ì‚¬í•­ì´ë‹¤. ì´ë•Œ HyperparameterëŠ” dataë¥¼ train, validation, testë¡œ ë‚˜ëˆˆ ë‹¤ìŒì— train dataë¡œ í›ˆë ¨í•œ hyperparameterë¥¼ validation dataìœ¼ë¡œ í‰ê°€í•´ì„œ ê³ ë¥´ê³ , ê·¸ê²ƒì„ test dataë¡œ í‰ê°€í•œë‹¤. K-fold validationì„ ì‚¬ìš©í•  ìˆ˜ ë„ ìˆë‹¤. (ê·¸ëŸ¬ë‚˜ K-fold validationì€ deep learningì—ì„œëŠ” ìì£¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.)</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125300021-9de1fc80-e364-11eb-9ade-8c9037c39c22.png" alt="image" /></p>

<p>ê·¸ëŸ¬ë‚˜ knnì€ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ì˜ ì•ˆì“´ë‹¤. ì™œëƒí•˜ë©´ test timeì´ ì˜¤ë˜ê±¸ë¦¬ê³ , distance metricì´ informativeí•˜ì§€ ì•Šê³ , ë˜í•œ classê°€ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ì°¨ì›ì˜ ì €ì£¼ê°€ ìƒê¸°ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<hr />

<h2 id="linear-classification">Linear classification</h2>

<p>Linear classificationì€ ì•„ì£¼ ë‹¨ìˆœí•˜ì§€ë§Œ CNNì— ë§¤ìš° ì¤‘ìš”í•˜ë‹¤!</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125465416-8358b39e-d545-460d-beec-7b04b5ce9938.png" alt="image" /></p>

<p>Linear classifierì„ ë ˆê³  ë¶ˆë¡ì— ë¹„ìœ í•  ìˆ˜ ìˆë‹¤. ì´ ë ˆê³  ë¸”ë¡ í•˜ë‚˜í•˜ë‚˜ê°€ linear classifierì¸ ê²ƒì´ê³  ì´ê²ƒì´ ëª¨ì—¬ì„œ í° Neural Networkê°€ ë˜ëŠ” ê²ƒì´ë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125465672-f4975e79-53a2-4193-8517-206b691ad911.png" alt="image" /></p>

<p>ì„ í˜•ë¶„ë¥˜ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ëª¨ìˆ˜ì  ëª¨ë¸(Parametirc model)ì´ë‹¤. Knnì€ ëª¨ìˆ˜ê°€ í•„ìš”í•˜ì§€ ì•Šì•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ì„ í˜•ë¶„ë¥˜ëŠ” trainì„ í†µí•´ Wë¥¼ ê°€ì§€ë©´ testì— Wë§Œìœ¼ë¡œ í”¼íŒ…í•  ìˆ˜ ìˆë‹¤. bëŠ” bias termì¸ë° ë§Œì•½ unbalanced dataë¼ì„œ ë§Œì•½ catì´ dogë³´ë‹¤ ë§ë‹¤ë©´ catì˜ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” bias termì´ í´ ê²ƒì´ë‹¤</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125470065-d7bfe460-787e-4bd2-84f6-e1e2a9cf3fb2.png" alt="image" /></p>

<p>ì´ë¯¸ì§€ classì— ëŒ€í•œ í•™ìŠµëœ Weightsë¥¼ ë³´ë©´(train í›„ ì—­ìœ¼ë¡œ êµ¬í•œ ì´ë¯¸ì§€) ê·¸ classì˜ ì´ë¯¸ì§€ì™€ ë¹„ìŠ·í•˜ê²Œ ìƒê²¼ë‹¤. ê·¸ëŸ¬ë‚˜ ì„ í˜•ì´ê¸° ë•Œë¬¸ì— ì¢Œìš° ëŒ€ì¹­ì´ë‹¤. (horseë¥¼ ë³´ë©´ ì˜ ë“œëŸ¬ë‚œë‹¤.)</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125470142-f5460cc7-1818-4613-9e90-7fb0b8585cd2.png" alt="image" /></p>

<p>Linear classifierì˜ ë¬¸ì œì ìœ¼ë¡œ, Odd even êµ¬ë¶„, Multimodal case, ê·¸ë¦¬ê³  í©ì–´ì§„ ê³µê°„ì— ì¡´ì¬í•˜ëŠ” ê²½ìš°ëŠ” ì„ í˜•ìœ¼ë¡œ êµ¬ë¶„í•˜ì§€ ëª»í•œë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  Linear ClassifierëŠ” ì•„ì£¼ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê³  í•´ì„í•˜ê¸° ì‰½ë‹¤ëŠ” ì¥ì ì´ íŒŒì›Œí’€í•˜ë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125471038-83ba0db8-dc58-4e6f-9737-16381f3e1f84.png" alt="image" /></p>

<p>ì§€ê¸ˆê¹Œì§€ linear score fucntionì„ ì•Œì•„ë³´ì•˜ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê³¼ì—° í•™ìŠµí•œ Weightsê°€ ì¢‹ì€ì§€ ë‚˜ìœì§€ëŠ” ì–´ë–»ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆì„ê¹Œ?</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125470452-d003b173-9d6f-4a4b-80a6-e7c3bf9c1fd4.png" alt="image" /></p>

<p>ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì¢‹ì€ Weightë€ ì–´ë–»ê²Œ ì •ëŸ‰í™”í•˜ëŠ”ê°€ì— ëŒ€í•œ Loss Function, ê·¸ë¦¬ê³  ê·¸ lossë¥¼ ì¤„ì—¬ì£¼ëŠ” Optimazation, í•¨ìˆ˜ í˜•íƒœë¥¼ ì¡°ì ˆí•˜ëŠ” CovNetì— ëŒ€í•´ì„œ ë°°ì›Œë³¼ ê²ƒì´ë‹¤.</p>
:ET