---
layout: post
title:  "YOLOX: Exceeding YOLO Series in 2021 (CVPR 2021)"
date:   2021-08-26
author: danahkim
tags: Object-Detection CVPR
categories: Computer-Vision
---



**YOLOX: Exceeding YOLO Series in 2021**, [PDF](https://arxiv.org/abs/2107.08430), Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun, arXiv 2021

------



![image](https://user-images.githubusercontent.com/62828866/130320478-f9098a97-1bcb-445c-ae0b-e32a2430012a.png)

  

안녕하세요. Real-time Object Detection에 혁명을 일으킨 YOLO의 최신 버전이 21년 7월에 release 됐습니다! 이름하야 **YOLOX**인데요, Stream Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)에서 YOLOX-L 모델 하나로 1등을 차지했다고 합니다. 오늘은 YOLOX를 리뷰해보겠습니다.

위에 그래프에 볼 수 있듯이 YOLOX는 기존 YOLOv5와 EfficientDet보다 더 좋은 성능을 보이고 있습니다. 종지부를 의미하는 듯한 막강해진 이름만큼, 어떠한 변화와 성능 개선이 있을지 새로 도입한 아이디어 위주로 논문을 통해 살펴보겠습니다.


## 1. Introduction

### Brief history of YOLO

![image](https://user-images.githubusercontent.com/62828866/131353784-0840ed10-5f5a-4b8f-9c32-c219b6d4f0c7.png)



본격적으로 YOLOX 논문 소개에 앞서 YOLO의 히스토리를 간략하게 소개하려고 합니다.

Object Detection은 2013년 R-CNN을 시작으로 많은 연구가 있었습니다. 초반에는 대부분 물체가 있을 만한 위치를 제안하는 Region Proposal이 주를 이뤘었는데, 이것은 시간이 꽤 소요되는 기법이라 당시에 real-time으로 동작이 어려워 실생활에 단독으로 적용되는 일이 거의 적었습니다. 이는 비행기 검색대에서 찍힌 X-ray 사진으로 물체 탐지하는 등 시간이 소요되는 곳에만 한정적으로 사용된 것이죠.

2015년 혜성처럼 **YOLO**가 등장합니다. YOLO(You Only Look Once: Unified, Real-Time Object Detection)의 저자 Redmon은 'You Only Look Once'의 뜻처럼, **이미지를 한번 보고 한번에 처리하는 방법을 제안합니다.** 정확도를 조금 포기하지만 빠른 처리 속도로 실시간 탐지가 가능해졌고 YOLO는 Real-time object detection으로 큰 주목을 받습니다. 그 후 버전을 거듭하며 성능을 개선했습니다.

그렇다면 YOLO가 어떤 식으로 발전했는지 주요 모델 위주로 간단히 살펴보겠습니다. 2015년 한번에 bounding box를 그리고 물체를 분류하는 **YOLO(2015)**가 제안됐고, **YOLOv2(2016)**에서는 미리 정해둔 형태의 박스를 bounding box의 후보로 사용하는 anchor box 개념을 도입합니다. 그 후 **YOLOv3(2018)**는 효율적으로 작은 object도 탐지가 가능한 모델이 제안합니다. 그리고 v3까지 연구했던 *원 저자 Redmon이 더 이상 연구에 참여하지 않았지만*, 연구자들이 다양한 아이디어를 시도했습니다.

### YOLOX Overview

#### Key concepts

YOLOX 논문에 따르면 주요한 변화는 크게 2가지입니다.

- <mark style='background-color: #fff5b1'> Anchor-free </mark> 방식의 YOLO 구조
- Object Detection을 위한 발전 기술 적용:
  - <mark style='background-color: #fff5b1'> Decoupled head </mark>
  - 발전된 label assignment 기법 사용: <mark style='background-color: #fff5b1'> SimOTA </mark>
  - 강력한 data augmentation: Mosaic, MixUp

Object Detection 분야에서 최근 2년동안 연구자들은 Anchor-free Detectors, 발전된 Label assignment기법, End-to-end detector에 주목하였으나, YOLOv4와 YOLOv5는 여전히 손수 training rule를 지정해줘야하는 Anchor-Based Detector입니다. 따라서 YOLOX는 위 주목된 기술을 적용하였습니다.

#### YOLOv3 baseline

YOLOX의 Baseline은 YOLOv4와 v5가 앵커 기반 파이프라인에 과도하게 최적화될 수 있다는 점을 고려하여 YOLOv3을 baseline으로 택했는데요, 그 중에서도 **Darknet53 backbone**과 **SPP layer**를 추가한 **YOLOv3-SPP 기반**입니다. 여기서 training 전략만 살짝 바꾸었다고 합니다. 

아래 표를 보시면, YOLOv3 baseline에서 decoupled head, strong augmentation 등으로 개선된 AP를 확인할 수 있는데요, baseline에 비해 AP가 약 9%p정도 개선이 되었습니다

<img src="https://user-images.githubusercontent.com/62828866/130322307-f779c20c-c856-486f-b28a-64a598689631.png" alt="image" style="zoom:67%;" />



## 2. Network design

### Architecture

- **Backbone : Darknet53**
- **Neck : SPP(Spatial Pyramid Pooling), FPN(Feature Pyramid Network)**
- **Head : YOLOv3 + Decoupled head** 

![yolox-architecture](https://user-images.githubusercontent.com/62828866/130464761-66d4221b-bc45-4a0b-b013-78cb45f1939a.png)

이제 YOLOX의 네트워크 구조에 대해 알아보겠습니다. YOLOX의 architecture를 위처럼 나타낼 수 있습니다. imput image가 **Darknet53**의 backbone을 통해 feature map을 추출합니다. 이때 공간 구조의 정보를 유지하는 **SPP Layer**를 추가하여 더 나은 성능을 기대할 수 있습니다. 

**FPN**을 통해 multi-scale feature map을 얻는데, 이는 feature에 대한 성숙도가 낮은 아래 feature map의 문제를 개선할 수 있습니다. 위에 high level extraction 정보를 아래 feature map에 더하는 방식으로, 아래 feature map이 가지고 있는 위치 정보를 같이 사용할 수 있습니다. 위에 작은 resolution의 feature map은 큰 물체를 detect하고, 아래에 resolution이 큰 feature map은 가장 작은 물체를 detect할 수 있죠.

그리고 **YOLOv3 기반**이지만 **분리된 2개의 head**로 구성되어 있습니다.



### Decoupled head

![Untitled 3](https://user-images.githubusercontent.com/62828866/131268358-f94dfb87-78a8-455e-9fe2-5a72958d7f66.png)

Object detection에서 Classification과 Regression이 서로 상충한다는 것은 잘 알려진 문제입니다. ([Rethinking Classification and Localization for Object Detection](https://arxiv.org/pdf/1904.06493.pdf) 논문에 보면 Fully connected head(for classification task)와 convolutional head가(for localization task) 서로 상충된다는 것에 대한 연구가 잘 되어있으니 참고하실 수 있습니다.) 

classification과 localization이 분리된 decoupled head는 널리 쓰이고 있지만 YOLO에서는 시도되지 않았습니다. YOLO는 원래 1개의 head로 구성되어서 anchor-box의 정보, object가 있을 확률에 대한 confidence score, object를 분류하는 classification이 같이 존재합니다. 

그러나 YOLOX는 **Classification과 Localization를 2개의 헤드로 분리합니다.** feature map의 채널을 256개로 줄이고, 3x3 conv layer를 지나는 2개의 브랜치를 추가합니다.

따라서 **Classification**에는 binary classification entropy loss를 사용해서 각각의 class에 대한 확률이 나오고 **Localization**에는 IOU loss를 사용한 bounding box에 대한 종횡거리가 나옵니다. 

논문의 결과에 따르면 Decoupled head가 원 YOLO head보다 수렴이 빠르며, end-to-end의 AP를 향상되었다고 합니다.

<img src="https://user-images.githubusercontent.com/62828866/130469660-8eccdfb2-f567-4b30-bae3-8e046502be06.png" alt="image" style="zoom:67%;" />

<img src="https://user-images.githubusercontent.com/62828866/131358372-6db84559-7357-444b-b9a4-4a114036a7f6.png" alt="image" style="zoom:67%;" />



### Anchor-free

YOLOX는 anchor-based design를 사용하지 않습니다. 그렇다면 anchor-based design은 어떤 게 문제가 될까요?

<img src="https://user-images.githubusercontent.com/62828866/129765842-1f93d532-a77f-42b5-bffa-1f67f32cc6e0.png" alt="image" style="zoom: 80%;" />
<center> <small> 이미지 출처: https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html </small> </center> <br/>

anchor-based detector는 많은 anchor-box들을 지정해두고 input image들이 CNN을 통과하여 feature map을 가집니다. 그리고 이 feature map은 bounding box를 예측하는데 쓰입니다. 그리고 각각의 grid는 anchor box들에 대응되며 물체의 offset을 찾도록 합니다. 그러나 이러한 방식은 단점이 존재합니다:

* anchor box 구성을 직접 선택하거나 clustering analysis를 사용하여 최적의 anchor box세트를 결정해야합니다. 그렇다면 domain에 specific해서 generalization 어려움이 있습니다.
* head의 복잡성과 예측의 수를 증가시킵니다. 임베디드 시스템이나 모바일 장치에서와 같이 리소스가 제한된 시스템에서 후처리를 수행하는 경우에 자원 친화적이지 않습니다.



![image](https://user-images.githubusercontent.com/62828866/131358877-15a7e122-91b2-4150-85f6-30bc6b29acb4.png)

이러한 이유로 YOLOX는 anchor가 없이 bounding box를 예측하는데, anchor-based detector의 단점을 개선한 anchor-free detector [FCOS](https://arxiv.org/abs/1904.01355) 방식을 사용합니다. FCOS는 어떤 포인트가 예측되면 그 포인트와 실제 ground truth와의 거리, 즉 사진에서 t,l,r,b 를 학습하여 예측합니다.

그리고 Bounding box의 regression 범위를 제한합니다. 피라미드 층의 scale에 따라 예측할 거리의 range가 미리 define되어 있어서, 높은 feature level(ex. 13x13)에서는 range가 크고, 낮은 feature level(ex. 52x52)에서는 range가 작게 regress 합니다.

즉, 위 오른쪽 사진에서 볼 때 저 파란색 point에서 낮은 feature level에서는 파란색 박스정도밖에 예측하지 못하도록 range를 작게 두어 큰 주황색 박스는 나올 수 없게 제한했습니다. 그리고 주황색 박스는 더 큰 feature level에서만 예측할 수 있습니다.

(참고.  [CenterNet - Objects as Points](https://arxiv.org/abs/1904.07850): anchor-free manner에 대해 더 알아볼 수 있다)



## 3. Training strategies

### SimOTA

이제 traing 방법에 대해 살펴보겠습니다. Label assignment는 어떤것이 positive이고 negative인지 샘플 데이터의 ground truth object에 할당해주는 것입니다. FCOS와 같은 anchor-free 방법은 ground truth object의 '중앙 박스 영역'을 해당하는 positive로 처리합니다. 그런데 이는 label 여러 개가 하나의 bounding box에 겹칠 때 문제가 됩니다.  아래 그림에서 붉은 점선 부분은 엄마와 아기의 애매한 경계이기도 하고, 또는 아예 백그라운드에 해당하는 부분도 있습니다. 따라서 단순히 중앙 부분을 positive로 할당하는 것이 아니라, simOTA라는 기법 사용해서 좀 더 정교하게 labeling합니다.

<img src="https://user-images.githubusercontent.com/62828866/130551791-d15a2682-c5b3-4077-b51f-111b8869d02e.png" alt="image" style="zoom:80%;" />

**OTA**([Optimal Transportation Algorithm](https://arxiv.org/abs/2103.14259))는 간단하게 설명해서 최적화 문제를 푸는 것입니다. 아래와 같은 Cost Plane이 있을 때 Sinkhorn-Knopp iteration을 통해서 깔끔하게 Optimal pi*를 찾을 수 있습니다.

<img src="https://user-images.githubusercontent.com/62828866/130551768-1c2b22a7-d695-46ed-a459-410f779ecc0f.png" alt="image" style="zoom:67%;" />

이제 한 이미지에 label이 여러개일 경우의 최적화를 하는 데에 이것을 적용합니다. 저자는 OTA의 경우 point by point가 아니라 global하게 assign을 해주기 때문에 저 빨간 점선 부분처럼 애매한 부분에 대해서도 깔끔하게 지정할 수 있다고 주장합니다.

그리고 이를 iteration없이 simple하게 적용한 것이 **simOTA**입니다. ground truth gi와 prediction pj의 cost 함수는 아래와 같습니다.

<img src="https://user-images.githubusercontent.com/62828866/130039847-7d0c401c-181b-4e7a-a570-0f8ee93bedc3.png" alt="image" style="zoom:67%;" />

(OTA에 대한 개념적인 이해를 설명하였으니 추가적으로 설명은 아래 논문 reference를 참고하시면 됩니다.)

### Multiple positives

Multiple positives는 Positive가 negative에 비해 매우 적은 imbalance를 줄이기 위해서 FCOS에서 'center sampling' 이라 불리는 방법을 사용하여 train하였습니다. Center point를 1x1에서 3x3로 확장한 positive를 주는 방법입니다. 

<img src="https://user-images.githubusercontent.com/62828866/130035076-601cbdeb-69dd-4054-8963-e1d2ca28cfa8.png" alt="image" style="zoom: 50%;" />



### Strong data augmentation

Mosaic와 MixUp 기법을 더한 데이터 증강 기법을 사용하여 성능을 개선시켰습니다.

<img src="https://user-images.githubusercontent.com/62828866/130033193-38f7d0de-1894-453b-96d5-6565ef498302.png" alt="image" style="zoom:67%;" />
<center> <small> 출처: YOLOv4: Optimal Speed and Accuracy of Object Detection </small> </center> <br/>

<img src="https://user-images.githubusercontent.com/62828866/130039548-50350cb6-9fea-4c4b-9466-118e73cf1d45.png" alt="image" style="zoom:67%;" />
<center> <small> 출처: BoF (https://arxiv.org/pdf/1902.04103.pdf) </small> </center> <br/>






## 4. Experimental results

<img src="https://user-images.githubusercontent.com/62828866/131361494-27146eb3-bbbd-41e1-84cb-a8f333b141b2.png" alt="image" style="zoom: 67%;" />

다양한 backbone과 다양한 사이즈, 그리고 경량화된 모델에서도 성능 개선이 된 결과를 확인할 수 있습니다.



<img src="https://user-images.githubusercontent.com/62828866/130037550-33edfc70-099a-475f-9314-95dfb1128722.png" alt="image" style="zoom:67%;" />

COCO dataset에 적용한 SOTA 모델과 비교한 성능입니다. SOTA 모델 중 정확도와 속도 측면에서 전반적으로 높은 성능을 보입니다. 특히 YOLOX-X의 AP가 51.2%로 가장 높습니다!

그리고 $AP_{S}$, $AP_{M}$, $AP_{L}$은 작은 물체/ 중간 물체/ 큰 물체에 대한 AP인데 보통 object detection이 작은 물체를 잘 못찾는 경향이 있습니다. 그래서 보면 오른쪽으로 갈 수록 AP가 높아집니다. YOLOX의 경우 큰 물체를 좀 더 잘 찾는 것 처럼 보이긴 합니다.

가장 아래 박스의 YOLOX의 모델을 보면 Baseline인 YOLOv3에 비해 속도와 정확도를 둘다 동시에 개선한 점이 눈에 띕니다. 결국 EfficentDet에 비견할만한 빠른 속도에, 그보다 훨씬 높은 정확도를 보이고 있는 꽤 괜찮은 모델이라 생각이 듭니다.



## 5. Conclusion

- 이 논문에서 anchor-free detector를 사용하여 YOLO 시리즈의 업데이트 버전으로 제안하였습니다.
- 최근 고도화된 detection 기술인 decoupled head, advanced label assigning strategy 등을 사용해 속도와 정확성이 전반적으로 훌륭한 성능을 보입니다.
- 가장 널리 쓰이는 YOLOv3의 architecture를 사용해서 AP(COCO)를 SOTA 최고로 개선시켰습니다.



이 논문을 접하면서 가장 재밌던 점은 사실 X라는 이름을 붙인 점인데요, X는 10이라는 의미를 가지고 있어서 10번째 시리즈를 의미하나 했지만 10번째 시리즈가 아니기 때문에, 버전 1, 2, 3으로 이어진 YOLO의 속편이 아닌 독자적인 다른 모델을 만들겠다는 의지로 해석했습니다. 예를 들면 YOLO에서 계속 사용되었던 Anchor를 사용하지 않고, 또한 head를 2개로 나눠눈 Architecture를 사용하고 있는 모습에서 알 수 있습니다.

기업과 학계의 Gap을 줄이는 방법론으로 교각이 되길 희망하는 저자의 바람처럼 Real-Time Object Detection이 실생활에 널리 사용되는 날이 기대하며 이상으로 논문 리뷰를 마치겠습니다. 감사합니다.



## References

[1] [YOLOv3: An Incremental Improvement(2018)](https://arxiv.org/pdf/1804.02767.pdf)

[2] [DC-SPP-YOLO: Dense Connection and Spatial Pyramid Pooling Based YOLO for Object Detection(2019)](https://arxiv.org/ftp/arxiv/papers/1903/1903.08589.pdf)

[3] [FCOS: Fully Convolutional One-Stage Object Detection(2019)](https://arxiv.org/pdf/1904.01355.pdf)

[4] [Rethinking Classification and Localization for Object Detection(2020)](https://arxiv.org/pdf/1904.06493.pdf)

[5] [OTA: Optimal Transport Assignment for Object Detection(2021)](https://arxiv.org/abs/2103.14259)

[6] [YOLOX review](https://aicurious.io/posts/papers-yolox)

[7] https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html
