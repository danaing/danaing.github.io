<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-03-25T16:53:41+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Danalog</title><subtitle></subtitle><author><name>Danalog</name></author><entry><title type="html">M1 mac - Jekyll blog 환경 세팅하기</title><link href="http://localhost:4000/etc/2022/03/14/M1-mac-jekyll-setting.html" rel="alternate" type="text/html" title="M1 mac - Jekyll blog 환경 세팅하기" /><published>2022-03-14T00:00:00+09:00</published><updated>2022-03-14T00:00:00+09:00</updated><id>http://localhost:4000/etc/2022/03/14/M1-mac-jekyll-setting</id><content type="html" xml:base="http://localhost:4000/etc/2022/03/14/M1-mac-jekyll-setting.html"><![CDATA[<blockquote>
  <p>맥린이의 에러와 함께하는 🍎 M1 mac에 Homebrew, Ruby, Jekyll 처음 설치하기</p>
</blockquote>

<p>Apple Silicon을 오랫동안 눈여겨 보다가, 새로나온 M1 Pro를 드디어 구매했다. 따라서 이번 글은 오랫동안 Window만 사용하다가 처음으로 했던 <mark style="background-color: #fff5b1"> M1 Mac에서 Github blog 사용을 위한 설치 방법 </mark>에 대해 기록해두었다.</p>

<ul>
  <li>설치리스트
    <ul>
      <li><mark style="background-color: #fff5b1"> Homebrew </mark></li>
      <li><mark style="background-color: #fff5b1"> Ruby </mark></li>
      <li><mark style="background-color: #fff5b1"> jekyll </mark></li>
    </ul>
  </li>
  <li>2021년형 M1 Pro</li>
  <li>macOS Monterey 기준</li>
</ul>

<h2 id="jekyll-blog란">Jekyll blog란?</h2>

<p>Jekyll은 정적 웹사이트 생성기로 Ruby 언어로 제작되었으며, 손쉽게 글을 쓸 수 있는 마크다운 글쓰기를 지원한다. GitHub Pages는 Jekyll로 구동되기 때문에 내가 원하는 사이트를 GitHub을 통해 무료로 만들 수 있다. 대부분 username.github.io라는 도메인으로 되어있는 테크 블로그가 그 예이다. 티스토리나 네이버블로그에 비해 원하는 대로 커스터마이징이 가능하고 다양한 테마를 쓸 수 있다는 엄청난 장점이 있다.</p>

<h2 id="homebrew-설치하기">Homebrew 설치하기</h2>

<p><a href="https://brew.sh/index_ko">Homebrew 홈페이지 바로가기</a></p>

<p>Homebrew는 macOS용(또는 Linux 시스템)에서 제공하지 않는 유용한 패키지 관리자이다. 홈페이지에 있는 명령어나 아래 명령어를 복사해서 터미널에 치면 Home Brew가 설치된다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>/bin/bash <span class="nt">-c</span> <span class="s2">"</span><span class="si">$(</span>curl <span class="nt">-fsSL</span> https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh<span class="si">)</span><span class="s2">"</span>
</code></pre></div></div>

<p>password 입력창에 사용 계정의 비밀번호를 입력한다. 그리고 RETURN을 누르면 설치가 완료된다.</p>

<p><img src="/assets/images/M1Mac-jekyll-setting/jekyll-setting-01.png" title="untitled" /></p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Warning: /opt/homebrew/bin is not in your PATH.
  Instructions on how to configure your shell for Homebrew
  can be found in the 'Next steps' section below.
</span><span class="c">....
</span><span class="gp">==&gt;</span><span class="w"> </span>Next steps:
<span class="go">- Run these two commands in your terminal to add Homebrew to your PATH:
</span><span class="gp">    echo 'eval "$</span><span class="o">(</span>/opt/homebrew/bin/brew shellenv<span class="o">)</span><span class="s2">"' &gt;&gt; /Users/awesome-d/.zprofile
</span><span class="gp">    eval "$</span><span class="s2">(/opt/homebrew/bin/brew shellenv)"</span>
<span class="go">- Run brew help to get started
</span></code></pre></div></div>

<p>그리고 아래처럼 PATH에 추가하라는 인스트럭션이 뜨면, 아래 두 줄을 추가로 터미널에 입력한다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>awesome-d@Danah-MacBookPro ~ % <span class="nb">echo</span> <span class="s1">'eval "$(/opt/homebrew/bin/brew shellenv)"'</span> <span class="o">&gt;&gt;</span> /Users/awesome-d/.zprofile
<span class="gp">$</span><span class="w"> </span>awesome-d@Danah-MacBookPro ~ % <span class="nb">eval</span> <span class="s2">"</span><span class="si">$(</span>/opt/homebrew/bin/brew shellenv<span class="si">)</span><span class="s2">"</span>                                                            
</code></pre></div></div>

<p>brew 명령어로 잘 설치가 되었는지 확인한다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>awesome-d@Danah-MacBookPro ~ % brew <span class="nt">--version</span>                                                             
</code></pre></div></div>

<p><img src="/assets/images/M1Mac-jekyll-setting/jekyll-setting-02.png" title="untitled" /></p>

<p>3.4.1 버전으로 설치가 완료되었다.</p>

<h2 id="ruby-설치하기">Ruby 설치하기</h2>

<p><a href="https://www.ruby-lang.org/ko/">Ruby 공식 홈페이지</a></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">rbenv</code>란?<br />
<a href="https://github.com/rbenv/rbenv#readme">rbenv</a>는 여러 종류의 Ruby를 설치할 수 있게 합니다. rbenv 자체는 Ruby 설치를 지원하지 않습니다만, <a href="https://www.ruby-lang.org/ko/documentation/installation/#ruby-build">ruby-build</a>라는 유명한 플러그인에서 Ruby를 설치할 수 있습니다. rbenv, ruby-build 모두 macOS, Linux나 다른 UNIX-계열 운영체제에서 사용가능합니다.</li>
  <li><code class="language-plaintext highlighter-rouge">ruby-build</code>란?<br />
 <a href="https://github.com/rbenv/ruby-build#readme">ruby-build</a>는 다른 버전의 Ruby를 임의의 디렉터리에 컴파일하고 설치할 수 있게 하는 <a href="https://www.ruby-lang.org/ko/documentation/installation/#rbenv">rbenv</a>의 플러그인입니다. ruby-build는 rbenv 없이 독자적으로 사용 할 수도 있습니다. macOS, Linux나 다른 UNIX-계열 운영체제에서만 사용 가능합니다.</li>
</ul>

<p>macOS에서는 <code class="language-plaintext highlighter-rouge">rbenv</code>를 통해 Ruby를 설치한다. <code class="language-plaintext highlighter-rouge">rbenv</code>는 여러개의 Ruby 버전을 독립적으로 관리할 수 있도록 하는 패키지이다. 그리고 <code class="language-plaintext highlighter-rouge">ruby-build</code> 플러그인도 설치한다.</p>

<p>아래는 Homebrew의 <code class="language-plaintext highlighter-rouge">brew</code> 명령어를 이용하여 <code class="language-plaintext highlighter-rouge">rbenv</code> 와 <code class="language-plaintext highlighter-rouge">ruby-build</code>를 설치하는 명렁어다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>brew <span class="nb">install </span>rbenv ruby-build
</code></pre></div></div>

<p><img src="/assets/images/M1Mac-jekyll-setting/jekyll-setting-03.png" title="untitled" /></p>

<p>아래 명령어를 통해 설치 가능한 Ruby 버전을 확인한다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>rbenv <span class="nb">install</span> <span class="nt">-l</span>
</code></pre></div></div>

<p><img src="/assets/images/M1Mac-jekyll-setting/jekyll-setting-04.png" title="untitled" /></p>

<p>Ruby는 최신 버전이 호환안되는 경우가 있기 때문에(에러의 원인 경우가 많았다ㅠㅠ) 안전하게 최신 버전보다 한두개 아래 버전을 설치할 것을 추천한다.</p>

<p>Ruby 원하는 버전을 아래 명령어를 통해 원하는 버전을 설치한다. 나는 <code class="language-plaintext highlighter-rouge">Ruby 2.7.5</code> 을 설치하였다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>rbenv <span class="nb">install</span> <span class="o">{</span>원하는 버전<span class="o">}</span>
</code></pre></div></div>

<p>그리고 아래 명령어를 쳐서 설치된 버전을 확인해본다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>rbenv versions
</code></pre></div></div>

<p><img src="/assets/images/M1Mac-jekyll-setting/jekyll-setting-05.png" title="untitled" /></p>

<p>별(*) 표시가 system 앞에 설정되어있는 것을 알 수 있고 방금 설치한 2.7.5 버전이 보인다. rbenv global 버전을 2.7.5로 변경한다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>rbenv global 2.7.5
<span class="gp">$</span><span class="w"> </span>rbenv rehash
</code></pre></div></div>

<p>버전을 다시 확인하면 2.7.5로 선택되어있다.</p>

<p><img src="/assets/images/M1Mac-jekyll-setting/jekyll-setting-06.png" title="untitled" /></p>

<h2 id="bundler-설치하기">Bundler 설치하기</h2>

<p>이제 Ruby의 Gem을 통해 Bundler를 설치하려고 하면… 아래 에러가 뜬다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>gem <span class="nb">install </span>bundler
<span class="go">ERROR:  While executing gem ... (Gem::FilePermissionError)
    You don't have write permissions for the /Library/Ruby/Gems/2.6.0 directory.
</span></code></pre></div></div>

<p>system Ruby를 사용하고 있기 때문에 권한이 없다는 에러이다.</p>
<h3 id="rbenv의-path를-추가하여-해결하기">rbenv의 PATH를 추가하여 해결하기</h3>

<p>rbenv의 PATH를 추가해야한다. <code class="language-plaintext highlighter-rouge">echo $SHELL</code> 을 통해 M1의 기본 Shell을 확인한 뒤 zsh를 열어 설정을 변경해준다.</p>

<p>아래 명령어를 쳐서 .zsh 설정 파일을 연다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>open ~/.zshrc
</code></pre></div></div>

<p>아래 두 줄을 추가해준다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">export PATH={$</span>Home<span class="o">}</span>/.rbenv/bin:<span class="nv">$PATH</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
<span class="gp">eval "$</span><span class="o">(</span>rbenv init -<span class="o">)</span><span class="s2">"
</span></code></pre></div></div>

<p>(혹은 터미널에서 <code class="language-plaintext highlighter-rouge">vi ~/.zshrc</code> 을 입력한 뒤에 두 줄을 추가해주고, :wq 를 입력해서 나가도 된다.)</p>

<p>변경한 설정을 적용시켜주는 명령어를 입력한다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">source</span> ~/.zshrc
</code></pre></div></div>

<p>그리고 다시 Bundler 설치 명렁어를 치고 아래처럼 뜨면 성공이다!!</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>gem <span class="nb">install </span>bundler
<span class="go">Fetching bundler-2.3.9.gem
Successfully installed bundler-2.3.9
Parsing documentation for bundler-2.3.9
Installing ri documentation for bundler-2.3.9
Done installing documentation for bundler after 0 seconds
1 gem installed
</span></code></pre></div></div>

<h2 id="jekyll-설치하기">Jekyll 설치하기</h2>

<p>이번에는 jekyll을 설치에서.. 또 에러와 조우했다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>gem <span class="nb">install </span>jekyll
<span class="go">Building native extensions. This could take a while...
ERROR:  Error installing jekyll:
	ERROR: Failed to build gem native extension.

    current directory: /Users/awesome-d/.rbenv/versions/2.7.5/lib/ruby/gems/2.7.0/gems/eventmachine-1.2.7/ext
/Users/awesome-d/.rbenv/versions/2.7.5/bin/ruby -I /Users/awesome-d/.rbenv/versions/2.7.5/lib/ruby/2.7.0 -r ./siteconf20220314-44873-emvjez.rb extconf.rb
</span><span class="c">.....
</span></code></pre></div></div>

<h3 id="command-line-tools-재설치하여-해결하기">Command Line Tools 재설치하여 해결하기</h3>

<p>이유인 즉슨, XCode 업데이트 후 Command Line Tools를 설치하지 않았기 때문이라한다. 다른 블로그를 참고하니 Jekyll 설치하기 전에 처음부터 Command Line Tools 설치를 권장하고 있었다.</p>

<p>Command Line Tools를 설치하는 <code class="language-plaintext highlighter-rouge">xcode-select --install</code> 명령어를 치면 이미 설치가 되어있고 업데이트를 하라고 뜬다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>xcode-select <span class="nt">--install</span>
<span class="go">xcode-select: error: command line tools are already installed, use "Software Update" to install updates
</span></code></pre></div></div>

<p>그래서 아예 삭제하고 다시 설치한다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">sudo rm</span> <span class="nt">-rf</span> /Library/Developer/CommandLineTools
<span class="gp">$</span><span class="w"> </span>xcode-select <span class="nt">--install</span>
</code></pre></div></div>

<p><img src="/assets/images/M1Mac-jekyll-setting/jekyll-setting-07.png" title="untitled" /></p>

<p>다시 jekyll을 설치한다</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>gem <span class="nb">install </span>jekyll
</code></pre></div></div>

<p><img src="/assets/images/M1Mac-jekyll-setting/jekyll-setting-08.png" title="untitled" /></p>

<p>jekyll 4.2.2 버전으로 설치 성공!</p>

<p>이제 cd로 디렉토리를 변경한 뒤 <code class="language-plaintext highlighter-rouge">jekyll new my-blog</code> 로 생성하거나, 원래 있던 프로젝트 폴더로 가서 아래 명령어로 bundler를 설치한다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>bundler <span class="nb">install</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">localhost:4000</code> 로컬 환경에서 돌아가는것을 확인한다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div></div>

<p><img src="/assets/images/M1Mac-jekyll-setting/jekyll-setting-09.png" title="untitled" /></p>

<p>드디어 jekyll blog 환경 세팅 끝!!!! 몇날 며칠 붙잡은 끝에 성공했다. Mac OS가 처음이라 하나하나 공부하고 검색하며 트러블슈팅하는데에 정말 오래 걸렸다.</p>

<p>편한 개발 환경과 터미널 커스터마이징의 필요성을 느껴서 다음은 iterm2와 Oh-My-Zhs를 설치하려한다.sss</p>

<h2 id="기타-이슈">기타 이슈</h2>

<h3 id="brew로-install할-때-에러">brew로 install할 때 에러</h3>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Cannot install under Rosetta 2 in ARM default prefix (/opt/homebrew)!
To rerun under ARM use:
    arch -arm64 brew install ...
To install under x86_64, install Homebrew into /usr/local.
</span></code></pre></div></div>

<p>brew 명령어 앞에 아래처럼 arch -arm64를 추가한다.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">arch</span> <span class="nt">-arm64</span> brew <span class="nb">install </span>rbenv
<span class="gp">$</span><span class="w"> </span><span class="nb">arch</span> <span class="nt">-arm64</span> gem <span class="nb">install</span> <span class="nt">--user-install</span> bundler jekyll
</code></pre></div></div>

<h3 id="기존-프로젝트에서-jekyll-실행-시-ruby-버전이-안맞는다는-에러">기존 프로젝트에서 jekyll 실행 시 Ruby 버전이 안맞는다는 에러</h3>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">rbenv: version `2.6.3' is not installed (set by /Users/awesome-d/Documents/GitHub/danaing.github.io/.ruby-version)
</span></code></pre></div></div>

<p>경로에 숨겨진 <code class="language-plaintext highlighter-rouge">.ruby-version</code> 파일을 찾아서 삭제해주었다.</p>

<h2 id="references">References</h2>

<ul>
  <li>Quick-start라고 하지만 전혀 Quick이 아닌 <a href="https://jekyllrb.com/">Jekyll 공식 사이트</a></li>
  <li>Ruby 환경변수 설정 트러블슈팅 참고 <a href="https://jojoldu.tistory.com/288">향로님 블로그</a></li>
</ul>]]></content><author><name>danahkim</name></author><category term="etc" /><category term="Jekyll" /><category term="Ruby" /><category term="MacOS" /><summary type="html"><![CDATA[맥린이의 에러와 함께하는 🍎 M1 mac에 Homebrew, Ruby, Jekyll 처음 설치하기]]></summary></entry><entry><title type="html">2021년 회고</title><link href="http://localhost:4000/diary/2022/01/20/review-2021.html" rel="alternate" type="text/html" title="2021년 회고" /><published>2022-01-20T00:00:00+09:00</published><updated>2022-01-20T00:00:00+09:00</updated><id>http://localhost:4000/diary/2022/01/20/review-2021</id><content type="html" xml:base="http://localhost:4000/diary/2022/01/20/review-2021.html"><![CDATA[<h2 id="2021년을-돌아보며">2021년을 돌아보며</h2>

<p><img src="/assets/images/review-2021/2021_review_001.jpg" alt="2021_review" width="50%" /></p>

<center> <small> 2021년 다이어리의 맨 앞장 </small> </center>
<p><br /></p>

<p>매년 그렇듯 또 한해가 지나갔지만 작년은 유독 나에게 있어 많은 변화가 있던 해이다. 2022년을 맞이하기에 앞서 작년 한 해를 회고를 해보려한다.</p>

<h2 id="나에게-2021년은">나에게 2021년은</h2>

<p>해를 시작하면 나는 그 해의 목표에 맞는 컨셉이 있다. 나는 작년 2021년을 시작하며, 2021년은 스스로 <strong>‘터닝 포인트’</strong>가 되는 해가 되길 바랬다.</p>

<p><img src="/assets/images/review-2021/2021_review_003.jpg" alt="2021_review" width="50%" /></p>

<center> <small> Turning Point를 의미하는 표지판 </small> </center>
<p><br /></p>

<blockquote>
  <p><strong>터닝 포인트(Turning Point)</strong>란 사전적으로 전환점을 말한다. 스포츠 경기에서는 경기의 승패를 좌우하는 플레이나 그 지점을 뜻한다. 즉 스포츠이든 인생이든 아주 중요한 변곡점이란 의미이다.</p>
</blockquote>

<p>작년은 나의 인생에서 <strong>중요한 전환점</strong>을 만들고 싶었다. 터닝 포인트라는 목표 아래 시간 순의 <strong>마일스톤</strong> 위주로 적어보려 한다.</p>

<h3 id="3월-정-반대편으로-이사하다">3월. 정 반대편으로 이사하다</h3>

<p><img src="/assets/images/review-2021/2021_review_011.jpg" alt="2021_review" width="50%" /></p>

<center> <small> 사진에는 다 담기지 않았지만 트럭 2대가 와서 실어갔다 </small> </center>
<p><br /></p>

<p><img src="/assets/images/review-2021/2021_review_012.jpg" alt="2021_review" width="50%" /></p>

<center> <small> 이사 당일 깨끗하게 비워진 방에 들어온 침대 </small> </center>
<p><br /></p>

<blockquote>
  <p><strong><em>“익숙한 것에서 멀어지기. 비우고 새로 시작하기”</em></strong></p>
</blockquote>

<p>오랜 익숙한 것에서 받는 따뜻한 위로도 있지만, <strong>새롭고 낯선 환경이 주는 신선한 활력</strong>이 있다. 20대의 시작부터 끝을 모두 보낸 곳에서의 생활을 끝내고 반대편으로 이사를 했다. 내가 다닌 대학교 주변에서만 살았기 때문에 그 동네를 눈감고도 다닐만큼, 버스 번호를 다 외울만큼 익숙한 동네였다. 그래서 그런지 나중에는 한번도 생활해보지 않은 동부권에 살아야겠다는 생각이 있었다. 새롭고 낯선 곳에서 처음 보는 버스 번호, 그리고 거꾸로 타야하는 2호선이라 처음에는 낯설어 한동안 거꾸로 타기도 했다. 익숙하던 생활의 관성과 바운더리를 깨뜨리는 낯선 변화로 새로운 활력을 얻을 수 있었다.</p>

<p>이사는 번거로운 일이지만, 그래도 이사가 주는 가장 큰 이점은 아마 <strong>비우고 새로 시작할 수 있다</strong>는 점일 것이다. 이사를 계약하고 온 날부터 짐을 정리하기 시작했는데 이 집에 너무 오래 살았기 때문에 사진에 다 담기지 않을 만큼 어마어마한 짐이 있었다. 아마 학부 1학년때 쯤 친구들이랑 동아리 박람회에서 받은 종이, 언젠지 모를 대동제때 받은 수건과 기념품들, 오답을 위해 시험 끝나고 가져온 시험지… 생각지도 못한 짐들이 구석 구석에 잠자고 있었다. 나는 묵은 것들을 하나 하나 비워내는데에만 한 달이 넘게 걸렸다. 가볍고 마음으로 이사를 하였고 새로 시작한다는 마음을 가지고 올 봄을 시작할 수 있었다.</p>

<h3 id="5월-computer-vision-공부를-하다">5월. Computer Vision 공부를 하다</h3>

<p><img src="/assets/images/review-2021/2021_review_021.jpg" alt="2021_review" width="50%" /></p>

<center> <small> 처음으로 입문한 논문 'YOLO'이다! <br />
    물론 이 논문을 이해하기 위해서 수-많은 레퍼런스 논문을 보며 공부했다 </small> </center>
<p><br /></p>

<blockquote>
  <p><strong><em>“해낼 수 있다는 자신감과 동기부여”</em></strong></p>
</blockquote>

<p>자율주행기술을 보면서 Computer Vision의 Object Detection쪽에 관심을 가지게 되었고 실제로 공부해보고 싶었다. 그래서 올해 딥러닝 관련 강의와 Computer Vision의 논문을 통해 공부를 하기로 결심했다. 그러나 <strong>내가 잘 할 수 있을까?</strong> 물론 수학적 기초는 있었지만 비전 쪽은 전혀 몰랐기 때문이다. 그런데 결과적으로는 내가 논문까지 면접에서 설명하기 시작하면서 <strong>해낼 수 있다는 자신감</strong>을 얻었고, 크고 작은 성취감을 통해 딥러닝 분야를 계속 공부하는데에 <strong>큰 동기부여</strong>가 되었다. 꾸준히 내가 좋아하는 일을 하고, 하고 싶었던 일을 배워가면서 나의 가치를 찾기도 했고, 목표에 매일 한발씩 내딛을 수 있었다.</p>

<p>퇴근 후 시간은 부족하다. 본격적으로 딥러닝 쪽의 저명한 대학원 강의를 수강하기로 계획하고서 계획대로 잘 되지 않아 좌절감을 느끼면서 퇴근 후 공부로만은 안되겠다고 생각했다. 그래서 출근 전 아침 시간을 공부하는 시간으로 만들기로 했다. 아침에 온전히 공부할 수 있는 시간을 마련하기 위해 <del>아침형 인간은 아니지만 강제</del> 미라클모닝과 공부시간을 실천했다. 공부량이 부족한 날에는 점심 시간을 활용해서 최대한 하루에 계획된 공부를 끝낼 수 있도록 했다. 그 결과 계획된 일과를 먼저 끝내고 홀가분하게 하루를 시작하는 기분, 자기 전에 하루를 마무리하며 느끼는 성취감을 모두 느낄 수 있었다. 그렇게 2달 동안 <strong>딥러닝 대학원 강의</strong>를 수강하고 정리하면서 기초를 쌓았다.</p>

<p>그리고는 관심있는 분야의 SOTA 논문을 읽고 직접 구현해보려는데.. 혼자로는 한계를 느꼈다. 한 논문을 이해하는데에 일주일 째 진도가 나가지 않으니 막다른 길에 도달한 기분이 들었다. 인터넷에서 논문 리뷰를 참고하고 있었는데, 문득 나처럼 같이 공부하고 싶은 사람이 있겠다는 생각이 들었고 모임을 찾아보았다. 시기적절하게 CV 논문 리뷰 스터디원을 모집하는 글을 발견했고 그리하여 3달 동안 <strong>CV 논문 스터디</strong>에 참여하였다. 처음에는 스터디에서 사람들의 발표가 무슨 말인지 이해하기도 벅찼다. 그리고 내가 발표하기로 고른 논문 1개를 이해하는데 꼬박 한 달을 다 쓰기도 했다.</p>

<p><img src="/assets/images/review-2021/2021_review_022.jpg" alt="2021_review" width="50%" /></p>

<center> <small> 처음으로 선정한 발표 논문 'YOLOX'를 스터디에서 발표한 자료 </small> </center>
<p><br /></p>

<p>3달 동안 매주 2시간씩 참여하고, 출근이며 퇴근이며 매일 논문을 붙잡고 고생한 결과, <strong>나중에는 점점 논문 이해에도 속도가 붙고, 사람들의 발표 내용도 감이 생기고 들리기 시작했다</strong>(!) 그렇게 눈과 귀가 트이고서는 내가 관심있는 분야의 컨퍼런스도 챙겨보게 되었고, 큰 줄기에서 어떻게 발전되고 있는지 감이 생겼으며, SOTA 논문이 어떤 식으로 접근하여 개선했는지가 예전보다 훨씬 잘 보이게 되었다. 처음에는 어렵고 힘들지만 <strong>시간을 들여 꾸준히 노력한다면 나도 잘 할 수 있다는 자신감</strong>을 얻을 수 있던 정말 소중한 경험이다.</p>

<h3 id="7월-서울-둘레길-157km를-완주하다">7월. 서울 둘레길 157km를 완주하다</h3>

<p><img src="/assets/images/review-2021/2021_review_031.jpg" alt="2021_review" width="50%" /></p>

<center> <small> 5월~7월은 산에서 정말 걷기 좋은 날씨였다 </small> </center>
<p><br /></p>

<p><img src="/assets/images/review-2021/2021_review_032.jpg" alt="2021_review" width="50%" /></p>

<center> <small> 스탬프 컬렉터. 빨간 우체통을 보면 달려갔다 </small> </center>
<p><br /></p>

<p><img src="/assets/images/review-2021/2021_review_033.jpg" alt="2021_review" width="50%" /></p>

<center> <small> 157km, 8개 코스에서 28개 스탬프 모으기 성공! </small> </center>
<p><br /></p>

<blockquote>
  <p><strong><em>“불필요한 한계를 설정하지 않기”</em></strong></p>
</blockquote>

<p>김연아 선수의 유명한 말이 있다. 훈련 중 무슨 생각을 하냐는 기자의 질문에 <strong>‘무슨 생각을 해.. 그냥 하는거지’</strong> 라고 대답한 인터뷰이다. 8개의 코스로 이루어진 157km에서 28개의 스탬프를 3달 만에 모으기 위해 몇 시간이 걸리는지, 며칠이 소요될지, 한 달에 몇 개의 스탬프를 받아야 가능할지 <strong>하나씩 따지고 계산할 시간에 그냥 하면 된다</strong>.</p>

<p>사실 올해 즐겨하던 등산에 동기부여를 위해 ‘서울 둘레길 마라톤’이 선착순으로 오픈한다는 글을 보고 아무것도 모르고 참여하기로 결제했다. 일을 벌린 이후에야 완주를 위해서는 3달이라는 기간 안에 157km의 코스 안에 있는 28개의 스탬프를 모두 모아야 한다는 것을 알게 되었는데, 처음에는 157km와 28개라는 숫자에 압도되어 내가 평소에 걸었던 거리를 기준으로 남은 기간동안 매주 몇 키로를 걸어야하는지, 한 번에 몇 시간을 써야하는지 따져보니 어마어마한 시간을 쏟아야했고 버거운 숫자로 느껴졌다.</p>

<p>그러나 이왕 시작한거 ‘묻고 따지지 말고 해보자!’라는 마음으로, 마음 속 계산기를 없애고 오늘의 날씨가 좋고 나쁜지, 하루 동안 얼마나 걸어야할지 계산 없이 매주말 묵묵히 산행에 나섰다. 그저 목표에 도달하기 위해 꾸준히  그 결과 모든 코스를 완주할 수 있었다! <strong>과거의 적당한 기준으로 나에게 불필요한 한계를 설정하는 것이 가능성을 제한하는 일이라고 느꼈다.</strong>  매 주말마다 서울 곳곳을 다니며 이렇게 멋진 곳을 직접 발로 밟고 알게되어서 오히려 감사하다는 생각이 든다. 체력적으로도 정신적으로도 한 단계 성장할 수 있었던 경험이다.</p>

<h3 id="12월-퇴사를-결심하다">12월. 퇴사를 결심하다</h3>

<p><img src="/assets/images/review-2021/2021_review_041.jpg" alt="2021_review" width="50%" /></p>

<center> <small> 마지막 출근일에 드렸던 초콜릿 </small> </center>
<p><br /></p>

<blockquote>
  <p><strong><em>“아무 것도 하지 않으면 아무 일도 일어나지 않는다.”</em></strong></p>
</blockquote>

<p>1월 부로 회사를 관두었다. 작년에 지원한 회사에 합격했기 때문이다. 2020년에 입사해 2년 동안 고생한 나 자신을 다독여주고 새로운 시작을 준비하려 한다. 나는 어떤 사람인지, 어떤 삶을 살고 있은지, 어떤 미래를 그리고 있는지 나에 대한 물음을 계속 던지면서 1년 간의 큰 고민 끝에 이직하기로 결심했다.</p>

<p>물론 그동안 들인 2년의 시간이 아깝지 않다면 거짓말이다. 그러나 <strong>해보고 싶은 일에 도전하고 직접 해보고 것</strong>과 안해보는 것은 앞으로 삶을 살아가는데에 가치관으로 큰 차이가 있다. 나는 나 스스로 <strong>‘성장’</strong>과 <strong>‘발전’</strong> 지향적인 일을 원하는 사람이라는 것을 시행착오 끝에 늦게 알았을 뿐이다. 이전 회사에서 내가 하는 업무와 장기적 커리어에 대한 불만이 있었고, 하고 싶은 일이 뚜렷해으니 이제 내가 할 일은 해보고 싶은 일에 도전하는 것이다. 불만족스러운 상황을 변화시키기 위해서는 무언가 해야한다. <strong>아무것도 하지 않으면 아무것도 변하지 않기 때문이다.</strong></p>

<p>새 회사로 내가 고려한 주요한 조건이다.</p>

<ul>
  <li><strong>개인적인 조건</strong>
    <ul>
      <li>전공을 살릴 수 있을 것</li>
      <li>전문성을 기를 수 있을 것</li>
      <li>일의 가치가 나의 인생의 가치와 방향이 같을 것</li>
      <li>자부심과 보람을 느낄 수 있을 것</li>
    </ul>
  </li>
  <li><strong>환경적인 조건</strong>
    <ul>
      <li>계속 학습하고 성장하는 문화일 것</li>
      <li>함께 일하는 동료에게 배울 점이 많은 곳일 것</li>
      <li>내가 하고 싶은 일을 할 수 있는 환경일 것</li>
      <li>현재보다 큰 마땅한 보상이 있을 것</li>
    </ul>
  </li>
  <li><strong>회사로서의 조건</strong>
    <ul>
      <li>기술(tech) 중심의 회사일 것</li>
      <li>리딩 기업일 것</li>
      <li>내수가 아닌 글로벌 기업일 것</li>
      <li>인류에 발전되는 일을 하는 기업일 것</li>
    </ul>
  </li>
</ul>

<p>물론 이 모든 조건을 만족하고 또 100% 만족하는 회사는 존재할 수가 없다는 걸 안다. 그래도 나만의 조건을 고민해보면서 내가 어떤 일을 하고 싶은지, 어떤 회사를 원하는지 그리고 나의 미래에 대해 선명히 그려볼 수 있었다.</p>

<hr />

<h3 id="-2021년-성찰">🔎 2021년 성찰</h3>

<ul>
  <li><strong>좋았던 점</strong>
    <ul>
      <li>새로운 분야, 컴퓨터 비전을 배웠고 공유하였다</li>
      <li>출근 전 아침 시간을 잘 활용했다</li>
      <li>새 회사로 새로운 커리어에 도전했다</li>
    </ul>
  </li>
  <li><strong>아쉬웠던 점</strong>
    <ul>
      <li>공부한 논문의 코드를 사용해서 실제 프로젝트를 진행해보지 못한 것</li>
      <li>데이터 경진대회에서 수상을 하지 못한 것</li>
      <li>하반기에 바쁘다는 이유로 운동을 하지 못해 체력 저하가 심했던 점</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="2022년을-시작하는-마음">2022년을 시작하는 마음</h2>

<p>눈에는 보이지 않지만 모든 순간이 차곡차곡 모여 내 미래를 만들고 있다. 그래서 분명한건 전 보다 확실히 성장했다는 것이다.</p>

<p><strong>인생은 속도보다 방향</strong>이라는 말이 있다. 자신의 인생은 다른 누구와의 경쟁이 아니라 <strong>나 자신과의 레이스</strong>이다. 나는 가끔 ‘도전하기에는 너무 늦은게 아닐까?’, 혹은 ‘나이가 너무 많은게 아닐까?’ 라고 남들과 나를 비교하며 스스로 불안하고 초조하게 만들기도 했다. ‘토끼와 거북이’의 이야기를 보면 거북이는 그저 자신의 능력대로 묵묵히 달린다. 토끼와 비교하지 않고 결승점만 보고 최선을 다해 달린 거북이는 결국 승리한다. 느리지만 꾸준히 본인의 페이스대로 성실하게 걸었기 때문이다. 나만의 강점을 살려 나의 페이스대로 목표를 향해 멈추지 말고 꾸준히 걸어가자. <strong>오직 배우고 성장하고 더 나은 사람이 되겠다는 결심이면 충분하다.</strong></p>

<ul>
  <li><strong>2022년 주요 계획 📌</strong>
    <ul>
      <li>Kaggle 메달에 도전!</li>
      <li>관심 분야의 SOTA 논문 꾸준히 리뷰하기</li>
      <li>사이드 프로젝트하기</li>
      <li>ADP 남은 실기 시험치기</li>
      <li>연간 독서 100권 달성! 책 아카이빙하기</li>
      <li>월 1회 이상 글쓰기 - 글쓰기를 취미로</li>
      <li>주 3회 이상 운동하기 - 필라테스, 헬스, 플라잉요가, 수영</li>
      <li>영어 회화 실력 키우기</li>
      <li>미라클모닝 실천하기</li>
    </ul>
  </li>
</ul>

<h4 id="새-출발을-응원해">새 출발을 응원해</h4>

<p>아무것도 정해지지 않은 것은 불안하고 두렵지만, 그만큼 <strong>무한한 가능성이 있다는 설렘</strong>이 공존한다. 2022년은 가능성을 믿고 <strong>Starting Line</strong>에 서서 부단히 목표를 향히 달리는 해이길 바란다. 내년에는 또 어떤 일들이 펼쳐질지, 내년의 나는 어떤 생각을 가진 사람이 될지, 2022년의 마지막 날에는 또 어떤 회고를 하고 있을지 내년의 나를 응원하고 기대한다!</p>]]></content><author><name>danahkim</name></author><category term="Diary" /><category term="Reflection" /><summary type="html"><![CDATA[2021년을 돌아보며]]></summary></entry><entry><title type="html">EfficientNetV2: Smaller Models and Faster Training (ICML 2021)</title><link href="http://localhost:4000/computer-vision/2021/09/15/efficientnetv2.html" rel="alternate" type="text/html" title="EfficientNetV2: Smaller Models and Faster Training (ICML 2021)" /><published>2021-09-15T00:00:00+09:00</published><updated>2021-09-15T00:00:00+09:00</updated><id>http://localhost:4000/computer-vision/2021/09/15/efficientnetv2</id><content type="html" xml:base="http://localhost:4000/computer-vision/2021/09/15/efficientnetv2.html"><![CDATA[<p><strong>EfficientNetV2: Smaller Models and Faster Training (ICML 2021)</strong>, <a href="https://arxiv.org/abs/2104.00298">PDF</a></p>

<p>Mingxing Tan, Quoc V. Le</p>

<hr />

<p>오늘 살펴볼 논문은 EfficientNetV2입니다. Google Brain에서 2년 전 공개한 EfficientNet은 다른 모델에 비해 <strong>빠른 학습 속도와 높은 성능</strong>으로 주목을 받아 현재까지 최고의 성능으로 널리 사용되었습니다. EfficientNet 관련 리뷰는 제가 작성한 <a href="https://danaing.github.io/computer-vision/2021/09/08/efficientnet.html">이곳</a>을 참고하시면 좋을 것 같습니다. 그리고 최근 Google Brain에서 학습 속도와 정확도를 개선하여 기존 모델의 성능을 크게 상회하는 <strong>EfficientNetV2(2021)</strong>를 발표했습니다. 본 논문을 전체적인 흐름에 따라 리뷰해보겠습니다.</p>

<p><img src="/assets/images/EfficientNetV2/0.png" /></p>

<h2 id="introduction">Introduction</h2>

<p>논문에서는 기존의 EfficientNet(이하 EfficientNetV1)에서 훈련을 느리게 하는 병목 현상을 3가지로 설명합니다. <strong>(1)</strong> 큰 이미지로 학습을 하면 학습 속도가 느립니다. <strong>(2)</strong> Depthwise convolution은 초기 레이어에서 학습 속도가 느립니다. <strong>(3)</strong> 모든 단계를 동일하게 scaling하는 것은 최선의 방법이 아닙니다. 따라서 EfficientNetV2(2021)는 이 3가지를 해결하기 위해서 <strong>adaptive regularization을 사용한 progressive learning</strong>, <strong>fused-MBConv</strong>, <strong>non-uniform scaling</strong> 방법을 소개합니다.</p>

<p>또한 본 논문에서 3가지 <strong>컨트리뷰션</strong>을 주장합니다.</p>

<ul>
  <li>
    <p><strong>더 작고 빠른 새로운 모델</strong>의 EfficientNetV2를 소개합니다. training-aware NAS와 scaling을 통해 발견된 EfficientNetV2는 훈련 속도와 파라미터 효율성 모두에서 이전 모델을 능가합니다.</p>
  </li>
  <li>
    <p>우리는 <strong>이미지 크기에 따라 정규화를 적응적으로 조정하는 Progressive learning</strong>을 제안합니다. 우리는 그것이 학습속도를 높이고, 동시에 정확도를 향상시킨다는 것을 보여줍니다.</p>
  </li>
  <li>
    <p>ImageNet, CIFAR, Cars 및 Flowers 데이터셋에서 기존 연구 모델보다 <strong>최대 11배 더 빠른 훈련 속도와 최대 6.8배 더 나은 파라미터 효율성</strong>을 보여줍니다.</p>
  </li>
</ul>

<p>그럼 어떻게 학습 속도와 정확도, 그리고 파라미터 효율성을 개선하였는지 중점적으로 살펴보겠습니다.</p>

<h2 id="efficientnetv2-design">EfficientNetV2 Design</h2>

<p>앞서 말씀드린 EfficientNetV1(2019)에서의 학습 병목 현상을 3가지를 연구하고 개선하는 방법을 소개합니다.</p>

<h3 id="1-training-with-very-large-image-sizes-is-slow">(1) Training with very large image sizes is slow</h3>

<p><strong>- 큰 이미지로 학습을 하면 학습 속도가 느립니다.</strong></p>

<p>EfficientNetV1(2019)의 큰 이미지 크기는 상당한 메모리 사용량을 가져옵니다. GPU/TPU의 메모리 총량은 고정되어 있기 때문에 더 작은 배치 크기로 학습하여야 하므로 훈련 속도가 크게 느려집니다. Table 2를 보면 더 작은 이미지 크기는 적은 연산량을 가져오고 더 큰 배치 크기를 가능하게 하기 때문에 학습 속도를 2.2배까지 향상시킵니다. 특히, 학습에 더 작은 이미지 크기의 사용은 더 좋은 정확도를 가져옵니다.</p>

<p><img src="/assets/images/EfficientNetV2/1.png" /></p>

<p>따라서 개선된 학습 방식인 <strong>이미지 크기와 정규화를 점진적으로 조정하는 Progressive learning</strong>에 대해 더 알아보겠습니다.</p>

<h4 id="progressive-learning">Progressive learning</h4>

<p>이미지 크기는 모델의 학습 시간에 중요한데, 이미지 크기를 변형시켜 학습하는 다양한 방법들이 있지만 종종 정확도의 저하를 야기하기도 합니다. 저자는 여기서 정확도의 저하가 불균형한 정규화에서 비롯된 것이라 가정합니다. 즉, 다른 이미지 크기를 학습할 때 고정된 정규화를 사용하는 것이 아니라 <strong>이미지 크기에 따라서 정규화의 강도도 조정해야 된다는 것</strong> 입니다.</p>

<p><img src="/assets/images/EfficientNetV2/2.png" /></p>

<p>위 테이블을 보면 이 가설을 검증하기 위해 모델을 다른 이미지 사이즈와 데이터 augmentation으로 학습시킨 결과입니다. 이미지 사이즈가 작을 때는 약한 augmentation에서 가장 성능이 좋고 반대로 이미지 사이즈가 크면 강한 augmentation에서 성능이 좋은 걸 확인할 수 있습니다. 즉 이 논문에서 저자는, 동일한 네트워크에서도 이미지 크기가 작을수록 네트워크 용량이 작아지므로 정규화가 더 약해질 필요가 있다고 주장합니다. 반대로 이미지 크기가 클수록 더 큰 용량에서 더 많은 연산을 수행하므로 과적합에 취약하기 때문에 더 강력한 정규화가 필요하다고 말합니다.</p>

<p><img src="/assets/images/EfficientNetV2/3.png" /></p>

<p>따라서 본 논문에서는 모델을 학습 시킬 때 이미지 사이즈에 따라서 조정된 정규화를 사용합니다. 위 그림은 이 논문의 progressive learning 학습 과정을 보여줍니다. 모델의 초기 학습 단계는 네트워크가 간단한 표현을 쉽고 빠르게 배울 수 있도록 더 작은 이미지와 약한 정규화로 네트워크를 학습합니다. 그리고 점차 이미지 크기를 늘리면서 더 강력한 정규화로 학습을 어렵게 합니다.</p>

<p><img src="/assets/images/EfficientNetV2/4.png" /></p>

<p>제안하는 progressive learning 학습은 기존의 모든 정규화에 호환되나, 본 논문에서는 다음 세 가지의 정규화에 대해서만 주로 다룹니다.</p>

<ul>
  <li><strong>Dropout</strong> (Srivastava et al., 2014): 채널을 무작위로 삭제하여 co-adaptation을 줄이는 네트워크 수준 정규화</li>
  <li><strong>RandAugment</strong> (Cubuk et al., 2020): 이미지별 데이터 증가</li>
  <li><strong>Mixup</strong> (Zhang et al., 2018): 교차 이미지 데이터 증가</li>
</ul>

<h3 id="2-depthwise-convolutions-are-slow-in-early-layers-but-effective-in-later-stages">(2) Depthwise convolutions are slow in early layers but effective in later stages</h3>

<p><strong>- Depthwise convolution은 초기 레이어에서 학습 속도가 느립니다.</strong></p>

<p>EfficientNetV1(2019)의 또 다른 병목은 광범위한 depthwise convolution에서 발생합니다. Depthwise convolutions은 MobileNetV1과 Xception에서 제안된 방법으로 효과가 입증되어 최신 모델에 이용하고 있습니다. Depthwise convolution은 Conv 연산량을 줄여 제한된 연산량 아래에서 더 많은 filter를 사용할 수 있는 이점이 있지만 modern accelerator를 사용하지 못하기 때문에 학습 속도가 느립니다. 따라서 <strong>Stage1-3에서는 MBConv(Mobile inverted bottleneck convolution) 대신에 Fused-MBConv를 사용</strong>합니다. Fused-MBConv는 MBConv의 1x1 conv + 3x3 depthwise conv 대신에 하나의 3x3 conv를 사용하는 것입니다. 모든 stage에 Fused-MBConv를 적용하니 오히려 학습 속도가 느려져 초기의 stage에만 Fused-MBConv를 사용합니다.</p>

<p><img src="/assets/images/EfficientNetV2/5.png" /></p>

<h3 id="3-equally-scaling-up-every-stage-is-sub-optimal">(3) Equally scaling up every stage is sub-optimal</h3>

<p><strong>- 모든 단계를 동일하게 확장하는 것은 최선이 아닙니다.</strong></p>

<p>EfficientNetV1(2019)은 depth, width, resolution의 3가지 scaling factor를 동시에 고려하는 <strong>compound scaling</strong>방법으로 모델 확장하는데, 모든 stage를 동일하게 확장했습니다. 하지만 stage가 학습 속도에 기여하는 정도가 다르다고 주장합니다. 따라서 <strong>stage가 증가할 수록 layer가 증가하는 정도를 높이는 non-uniform scaling 전략</strong>을 제안합니다. 이 때 증가하는 정도는 heuristic하게 정합니다. 또한 compound scaling에서 최대 이미지 크기를 제한하는데, progressive learning은 학습이 진행될 수록 입력 이미지 크기가 커지므로 메모리 사용량이 커져 학습 효율 감소를 최소화하기 위해서입니다.</p>

<h2 id="efficientnetv2-architecture">EfficientNetV2 Architecture</h2>

<p>EfficientNetV1(2019)와 마찬가지로 AutoML 방법인 신경망 아키텍쳐 탐색 (Neural Architecture Search; NAS) 기술을 사용하여 네트워크 구조를 탐색합니다. 저자는 이전 NAS를 기반으로 하되, 정확도, 파라미터 효율성 및 훈련 효율성을 공동으로 최적화하는 것을 목표로 합니다.  EfficientNet을 백본으로 사용하여 이 과정을 통해 찾은 <strong>EfficientNetV2-S</strong>의 아키텍쳐는 아래와 같습니다.</p>

<p><img src="/assets/images/EfficientNetV2/6.png" /></p>

<p>기존 EfficientNetV1(2019)과 주요 차이점은 다음과 같습니다.</p>

<ol>
  <li>MBConv와 새로 추가된 fused-MBConv를 초기 레이어에 사용합니다.</li>
  <li>메모리 접근 비용을 줄이기 위해 MBConv에 대해서 작은 expansion ratio를 사용합니다.</li>
  <li>3x3의 작은 커널 사이즈를 사용하고 그에 따른 receptive field 감소를 보완하기 위해서 더 많은 레이어를 사용합니다.</li>
  <li>파라미터와 메모리 접근 비용을 줄이기 위해 기존의 EfficientNetV1에서의 마지막 stride-1 단계를 완전히 제거합니다.</li>
</ol>

<h2 id="efficientnetv2-scaling">EfficientNetV2 scaling</h2>

<p>EfficientNetV2-S 모델을 기존의 compound scaling에 몇가지 추가적인 최적화를 더하여 EfficientNetV2-M/L 모델로 확장했습니다.</p>

<p><strong>추가된 최적화</strong></p>

<ol>
  <li>최대 추론 이미지 크기를 480으로 제한합니다. (너무 큰 이미지는 비싼 메모리와 학습 속도 비용을 발생시킴)</li>
  <li>뒤쪽 단계(stage 5, 6)에 점진적으로 더 많은 layer를 추가합니다. (런타임 비용없이 네트워크 용량을 늘리기 위해서)</li>
</ol>

<p><img src="/assets/images/EfficientNetV2/7.png" /></p>

<h2 id="performance-results">Performance Results</h2>

<p>기존 ConvNet과 비교한 결과는 다음 표에 나와있습니다.</p>

<p><img src="/assets/images/EfficientNetV2/8.png" /></p>

<p><img src="/assets/images/EfficientNetV2/9.png" /></p>

<p>Figure 5를 보면 동일 파라미터 수와 FLOPS수에서 기존 ConvNet보다 더 높은 성능을 보이고 있습니다. 기존 ConvNet에 비해 비슷한 정확도를 보이면서 파라미터 수와 FLOPS 수를 훨씬 절약한 것을 알 수 있습니다. 이 외에도 다양한 실험 결과들은 논문에서 추가로 확인하실 수 있습니다.</p>

<p><img src="/assets/images/EfficientNetV2/10.png" /></p>

<p>EfficientNetV1에서 compound scaling 방법이 휴리스틱한 계수에 크게 의존한다고 지적되었던 부분이 여전히 개선되지 않은 부분은 아쉬웠습니다. 그러나 기존 EfficientNetV1의 문제점을 개선하면서 더 월등한 성능과 동시에 학습 속도와 파라미터 수를 동시에 절약한 점이 인상깊습니다.</p>

<h2 id="references">References</h2>

<p>[1] <a href="https://arxiv.org/abs/2104.00298">EfficientNetV2: Smaller Models and Faster Training</a></p>

<p>[2] <a href="https://github.com/google/automl/tree/master/efficientnetv2">EfficientNetV2 Code</a></p>]]></content><author><name>DanahKim</name></author><category term="Computer-vision" /><category term="Image-Classification" /><category term="ICML" /><summary type="html"><![CDATA[EfficientNetV2: Smaller Models and Faster Training (ICML 2021), PDF]]></summary></entry><entry><title type="html">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019)</title><link href="http://localhost:4000/computer-vision/2021/09/08/efficientnet.html" rel="alternate" type="text/html" title="EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019)" /><published>2021-09-08T00:00:00+09:00</published><updated>2021-09-08T00:00:00+09:00</updated><id>http://localhost:4000/computer-vision/2021/09/08/efficientnet</id><content type="html" xml:base="http://localhost:4000/computer-vision/2021/09/08/efficientnet.html"><![CDATA[<h3 id="들어가며">들어가며</h3>

<p>ICML 2019에서 발표된 “<a href="https://arxiv.org/abs/1905.11946">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a>“은 현재(‘21년 9월)까지도 인용수 3천회 이상을 돌파하며 현재까지도 엄청난 인기를 가진 논문입니다. Image Classification 분야에서 월등한 성능과 빠른 학습으로 큰 반향을 가져왔습니다. 본 논문의 주요 아이디어와 결과 위주로 살펴보겠습니다.</p>

<p>Convolutional neural networks(CNNs)은 일반적으로 고정된 리소스 비용으로 개발된 다음, 더 많은 리소스를 사용할 수 있게 되면 정확도를 높이기 위해 확장(scale up)됩니다. 예컨대 ResNet (<a href="https://arxiv.org/abs/1512.03385">He et al, 2016</a>) 은 ResNet-18에서 layer를 ResNet-200까지 확장하고 GPipe (<a href="https://arxiv.org/abs/1811.06965">Huang et al, 2018</a>)는 기준 CNN을 4배 확장하여 84.3%의 ImageNet top-1 정확도를 달성했습니다. 기존의 모델 확장은 CNN의 깊이 또는 너비를 임의로 늘리거나 훈련에 더 큰 이미지 해상도를 사용하는 것입니다. 이러한 방법을 사용하면 정확도가 향상되지만 수동 조절이 필요하며 최상의 성능을 보여주기는 어렵습니다. 따라서 <strong>더 나은 정확도와 효율성을 위해 CNN을 확장하는 좋은 방법은 무엇인지 살펴봅니다.</strong></p>

<p>본 논문에서는 모델 확장 방법(Model scaling method)로 <strong>Compound Model Scaling</strong>이라는 compound coefficient를 사용하는 방법을 제안합니다. <strong>너비(width), 깊이(depth), 해상도(resolution)</strong>와 같은 네트워크 차원 중 1가지 임의로 확장하는 기존 방식과 달리, 이 방법은 고정된 계수를 사용하여 각 차원을 균일하게 확장합니다. 이 새로운 확장 방법과 <a href="https://arxiv.org/abs/1807.11626">AutoML</a>의 발전에 힘입어 EfficientNet이라는 <strong>더 작고 더 빠른</strong> 모델을 개발했다고 합니다.</p>

<h3 id="compound-model-scaling-a-better-way-to-scale-up-cnns">Compound Model Scaling: A Better Way to Scale Up CNNs</h3>

<p><img src="/assets/images/EfficientNet/image1.png" /></p>

<p>네트워크 확장 효과에 대해 이해하기 위해서 모델의 다른 차원의 확장 효과에 대해 연구하였습니다. 채널의 개수를 늘리는 <strong>width scaling</strong>, layer의 개수를 늘리는 <strong>depth scaling</strong>, input image의 해상도를 높이는 <strong>resolution scaling</strong>이 있습니다. ResNet (<a href="https://arxiv.org/abs/1512.03385">He et al., 2016</a>)은 depth를 통해 모델을 확장하는 대표적인 모델이며 MobileNet (<a href="https://arxiv.org/abs/1704.04861">Howard et al., 2017</a>)은 width를 조절하여 모델을 확장하는 대표적인 모델입니다. 각각의 차원을 개별적으로 확장하면 모델의 성능이 향상되지만, 모든 차원의 균형을 맞추어 동시에 확장하는 것이 전반적인 성능을 더 잘 향상시킨다는 것을 논문에서 실험으로 보여줍니다. 즉 모델을 확장하기 위해 3가지 차원을 함께 고려하는 <strong>Compound scaling</strong> 방법을 제안합니다.</p>

<p><img src="/assets/images/EfficientNet/image2.png" /></p>

<p>$\alpha$, $\beta$, $\gamma$ 는 grid search를 통해 고정 리소스 제약 조건(e.g., 2배 더 많은 FLOPS)에서 위 관계식을 만족하는 계수를 찾습니다. 여기서 width에 해당하는 $\beta$와 resolution에 해당하는 $\gamma$가 제곱인 이유는 FLOPS에 가로, 세로로 각각 곱해지기 때문에 제곱 배 증가하기 때문입니다. 처음에는 $\pi$를 1로 두어 계수를 찾습니다. 논문에서는 $\alpha=1.2$, $\beta=1.1$, $\gamma=1.15$를 사용했습니다. 그런 다음 $\pi$를 늘려준 계수를 적용하여 baseline 네트워크를 원하는 타겟 모델 사이즈 또는 계산 비용에 맞게 확장시킵니다.</p>

<p>이 Compound scaling 방법은 기존 scaling 방법에 비해 MobileNet(+1.4%) 및 ResNet(+0.7%)로 모델의 정확도를 향상시켰습니다.</p>

<h3 id="efficientnet-architecture">EfficientNet Architecture</h3>

<p>모델 확장(model scaling)은 baseline 네트워크에 크게 좌우됩니다. 따라서 정확도와 효율성(FLOPS)을 모두 최적화하는 <a href="https://arxiv.org/abs/1807.11626">AutoML MNAS 프레임워크</a>를 사용하여 Neural Architecture Search(NAS) 방법을 통해 새로운 baseline 네트워크를 개발했습니다. 그 아키텍쳐는 <strong>mobile inverted bottleneck convolution(MBConv)</strong>를 사용하는데, 이는 <a href="https://arxiv.org/abs/1801.04381">MobileNetV2</a> 및 <a href="https://arxiv.org/abs/1807.11626">MnasNet</a>과 유사한 구조입니다.</p>

<p><img src="/assets/images/EfficientNet/image3.png" /></p>

<p><img src="/assets/images/EfficientNet/image4.png" /></p>

<h3 id="efficientnet-performance">EfficientNet Performance</h3>

<p><img src="/assets/images/EfficientNet/image5.png" /></p>

<p><img src="/assets/images/EfficientNet/image6.png" /></p>

<p>EfficientNet을 ImageNet의 다른 기존 CNN과 성능을 비교한 결과입니다. EfficientNet 모델은 다른 모델에 비해 더 높은 정확도와 더 나은 효율성을 달성하여 파라미터 크기와 FLOP를 수십 배 줄인 것을 확인할 수 있습니다. EfficientNet-B7은 ImageNet에서 top-1 정확도 84.4%, top-5 정확도 97.1% 를 도달하는데 CPU 추론에서 8.4배 더 작고 속도는 6.1배 더 빠릅니다. 특히 널리 사용되는 ResNet-50과 비교하면 EfficientNet-B4와 비슷한 FLOP을 사용하지만, top-1 정확도가 76.3%에서 82.6%으로 크게 향상됐습니다.</p>

<p><img src="/assets/images/EfficientNet/image7.png" /></p>

<p>EfficientNet이 가장 유용하기 위해서는 다른 데이터 세트로도 전이해야 합니다. 이를 평가하기 위해 널리 사용되는 8개의 transfer learning 데이터 세트에서 EfficientNet을 시험한 결과가 위와 같습니다. EfficientNets는 CIFAR-100(91.7%), Flowers(98.8%)와 같은 8개 데이터 세트 중 5개에서 SOTA 정확도를 달성면서 게다가 이 때 파라미터 수는 최대 21배 더 적습니다. EfficientNet이 전이 또한 잘 된다는 것을 의미합니다.</p>

<h3 id="conclusion">Conclusion</h3>

<p>모델 효율성에 상당한 개선과 함께 압도적인 성능 향상을 제공한 EfficientNet이 미래의 컴퓨터 비전 태스크를 위한 새로운 토대 역할을 할 수 있을 것으로 기대합니다.</p>

<h3 id="references">References</h3>

<p>[1] <a href="https://arxiv.org/abs/1905.11946">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019)</a></p>

<p>[2] <a href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html">Google AI Blog</a></p>]]></content><author><name>danahkim</name></author><category term="Computer-vision" /><category term="Image-Classification" /><category term="ICML" /><summary type="html"><![CDATA[들어가며]]></summary></entry><entry><title type="html">YOLOX: Exceeding YOLO Series in 2021 (CVPR 2021)</title><link href="http://localhost:4000/computer-vision/2021/08/26/YOLOX.html" rel="alternate" type="text/html" title="YOLOX: Exceeding YOLO Series in 2021 (CVPR 2021)" /><published>2021-08-26T00:00:00+09:00</published><updated>2021-08-26T00:00:00+09:00</updated><id>http://localhost:4000/computer-vision/2021/08/26/YOLOX</id><content type="html" xml:base="http://localhost:4000/computer-vision/2021/08/26/YOLOX.html"><![CDATA[<p><strong>YOLOX: Exceeding YOLO Series in 2021</strong>, <a href="https://arxiv.org/abs/2107.08430">PDF</a>, Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun, arXiv 2021</p>

<hr />

<p><img src="https://user-images.githubusercontent.com/62828866/130320478-f9098a97-1bcb-445c-ae0b-e32a2430012a.png" alt="image" /></p>

<p>안녕하세요. Real-time Object Detection에 혁명을 일으킨 YOLO의 최신 버전이 21년 7월에 release 됐습니다! 이름하야 <strong>YOLOX</strong>인데요, Stream Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)에서 YOLOX-L 모델 하나로 1등을 차지했다고 합니다. 오늘은 YOLOX를 리뷰해보겠습니다.</p>

<p>위에 그래프에 볼 수 있듯이 YOLOX는 기존 YOLOv5와 EfficientDet보다 더 좋은 성능을 보이고 있습니다. 종지부를 의미하는 듯한 막강해진 이름만큼, 어떠한 변화와 성능 개선이 있을지 새로 도입한 아이디어 위주로 논문을 통해 살펴보겠습니다.</p>

<h2 id="1-introduction">1. Introduction</h2>

<h3 id="brief-history-of-yolo">Brief history of YOLO</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/131353784-0840ed10-5f5a-4b8f-9c32-c219b6d4f0c7.png" alt="image" /></p>

<p>본격적으로 YOLOX 논문 소개에 앞서 YOLO의 히스토리를 간략하게 소개하려고 합니다.</p>

<p>Object Detection은 2013년 R-CNN을 시작으로 많은 연구가 있었습니다. 초반에는 대부분 물체가 있을 만한 위치를 제안하는 Region Proposal이 주를 이뤘었는데, 이것은 시간이 꽤 소요되는 기법이라 당시에 real-time으로 동작이 어려워 실생활에 단독으로 적용되는 일이 거의 적었습니다. 이는 비행기 검색대에서 찍힌 X-ray 사진으로 물체 탐지하는 등 시간이 소요되는 곳에만 한정적으로 사용된 것이죠.</p>

<p>2015년 혜성처럼 <strong>YOLO</strong>가 등장합니다. YOLO(You Only Look Once: Unified, Real-Time Object Detection)의 저자 Redmon은 ‘You Only Look Once’의 뜻처럼, <strong>이미지를 한번 보고 한번에 처리하는 방법을 제안합니다.</strong> 정확도를 조금 포기하지만 빠른 처리 속도로 실시간 탐지가 가능해졌고 YOLO는 Real-time object detection으로 큰 주목을 받습니다. 그 후 버전을 거듭하며 성능을 개선했습니다.</p>

<p>그렇다면 YOLO가 어떤 식으로 발전했는지 주요 모델 위주로 간단히 살펴보겠습니다. 2015년 한번에 bounding box를 그리고 물체를 분류하는 <strong>YOLO(2015)</strong>가 제안됐고, <strong>YOLOv2(2016)</strong>에서는 미리 정해둔 형태의 박스를 bounding box의 후보로 사용하는 anchor box 개념을 도입합니다. 그 후 <strong>YOLOv3(2018)</strong>는 효율적으로 작은 object도 탐지가 가능한 모델이 제안합니다. 그리고 v3까지 연구했던 <em>원 저자 Redmon이 더 이상 연구에 참여하지 않았지만</em>, 연구자들이 다양한 아이디어를 시도했습니다.</p>

<h3 id="yolox-overview">YOLOX Overview</h3>

<h4 id="key-concepts">Key concepts</h4>

<p>YOLOX 논문에 따르면 주요한 변화는 크게 2가지입니다.</p>

<ul>
  <li><mark style="background-color: #fff5b1"> Anchor-free </mark> 방식의 YOLO 구조</li>
  <li>Object Detection을 위한 발전 기술 적용:
    <ul>
      <li><mark style="background-color: #fff5b1"> Decoupled head </mark></li>
      <li>발전된 label assignment 기법 사용: <mark style="background-color: #fff5b1"> SimOTA </mark></li>
      <li>강력한 data augmentation: Mosaic, MixUp</li>
    </ul>
  </li>
</ul>

<p>Object Detection 분야에서 최근 2년동안 연구자들은 Anchor-free Detectors, 발전된 Label assignment기법, End-to-end detector에 주목하였으나, YOLOv4와 YOLOv5는 여전히 손수 training rule를 지정해줘야하는 Anchor-Based Detector입니다. 따라서 YOLOX는 위 주목된 기술을 적용하였습니다.</p>

<h4 id="yolov3-baseline">YOLOv3 baseline</h4>

<p>YOLOX의 Baseline은 YOLOv4와 v5가 앵커 기반 파이프라인에 과도하게 최적화될 수 있다는 점을 고려하여 YOLOv3을 baseline으로 택했는데요, 그 중에서도 <strong>Darknet53 backbone</strong>과 <strong>SPP layer</strong>를 추가한 <strong>YOLOv3-SPP 기반</strong>입니다. 여기서 training 전략만 살짝 바꾸었다고 합니다.</p>

<p>아래 표를 보시면, YOLOv3 baseline에서 decoupled head, strong augmentation 등으로 개선된 AP를 확인할 수 있는데요, baseline에 비해 AP가 약 9%p정도 개선이 되었습니다</p>

<p><img src="https://user-images.githubusercontent.com/62828866/130322307-f779c20c-c856-486f-b28a-64a598689631.png" alt="image" style="zoom:67%;" /></p>

<h2 id="2-network-design">2. Network design</h2>

<h3 id="architecture">Architecture</h3>

<ul>
  <li><strong>Backbone : Darknet53</strong></li>
  <li><strong>Neck : SPP(Spatial Pyramid Pooling), FPN(Feature Pyramid Network)</strong></li>
  <li><strong>Head : YOLOv3 + Decoupled head</strong></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/62828866/130464761-66d4221b-bc45-4a0b-b013-78cb45f1939a.png" alt="yolox-architecture" /></p>

<p>이제 YOLOX의 네트워크 구조에 대해 알아보겠습니다. YOLOX의 architecture를 위처럼 나타낼 수 있습니다. imput image가 <strong>Darknet53</strong>의 backbone을 통해 feature map을 추출합니다. 이때 공간 구조의 정보를 유지하는 <strong>SPP Layer</strong>를 추가하여 더 나은 성능을 기대할 수 있습니다.</p>

<p><strong>FPN</strong>을 통해 multi-scale feature map을 얻는데, 이는 feature에 대한 성숙도가 낮은 아래 feature map의 문제를 개선할 수 있습니다. 위에 high level extraction 정보를 아래 feature map에 더하는 방식으로, 아래 feature map이 가지고 있는 위치 정보를 같이 사용할 수 있습니다. 위에 작은 resolution의 feature map은 큰 물체를 detect하고, 아래에 resolution이 큰 feature map은 가장 작은 물체를 detect할 수 있죠.</p>

<p>그리고 <strong>YOLOv3 기반</strong>이지만 <strong>분리된 2개의 head</strong>로 구성되어 있습니다.</p>

<h3 id="decoupled-head">Decoupled head</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/131268358-f94dfb87-78a8-455e-9fe2-5a72958d7f66.png" alt="Untitled 3" /></p>

<p>Object detection에서 Classification과 Regression이 서로 상충한다는 것은 잘 알려진 문제입니다. (<a href="https://arxiv.org/pdf/1904.06493.pdf">Rethinking Classification and Localization for Object Detection</a> 논문에 보면 Fully connected head(for classification task)와 convolutional head가(for localization task) 서로 상충된다는 것에 대한 연구가 잘 되어있으니 참고하실 수 있습니다.)</p>

<p>classification과 localization이 분리된 decoupled head는 널리 쓰이고 있지만 YOLO에서는 시도되지 않았습니다. YOLO는 원래 1개의 head로 구성되어서 anchor-box의 정보, object가 있을 확률에 대한 confidence score, object를 분류하는 classification이 같이 존재합니다.</p>

<p>그러나 YOLOX는 <strong>Classification과 Localization를 2개의 헤드로 분리합니다.</strong> feature map의 채널을 256개로 줄이고, 3x3 conv layer를 지나는 2개의 브랜치를 추가합니다.</p>

<p>따라서 <strong>Classification</strong>에는 binary classification entropy loss를 사용해서 각각의 class에 대한 확률이 나오고 <strong>Localization</strong>에는 IOU loss를 사용한 bounding box에 대한 종횡거리가 나옵니다.</p>

<p>논문의 결과에 따르면 Decoupled head가 원 YOLO head보다 수렴이 빠르며, end-to-end의 AP를 향상되었다고 합니다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/130469660-8eccdfb2-f567-4b30-bae3-8e046502be06.png" alt="image" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/131358372-6db84559-7357-444b-b9a4-4a114036a7f6.png" alt="image" style="zoom:67%;" /></p>

<h3 id="anchor-free">Anchor-free</h3>

<p>YOLOX는 anchor-based design를 사용하지 않습니다. 그렇다면 anchor-based design은 어떤 게 문제가 될까요?</p>

<p><img src="https://user-images.githubusercontent.com/62828866/129765842-1f93d532-a77f-42b5-bffa-1f67f32cc6e0.png" alt="image" style="zoom: 80%;" /></p>
<center> <small> 이미지 출처: https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html </small> </center>
<p><br /></p>

<p>anchor-based detector는 많은 anchor-box들을 지정해두고 input image들이 CNN을 통과하여 feature map을 가집니다. 그리고 이 feature map은 bounding box를 예측하는데 쓰입니다. 그리고 각각의 grid는 anchor box들에 대응되며 물체의 offset을 찾도록 합니다. 그러나 이러한 방식은 단점이 존재합니다:</p>

<ul>
  <li>anchor box 구성을 직접 선택하거나 clustering analysis를 사용하여 최적의 anchor box세트를 결정해야합니다. 그렇다면 domain에 specific해서 generalization 어려움이 있습니다.</li>
  <li>head의 복잡성과 예측의 수를 증가시킵니다. 임베디드 시스템이나 모바일 장치에서와 같이 리소스가 제한된 시스템에서 후처리를 수행하는 경우에 자원 친화적이지 않습니다.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/62828866/131358877-15a7e122-91b2-4150-85f6-30bc6b29acb4.png" alt="image" /></p>

<p>이러한 이유로 YOLOX는 anchor가 없이 bounding box를 예측하는데, anchor-based detector의 단점을 개선한 anchor-free detector <a href="https://arxiv.org/abs/1904.01355">FCOS</a> 방식을 사용합니다. FCOS는 어떤 포인트가 예측되면 그 포인트와 실제 ground truth와의 거리, 즉 사진에서 t,l,r,b 를 학습하여 예측합니다.</p>

<p>그리고 Bounding box의 regression 범위를 제한합니다. 피라미드 층의 scale에 따라 예측할 거리의 range가 미리 define되어 있어서, 높은 feature level(ex. 13x13)에서는 range가 크고, 낮은 feature level(ex. 52x52)에서는 range가 작게 regress 합니다.</p>

<p>즉, 위 오른쪽 사진에서 볼 때 저 파란색 point에서 낮은 feature level에서는 파란색 박스정도밖에 예측하지 못하도록 range를 작게 두어 큰 주황색 박스는 나올 수 없게 제한했습니다. 그리고 주황색 박스는 더 큰 feature level에서만 예측할 수 있습니다.</p>

<p>(참고.  <a href="https://arxiv.org/abs/1904.07850">CenterNet - Objects as Points</a>: anchor-free manner에 대해 더 알아볼 수 있다)</p>

<h2 id="3-training-strategies">3. Training strategies</h2>

<h3 id="simota">SimOTA</h3>

<p>이제 traing 방법에 대해 살펴보겠습니다. Label assignment는 어떤것이 positive이고 negative인지 샘플 데이터의 ground truth object에 할당해주는 것입니다. FCOS와 같은 anchor-free 방법은 ground truth object의 ‘중앙 박스 영역’을 해당하는 positive로 처리합니다. 그런데 이는 label 여러 개가 하나의 bounding box에 겹칠 때 문제가 됩니다.  아래 그림에서 붉은 점선 부분은 엄마와 아기의 애매한 경계이기도 하고, 또는 아예 백그라운드에 해당하는 부분도 있습니다. 따라서 단순히 중앙 부분을 positive로 할당하는 것이 아니라, simOTA라는 기법 사용해서 좀 더 정교하게 labeling합니다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/130551791-d15a2682-c5b3-4077-b51f-111b8869d02e.png" alt="image" style="zoom:80%;" /></p>

<p><strong>OTA</strong>(<a href="https://arxiv.org/abs/2103.14259">Optimal Transportation Algorithm</a>)는 간단하게 설명해서 최적화 문제를 푸는 것입니다. 아래와 같은 Cost Plane이 있을 때 Sinkhorn-Knopp iteration을 통해서 깔끔하게 Optimal pi*를 찾을 수 있습니다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/130551768-1c2b22a7-d695-46ed-a459-410f779ecc0f.png" alt="image" style="zoom:67%;" /></p>

<p>이제 한 이미지에 label이 여러개일 경우의 최적화를 하는 데에 이것을 적용합니다. 저자는 OTA의 경우 point by point가 아니라 global하게 assign을 해주기 때문에 저 빨간 점선 부분처럼 애매한 부분에 대해서도 깔끔하게 지정할 수 있다고 주장합니다.</p>

<p>그리고 이를 iteration없이 simple하게 적용한 것이 <strong>simOTA</strong>입니다. ground truth gi와 prediction pj의 cost 함수는 아래와 같습니다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/130039847-7d0c401c-181b-4e7a-a570-0f8ee93bedc3.png" alt="image" style="zoom:67%;" /></p>

<p>(OTA에 대한 개념적인 이해를 설명하였으니 추가적으로 설명은 아래 논문 reference를 참고하시면 됩니다.)</p>

<h3 id="multiple-positives">Multiple positives</h3>

<p>Multiple positives는 Positive가 negative에 비해 매우 적은 imbalance를 줄이기 위해서 FCOS에서 ‘center sampling’ 이라 불리는 방법을 사용하여 train하였습니다. Center point를 1x1에서 3x3로 확장한 positive를 주는 방법입니다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/130035076-601cbdeb-69dd-4054-8963-e1d2ca28cfa8.png" alt="image" style="zoom: 50%;" /></p>

<h3 id="strong-data-augmentation">Strong data augmentation</h3>

<p>Mosaic와 MixUp 기법을 더한 데이터 증강 기법을 사용하여 성능을 개선시켰습니다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/130033193-38f7d0de-1894-453b-96d5-6565ef498302.png" alt="image" style="zoom:67%;" /></p>
<center> <small> 출처: YOLOv4: Optimal Speed and Accuracy of Object Detection </small> </center>
<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/130039548-50350cb6-9fea-4c4b-9466-118e73cf1d45.png" alt="image" style="zoom:67%;" /></p>
<center> <small> 출처: BoF (https://arxiv.org/pdf/1902.04103.pdf) </small> </center>
<p><br /></p>

<h2 id="4-experimental-results">4. Experimental results</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/131361494-27146eb3-bbbd-41e1-84cb-a8f333b141b2.png" alt="image" style="zoom: 67%;" /></p>

<p>다양한 backbone과 다양한 사이즈, 그리고 경량화된 모델에서도 성능 개선이 된 결과를 확인할 수 있습니다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/130037550-33edfc70-099a-475f-9314-95dfb1128722.png" alt="image" style="zoom:67%;" /></p>

<p>COCO dataset에 적용한 SOTA 모델과 비교한 성능입니다. SOTA 모델 중 정확도와 속도 측면에서 전반적으로 높은 성능을 보입니다. 특히 YOLOX-X의 AP가 51.2%로 가장 높습니다!</p>

<p>그리고 $AP_{S}$, $AP_{M}$, $AP_{L}$은 작은 물체/ 중간 물체/ 큰 물체에 대한 AP인데 보통 object detection이 작은 물체를 잘 못찾는 경향이 있습니다. 그래서 보면 오른쪽으로 갈 수록 AP가 높아집니다. YOLOX의 경우 큰 물체를 좀 더 잘 찾는 것 처럼 보이긴 합니다.</p>

<p>가장 아래 박스의 YOLOX의 모델을 보면 Baseline인 YOLOv3에 비해 속도와 정확도를 둘다 동시에 개선한 점이 눈에 띕니다. 결국 EfficentDet에 비견할만한 빠른 속도에, 그보다 훨씬 높은 정확도를 보이고 있는 꽤 괜찮은 모델이라 생각이 듭니다.</p>

<h2 id="5-conclusion">5. Conclusion</h2>

<ul>
  <li>이 논문에서 anchor-free detector를 사용하여 YOLO 시리즈의 업데이트 버전으로 제안하였습니다.</li>
  <li>최근 고도화된 detection 기술인 decoupled head, advanced label assigning strategy 등을 사용해 속도와 정확성이 전반적으로 훌륭한 성능을 보입니다.</li>
  <li>가장 널리 쓰이는 YOLOv3의 architecture를 사용해서 AP(COCO)를 SOTA 최고로 개선시켰습니다.</li>
</ul>

<p>이 논문을 접하면서 가장 재밌던 점은 사실 X라는 이름을 붙인 점인데요, X는 10이라는 의미를 가지고 있어서 10번째 시리즈를 의미하나 했지만 10번째 시리즈가 아니기 때문에, 버전 1, 2, 3으로 이어진 YOLO의 속편이 아닌 독자적인 다른 모델을 만들겠다는 의지로 해석했습니다. 예를 들면 YOLO에서 계속 사용되었던 Anchor를 사용하지 않고, 또한 head를 2개로 나눠눈 Architecture를 사용하고 있는 모습에서 알 수 있습니다.</p>

<p>기업과 학계의 Gap을 줄이는 방법론으로 교각이 되길 희망하는 저자의 바람처럼 Real-Time Object Detection이 실생활에 널리 사용되는 날이 기대하며 이상으로 논문 리뷰를 마치겠습니다. 감사합니다.</p>

<h2 id="references">References</h2>

<p>[1] <a href="https://arxiv.org/pdf/1804.02767.pdf">YOLOv3: An Incremental Improvement(2018)</a></p>

<p>[2] <a href="https://arxiv.org/ftp/arxiv/papers/1903/1903.08589.pdf">DC-SPP-YOLO: Dense Connection and Spatial Pyramid Pooling Based YOLO for Object Detection(2019)</a></p>

<p>[3] <a href="https://arxiv.org/pdf/1904.01355.pdf">FCOS: Fully Convolutional One-Stage Object Detection(2019)</a></p>

<p>[4] <a href="https://arxiv.org/pdf/1904.06493.pdf">Rethinking Classification and Localization for Object Detection(2020)</a></p>

<p>[5] <a href="https://arxiv.org/abs/2103.14259">OTA: Optimal Transport Assignment for Object Detection(2021)</a></p>

<p>[6] <a href="https://aicurious.io/posts/papers-yolox">YOLOX review</a></p>

<p>[7] https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html</p>]]></content><author><name>danahkim</name></author><category term="Computer-vision" /><category term="Object-Detection" /><category term="CVPR" /><summary type="html"><![CDATA[YOLOX: Exceeding YOLO Series in 2021, PDF, Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun, arXiv 2021]]></summary></entry><entry><title type="html">CS231n | 6강 - Training Neural Networks part I</title><link href="http://localhost:4000/deep-learning/2021/08/11/cs231n-06.html" rel="alternate" type="text/html" title="CS231n | 6강 - Training Neural Networks part I" /><published>2021-08-11T00:00:00+09:00</published><updated>2021-08-11T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning/2021/08/11/cs231n-06</id><content type="html" xml:base="http://localhost:4000/deep-learning/2021/08/11/cs231n-06.html"><![CDATA[<p>[CS231n] Lecture-6-Training-Neural-Networks-part-I<br />
<a href="https://www.youtube.com/watch?v=wEoyxE0GP2M&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=6">6강 Video 바로가기</a></p>

<ul>
  <li>Contents
    <ul>
      <li>Activation functions</li>
      <li>Data Preprocessing</li>
      <li>Weight Initialization</li>
      <li>Batch Normalization</li>
      <li>Babysitting the Learning Process</li>
      <li>Hyperparameter Optimization</li>
    </ul>
  </li>
</ul>

<hr />

<p><img src="https://user-images.githubusercontent.com/62828866/128451130-2b33f319-53ef-41df-9863-0b73a5c06b91.png" alt="image" /></p>

<p>이번 6강에서는 Neural Networks를 어떻게 train하는지에 대한 실제적인 디테일한 이슈들에 대해 배운다. 다양한 활성화 함수(activation function), 데이터 전처리(data preprocessing), 가중치 초기화(weight initialization), 그리고 배치 정규화(batch normalization)에 대해 다룰 것이다. 또한 학습 과정을 전반적으로 모니터링하는 방법과 하이퍼파라메터를 고르는 방법에 대해서도 배운다.</p>

<h2 id="activation-functions">[Activation Functions]</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/128451400-e9aec3b0-4930-45c0-963e-f50f016c9a69.png" alt="image" /></p>

<p>W’x+b의 선형합의 output은 파란색 cell body라 적혀있는 부분에서 <strong>activation function을 통해 f(W’x+b)로 활성화된다.</strong> 이때 사용되는 activation function은 앞서 살펴본 sigmoid와 ReLU 이외에 대표적인 몇가지 함수가 더 존재한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128451424-16822648-0ce0-4cc4-87e2-c7ce2a58c293.png" alt="image" /></p>

<p>위의 6가지의 비선형 함수들의 트레이드 오프에 대해 살펴본다.</p>

<h3 id="1-sigmoid">1. Sigmoid</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/128451813-6ddee839-514b-45c2-bbc1-924d0cdf769b.png" alt="image" />
먼저 Sigmoid 함수는 해석이 쉽다는 장점이 있다. 그러나 3가지 문제점이 있다.
<strong>첫번째 문제점</strong>은 그래디언트를 죽인다는 것이다(Saturated neurons “kill” the gradients)</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128451888-0f7bdef4-6530-4845-abdf-bf6b650205ba.png" alt="image" /></p>

<p>여기서 x=-10이거나 x=10이면 사실상 플랫해서 그래디언트가 0에 가까워진다. 그래서 전체 그래디언트를 0으로 죽인다는 문제점이 있다. (따라서 가운데 기울기가 있는 부분을 active region of sigmoid라고 하고 좌우 끝 부분을 saturated regime이라고 한다.)</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128451937-155dad33-7c7a-466a-a10a-9273b56fb0a2.png" alt="image" /></p>

<p><strong>두번째 문제점</strong>은 시그모이드 결과가 원점에 위치하지 않는다는 것이다. (Sigmoid outputs are not zero-centered) 그래프의 y축을 보면 sigmoid 함수의 결과 값이 모두 양수이기 때문에 0을 중심으로 하고 있지 않다는 것이다! 이것은 convergence가 느려지게 한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128452000-7e8fac95-299c-4dfd-a147-f724861cad9c.png" alt="image" /></p>

<p>sigmoid function을 지나면 무조건 양수의 값이 나오고 이 양수 값이 다음 뉴런에 input으로 들어간다. 이렇게 항상 양수의 값이 x input으로 들어간다면 gradient에 어떤 영향을 미치는지 보면
$$
\frac{df}{dw_i}=x_{i} <br />
\frac{dL}{dw_i}=\frac{dL}{df}<em>\frac{df}{dw_{i}}=\frac{dL}{df}</em>x_{i}
$$
여기서 x는 항상 양수이므로 W의 그라이언트는 모두 양수이거나 모두 음수여야한다. 이것을 w에 대한 2차원 그래프의 예시로 표현해보면, 녹색으로 칠해진 방향으로만 움직일 것이다. optimal w vector인 파란색 방향으로 이동해야하는데 지그재그로 이동하게 되어서 결과적으로 convergence가 매우 느리게된다! 따라서 zero-center가 아닌 activation function은 convergence를 느리게 한다는 단점을 알 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128452026-b645936c-196f-40d9-bccd-ae3f2de32875.png" alt="image" /></p>

<p><strong>세번째 문제점</strong>은 Exponential function의 연산이 비싸다는 점이다. (Exp() si a bit compute expensive)</p>

<p>이러한 이유로 sigmoid function는 거의 쓰이지 않는다.</p>

<h3 id="2-tanh">2. tanh</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/128452100-3ef7dff0-0b8b-42c3-845b-6960fb2c5015.png" alt="image" /></p>

<p>tanh 함수는 zero-center이기는 하나 여전히 좌우 끝 부분에서 그래디언트를 죽인다는 단점이 있다.</p>

<h3 id="3-relu">3. ReLU</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/128452164-4dd85d36-f75d-421e-bfb6-1b16af709c76.png" alt="image" /></p>

<p>ReLU는 현재 <em>디폴트로 택하는 activation function</em>이다. 그렇지만 이것의 특징과 한계점을 제대로 알아야 나중에 문제가 생겼을 때 디버깅이 가능하므로 잘 알고 있어야한다.</p>

<ul>
  <li>양수에서 전혀 saturated되지 않고</li>
  <li>6배나 연산이 빠르며</li>
  <li>진짜 뉴런이 ReLU 함수와 비슷하게 반응하여 생물학적 타당성이 더 높다고 한다.</li>
</ul>

<p>따라서 2012년에 나온 Alexnet부터 ReLU가 더 많이 쓰였다.</p>

<p>그러나 여기에도 3가지 문제가 존재한다.</p>

<ul>
  <li>zero-center가 아니기 때문에, 여전히 convergence가 느리고</li>
  <li>x가 음수일 때 gradient가 여전히 죽게 되며</li>
  <li>Dead ReLU라는 ReLU가 활성화되지 않는 구역이 있다는 것이다.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/62828866/128452406-8af65c1b-650a-4e22-9c6c-903294808cf7.png" alt="image" /></p>

<p>여기서 <strong>Dead ReLU</strong>라는 개념에 대해 살펴보겠다. Dead ReLU란 절대 업데이트가 되지 않는 것을 뜻한다. 회색의 별모양 부분을 input이 들어가는 DATA CLOUD라고 할 때, 뉴런이 DATA CLOUD안에 있으면 ReLU 함수가 반응하는 active ReLU이고, DATA CLOUD 외부에 있으면 ReLU함수가 activation되지 않고 죽어버리는 dead ReLU이다. 이렇게 Dead ReLU가 되는 경우는 크게 2경우이다. 첫번째 경우는 <strong>초기값이 잘못 설정된 경우</strong>로 dead ReLU 구역에서 시작해서 어떤 activation도 일어나지 않는 경우이다. 두번째 경우는 <strong>학습 시에 learning rate가 지나치게 큰 경우</strong>로 뉴런의 값이 dead ReLU존으로 나가버려서 활성화 되지 못하는 경우가 생긴다. 이것을 방지하기 위해 0.01 정도의 아주 작은 값으로 초기화하는 방법을 사용하기도 한다.</p>

<h3 id="4-leaky-relu">4. Leaky ReLU</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/128800379-e3bc300e-d428-4286-9f20-57d47906af43.png" alt="image" /></p>

<p>2013년, 2015년에 나온 방법으로 ReLU를 변형한 모양이다. ReLU와 달리 X&lt;0일때 기울기를 가지고 있기 때문에 음수에서 gradient가 죽는 saturated가 발생하지 않는다.</p>

<p>또한 alpha를 넣어서 parametric Rectifier된 PReLU가 있다. 이것은 x&lt;0일때 Leaky ReLU의 0.01x 부분의 계수를 alpha러 두고 alpha 자체를 학습해서 사용하는 것이다. Flexibility가 더 높지만 완전히 이 방법이 더 좋다는 검증이 아직 있는 것은 아니다</p>

<h3 id="5-elu">5. ELU</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/128800405-9e3de8c9-d2ff-4016-ad38-95b29eb32a12.png" alt="image" /></p>

<p>ReLU와 LeakyReLU의 중간에 있는 모양이다. 그러나 exponential을 계산하는 비용이 많이 든다는 단점이 존재한다.</p>

<h3 id="6-maxout-neuron">6. Maxout Neuron</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/128800432-57f00a14-dc74-43dd-a8fe-5a3cbc8198fc.png" alt="image" /></p>

<p>유명한 굿펠로우가 만든 함수로 ReLU와 Leaky ReLU의 장점을 가져왔다. 그러나 2개의 파라미터 값을 가진다는 단점이 있다.</p>

<p><img src="C:\Users\Danah\AppData\Roaming\Typora\typora-user-images\image-20210810114541686.png" alt="image-20210810114541686" />
정리하자면 통상적으로 Learning Rate를 잘 조정해서 ReLU를 쓰는 것이 좋고, 좀 더 실험이 필요한 경우에는 변형된 ReLU를 써보는 것이 좋다. tanh()는 더 이상 사용하지 않는다. Sigmoid는 LSTM을 제외하고 사용하지 않는다.</p>

<h2 id="data-preprocessing">[Data Preprocessing]</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/128800513-aece8eca-1dbb-425a-bed5-cc48d24c1169.png" alt="image" />
머신러닝 수업에서도 데이터 전처리시 zero-centered와 normalize의 중요성에 대해 배웠을 것이다. 동일한 range로 해서 같은 영향을 주게 하는 것이다. 그러나 이미지의 경우는 이미 0~255의 특정 범위에 들어있기 때문에 normalize를 하지 않아도 되기 때문에 zero-center만 해주면 된다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128800585-cc0bd9b8-0d70-487b-bed5-256ed0fe0bb1.png" alt="image" />
정리하자면, 이미지는 zero-center만 신경써주면 된다. 데이터 전체 mean을 빼거나, 채널별로 mean을 빼서 사용한다. normalize나 PCA, whitening은 잘 사용하지 않는다.</p>

<h2 id="weight-initialization">[Weight initialization]</h2>

<p>2006년에 restricted 볼츠만 머신이 잘 도착하지 않았던 가장 큰 이유가 가중치 초기화가 잘못되어있었던 점이었다고 한다. 그만큼 가중치 초기화는 매우 중요하다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128967744-cce84f6c-4c31-4b17-98cb-e9380e97e39e.png" alt="image" /></p>

<p>만약 모든 초기 가중치를 0으로 한다면 어떻게 될까?</p>

<p>모두 동일한 연산을 수행하게 되고 백프로퍼게이션도 동일하게 수행하니까 가중치가 업데이트 되지 않는다. 그래서 가중치를 초기화 하는 가장 기본적인 아이디어는 <strong>작은 랜덤 숫자</strong>로 시작하는 것이다!</p>

<h3 id="first-idea">first idea</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/128968664-0cb686d8-2ac3-4ce3-897d-d95f50a5616b.png" alt="image" /></p>

<p>0의 평균과 0.01의 표준편차를 가진 가우시안 분포에서 작은 랜덤 값으로 초기값을 두는 것이다. 그러나 이 경우는 네트워크가 커지면 문제가 생긴다. 10개의 레이어에 500개씩의 노드를 가지고 tanh의 activation 함수를 사용해서 시뮬레이션을 해본다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128969306-a376cc96-4d50-4b27-89fe-e80341de7269.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/128969501-8a949391-a5b2-4cab-bdbc-cd10d44d822f.png" alt="image" /></p>

<p>tanh 함수를 사용하였기 때문에 평균은 0으로 수렴하는데, 표준편차가 0으로 수렴한다. 즉 점점 collapse되어서 activations이 0으로 간다는 것이다. 이때 backward pass를 생각하면,
$$
dW_{1} = X*dW_{2}
$$
 x가 매우 작아지기 때문에 gradient가 점점 작아지는 즉 vanishing gradient가 발생한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128969574-3726cce3-1020-4793-a3b5-edb3bcc5651c.png" alt="image" />
만약 0.01이 아니라 1을 곱하면 어떻게 될까? 오버 슈팅이 되면서-1로 1로 saturated된다. 실제로 학습시에 Loss가 어느 순간부터 전혀 변하지 않는 경우가 초기 가중치값을 처음에 크게 잡은 경우가 있기 때문에 주의가 필요하다.</p>

<h3 id="xavier-initialization">Xavier initialization</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/128969627-1a4b9b57-ceb9-4c0b-8dcc-857aa4be5b13.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/128969674-0e90ea6d-e64d-457c-8abc-008c475cdef9.png" alt="image" /></p>

<p>가장 합리적인 초기화 방법으로 input의 개수가 많으면 가중치 값이 적어지고, input의 개수가 적으면 가중치 값이 커지는 방법이다. 가중치가 Saturated가 되지 않고 잘 되는 것을 위의 시뮬레이션으로 확인할 수 있다. (그러나 이것은 ReLU를 사용하면 잘 안된다는 단점이 있다.)</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128969751-b5a50ffb-19ba-4597-b3f1-c91c063093a0.png" alt="image" /></p>

<p>이것을 보완하기 위해 2015년에 제안된 방법으로 input 개수를 2로 나눠주는 방법이 제안되었다.</p>

<h2 id="batch-normalization">[Batch Normalization]</h2>

<p>그런데 가중치 초기화에 너무 의존하지 않아도 되는 방법이 개발되었다. 그것이 바로 2015년에 제안된 Batch Normalization(배치 정규화)이다. 우리가 지금까지 가중치 초기화를 신중하게 하거나 activation function을 바꾸는 방법을 생각했는데, 이것은 그런 간접적인 방법이 아니라 안정적인 학습이 되도록 제시한 방법이다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128968726-45581115-7bc3-4a72-9b48-036febf76a2c.png" alt="image" /></p>

<p>학습에서 불안정화가 일어나는 이유가 입력값의 covariance 분포가 달라지는것 때문이다. 그러므로 각 레이어를 거칠 때 마다 노말라이제이션을 해주자는 방법이다!</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128975436-c80ce257-7399-4da0-a907-011085d7f23a.png" alt="image" /></p>

<p>미니 배치인 N을 뽑아서 (D는 feature) 그 배치에 대해서 노말라이즈를 한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128975598-7add9161-669d-46c4-af16-cf6e50e82c3f.png" alt="image" /></p>

<p>이것은 FC와 activation function이전에 배치된다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128975783-be79fa94-0e56-4b32-834a-02890994e5c7.png" alt="image" /></p>

<p>Batch Normalizaton은 결과적으로 learning rate가 다소 큰 값을 가지더라도 그것을 허용해서 빠른 학습이 가능하다. 그리고 가장 핵심은 초기화에 의존하지 않아도 된다는 것이다. 그리고 이 자체가 정규화 효과(Regularization)가 있어서 드랍 아웃을 사용하지 않아도 된다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128976057-747b6b35-46a3-44db-88c4-5e3316c31597.png" alt="image" /></p>

<p>그러나 주의해야할 점은 train과 test에서 배치에 평균 값이 다르다는 점이다.</p>

<h2 id="babysitting-the-learning-process">[Babysitting the Learning Process]</h2>

<p>이제 어떻게 학습을 모니터링하고 Hyperparameter를 조정하는지 전반적인 프로세스에 대해 배워보자. (강의안 61-74 참고)</p>

<ul>
  <li>step1 - <strong>Preprocess the data</strong>: zero-centered data로 만듦</li>
  <li>Step2 - <strong>Choose the architecture</strong>: layer와 node수의 결정</li>
  <li>step3 - <strong>Loss가 제대로 가고 있는지 체크</strong>: Regularization term이 없이 구한 Loss보다 Regularization term을 추가한 Loss가 올라가는 것을 확인하고 추가한다</li>
  <li>Step4 - <strong>train data 일부분을 이용해서 오버피팅 확인</strong>: Loss가 매우 작고 train accuracy가 1.00이 되는지 체크</li>
  <li>Step5 - <strong>적당한 Learning rate 구간 찾는다</strong>: 작은 Learning rate부터 시작한다. loss가 별로 바뀌지 않으면 너무 작은 learning rate이고 loss가 NaN으로 exploding하면 너무 큰 learning rate이다. 이렇게 learning rate를 줄이고 키우면서 어느 사이에 적당한 러닝레이트가 존재하겠구나 알 수 있다.
(정확한 러닝레이트는 Cross-validation을 통해 알 수 있다.)</li>
</ul>

<h2 id="hyperparameter-optimization">[Hyperparameter Optimization]</h2>

<p>이제 learning rate와 regularization을 결정하는 일이 남았다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128979835-0871f363-04f3-4930-8a2a-a325581fbd79.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/128979964-2be636c0-06cc-44e2-a325-75e33caab9b6.png" alt="image" /></p>

<p>Regularization term과 learning rate를 log space화하여 찾는 것이 연산에 유리하다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128979114-5eafcf08-dc7c-45c6-aebd-db6df1b64997.png" alt="image" /></p>

<p>그런데 grid search를 하여 찾는 것이 더 좋지 않는가?라고 생각할 수 있다. 그러나 이 때 grid search는 사용하지 않는다. random search를 통해 중요한 변수의 optimal을 찾을 수 있기 때문이다.</p>

<p>이러한 hyperparameter tuning작업은 마치 디제이가 믹싱 작업을 하는 것 처럼 정교한 예술 작업과 비슷하다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128980997-790bd0a7-2ec4-465c-b31e-b98f6104cf83.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/128981034-f369c350-1053-4fa6-92fb-023fdce03606.png" alt="image" /></p>

<p>Loss function을 모니터링하면서 regularization term, weight initialization, model capacity 등을 점검한다.</p>

<p>드디어 길었던 강의의 Summary이다!</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128976693-23b22e73-4cb9-4d0e-8762-5d60d48577cc.png" alt="image" /></p>

<p>이번 6강에서는 Neural Networks를 어떻게 train하는지에 위의 내용에 대해 배워보았다. 다음 시간에도 train하는 나머지 과정에 대해 배워볼 것이다.</p>]]></content><author><name>danahkim</name></author><category term="Deep-Learning" /><category term="CS231n" /><summary type="html"><![CDATA[[CS231n] Lecture-6-Training-Neural-Networks-part-I 6강 Video 바로가기]]></summary></entry><entry><title type="html">CS231n | 5강 - Convolutional Neural Networks</title><link href="http://localhost:4000/deep-learning/2021/08/04/cs231n-05.html" rel="alternate" type="text/html" title="CS231n | 5강 - Convolutional Neural Networks" /><published>2021-08-04T00:00:00+09:00</published><updated>2021-08-04T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning/2021/08/04/cs231n-05</id><content type="html" xml:base="http://localhost:4000/deep-learning/2021/08/04/cs231n-05.html"><![CDATA[<p>[CS231n] Lecture-5-Convolutional-Neural-Networks<br />
<a href="https://www.youtube.com/watch?v=bNb2fEVKeEo&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=5">5강 Video 바로가기</a></p>

<ul>
  <li>Contents
    <ul>
      <li>History</li>
      <li>Convolution and pooling</li>
      <li>ConvNets outside vision</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="history">History</h2>

<p>Convolutional Neural Network에 들어가기 앞서 Computer Vision에 대한 히스토리를 소개한다. 이것은 1강 Orientation에 Fei-Fei Li 교수님이 한 강의와 대다수 동일하므로 생략하도록 한다. 1강 강의 정리 참고</p>

<h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2>

<h3 id="first-without-the-brain-stuff">(First without the brain stuff)</h3>

<p>일단 Convolutional Neural Network에 대해서 수식적으로 크게 머리를 쓰지 않고 컨셉을 이해하고 시작하도록 한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127259278-d420209e-941a-4d1e-bc15-820cd0d8d2cc.png" alt="image" /></p>

<p>지난 시간에 우리는 Fully Connected Layer에 대해 배웠다. 예를 들어 32x32x3 크기의 이미지가 있다면 이것을 <strong>3072길이의 1차원 벡터로 길게 늘이고</strong>, 가중치 행렬인 W와 내적하여 각 class에 대한 score를 구한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127259739-68395762-aaac-44b7-b87d-436fcb31f50f.png" alt="image" /></p>

<p>Convolution Layer는 1개의 긴 벡터로 만들지 않고 <strong>이미지 모양 그대로 학습하여</strong> 공간 구조를 보존한다. 이것이 가장 큰 차이점이다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127771894-26b53842-3008-4a43-b2f1-c3c40ef98857.png" alt="image" /></p>

<p>이미지가 있으면 동일한 depth의 <strong>필터</strong>를 이용해서 이미지를 <em>공간적으로</em> 내적한다. (어떻게 계산하고 적용되는지 그 디테일에 대해서는 뒤에서 다룰 것이다.)</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127772058-5cea0c24-61aa-4c59-912a-d10e667613ab.png" alt="image" />
그런데 element-wise로 곱하는 것은 결국 필터(w)와 이미지(x)를 쭉 펼치고 그것을 내적하는 것과 같다. 그래서 <strong>w’x+b</strong>라는 우리가 배운 간단한 수식으로 표현할 수도 있다.</p>

<p><img src="C:\Users\Danah\AppData\Roaming\Typora\typora-user-images\image-20210801222258723.png" alt="image-20210801222258723" />
모든 공간에서 이 필터를 곱하는 계산을 해서 1개의 필터로 1개의 <strong>activation map</strong>을 만든다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/128187518-b7585a69-276e-490a-acb5-20195d11b619.png" alt="image" /></p>

<p>그리고 또 다른 필터를 추가해서 수행할 수 있는데, 6개의 필터를 사용하면 6개의 activation map을 얻을 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127773726-28a18e9e-98cf-44be-aa43-93cc6e2a2bd1.png" alt="image" />
쌓아진 activation map에 또 필터를 곱해서 계속 레이어를 쌓을 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127773932-00c94725-fc78-409e-8be1-4ea160534d88.png" alt="image" />
이렇게 계속 레이어를 스택하면 어떻게 될까?
위 예시를 보자. 하나의 그리드가 하나의 뉴런이라고 생각하면 된다. 그리고 그 뉴런이 가장 잘 반응하는 이미지를 나타냈다고 보면 된다.
처음에는 edge를 찾는 저차원수준이고, 중간에는 좀 더 복잡한 모양인데 코너 같은 것을 찾을 수 있다. 그리고 점점 반복할 수록 고차원의 형상을 나타내게 된다.
레이어를 계층적으로 스택하는 것을 이렇듯 고차원을 표현할 수 있게 된다는 것이다.
그러므로 필터의 크기나 모양, stride, 필터의 개수 같은 것을 정하는 것은 분석자의 중요한 디자인이다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127774144-726a3006-4b1e-4905-b1bb-2078710a74d8.png" alt="image" />
위처럼 Convolution, RELU, Pooling등의 연속된 레이어를 지나면서 비선형으로 바뀌고 마지막에는 fully connected layer로 리턴하면서 이미지인지 score를 보여준다</p>

<p>이제 어떻게 공간 차원에 필터가 작용하는지 예시를 보자.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127774458-ccd5af41-b6e2-403a-8500-fe3641a27125.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/127774481-1d2a1809-865a-4f6c-942a-92ce7aa6a0b6.png" alt="image" /></p>

<p>위는 3x3 필터가 stride가 1일때 어떻게 계산되는지 보여주는 예시이다. slide는 공간을 이동하는 칸 수를 의미한다. 공간을 1칸씩 slide하면서 element-wise로 값을 곱한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127774778-7462a503-20ac-46d5-be08-7094667b6b5e.png" alt="image" /></p>

<p>7x7의 이미지에서 3x3의 필터를 적용한다면 stride=1일 때 7-3+1=5로 5x5의 크기의 activation map을 갖는다.
stride=2로 하면 2칸씩 좌우로 움직이는데 따라서 (7-3)/2+1=3으로 3x3의 크기를 가지게 된다.
stride=3일때는 2.3이 되는데.. 이 때는 <strong>패딩</strong>을 이용한다!</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127774826-3c498e53-5500-40d3-8c4b-4cf32fa5885a.png" alt="image" /></p>

<p>패딩 1을 한다면 이미지 주변으로 1칸의 값을 넣는다. 위의 예시에는 패딩 값이 0이지만 0이 아니라 그 주변의 값을 mirroring해서 쓸 수도 있다. 패딩을 하면 stride=1일때 9-3+1= 7이 된다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127774873-cade6f2b-b3b7-4f6a-964f-5a89e2474ce2.png" alt="image" />
이렇듯 필터를 지나면 이미지의 크기가 작아지기 때문에, 패딩은 이미지 원래의 사이즈를 공간적으로 잃지 않고 보존하게 해주는 역할을 한다. 또한 패딩을 통해 테두리 부분의 이미지의 정보를 잃지 않게 해준다.
통상적으로 필터 사이즈로 3x3, 5x5, 7x7을 많이 쓰는데, 이 때 적당한 크기의 패딩을 사용하여 사이즈가 많이 줄지 않게 만든다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127774966-751c760e-5fa5-4c10-8fcb-8f7598464309.png" alt="image" />
연속적인 레이어를 반복하면 이미지의 크기가 빠르게 줄어들고 이것은 엣지의 인포메이션을 잃는 것이기 때문에 좋지 않다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127774988-7ce92de1-bc3d-44bd-b5e2-13071df738d1.png" alt="image" />
이때 패딩을 사용한다면 사이즈를 잃지 않은 activation map을 구할 수 있다.</p>

<p>32x32x3의 이미지에 5x5x3의 필터, stride=1을 이용할 때, 원래는 이미지가 28x28로 줄어든다. 그러나 pad=2의 패딩을 이용하면 이미지의 크기 32를 보존할 수 있다!</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127775543-a37abac8-9c81-47f0-91af-0c8ec02cb456.png" alt="image" />
parameter 또한 w에 해당하는 필터의 paramter값으로 구할 수 있는데 여기서 중요한것은 bias term을 하나 꼭 더해줘야한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127772859-34ef8d9b-6f5c-47ac-bf57-eada8321bdfb.png" alt="image" /></p>

<p>7x7x3의 이미지에 3x3x3 필터 2개가 구체적으로 어떻게 계산이 되는지 나타낸 그림이다. <a href="https://cs231n.github.io/convolutional-networks/">ConvNet notes</a>를 참고하면 자세한 설명과 예시를 볼 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127775042-bc3ff652-7b98-4d24-94fa-a2da5ab8a6bd.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/127776011-ba4e812e-2c79-4a3e-8a17-42b0785c9a5e.png" alt="image" /></p>

<p>PyTorch 프레임워크로 CNN을 구현할 수 있는데 안에 있는, 디자인은 본인이 설정할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127775075-304dc533-faf8-489a-b1f9-d40b10b123a7.png" alt="image" />
Stride의 크기는 결국 activation map을 작게해서 다운사이즈 시키는데 parameter가 줄어들고 오버피팅을 막는다. 이와 비슷하게 pooling layer은 이미지를 downsampling해서 공간적으로 줄인다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127775089-0ff6345b-d21c-4ac8-8e82-15d0bc585272.png" alt="image" />Max-pooling을 가장 많이 쓰는데 그 공간에서 가장 큰 값만 가져오는 것이다. 그 공간을 가장 잘 나타내는 값 1개로 줄이는 거라고 직관적으로 이해하면 된다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127775104-9848263a-0fd9-4cc3-8daf-461878da90a0.png" alt="image" />
풀링 레이어로 2x2, 3x3을 가장 많이 쓴다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127775120-bfe98d6f-f8d6-4691-a617-cbf379dadb0e.png" alt="image" />
정리하면 CNN은 Convolution layer와 Pooling layer, Full-connected layer를 쌓은 구조이다.</p>

<p>작은 필터로 깊게 쌓는 구조와 Pooling layer와 Full-connected layer 없이 Convolution layer로만 쌓는 것이 최근 트렌드이다. 예를 들어, Conv, Relu 후에 pooling을 하는 레이어를 아주 여러 번 반복하고, 마지막에 Full-connected layer를 생략하거나 2번 정도로 한 후에 softmax를 거쳐 score가 나오는 아키텍쳐가 전형적이다.</p>

<p>그러나 최근에는 ResNet이나 GoogLeNet은 이러한 패러다임에 도전하여 좋은 성능을 냈다!</p>

<p>(위는 2017년 기준이며 현재는 이보다 더 좋은 아키텍쳐가 더 많이 공개되었다. 주요 CNN 아키텍쳐를 설명해놓은 <a href="https://hoya012.github.io/blog/deeplearning-classification-guidebook-4/">블로그 글</a>을 참고하면 좋을 것 같다.)</p>]]></content><author><name>danahkim</name></author><category term="Deep-Learning" /><category term="CS231n" /><summary type="html"><![CDATA[[CS231n] Lecture-5-Convolutional-Neural-Networks 5강 Video 바로가기]]></summary></entry><entry><title type="html">CS231n | 4강 - Introduction to Neural Networks</title><link href="http://localhost:4000/deep-learning/2021/07/28/cs231n-04.html" rel="alternate" type="text/html" title="CS231n | 4강 - Introduction to Neural Networks" /><published>2021-07-28T00:00:00+09:00</published><updated>2021-07-28T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning/2021/07/28/cs231n-04</id><content type="html" xml:base="http://localhost:4000/deep-learning/2021/07/28/cs231n-04.html"><![CDATA[<p>[CS231n] Lecture-4-Introduction-to-Neural-Networks<br />
<a href="https://www.youtube.com/watch?v=d14TUNcbn1k&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=4">4강 Video 바로가기</a></p>

<ul>
  <li>Contents
    <ul>
      <li>Backpropagation</li>
      <li>Multi-layer Perceptrons</li>
      <li>The neural viewpoint</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="review">Review</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/125886711-42556497-bc4f-4a31-80bf-7c1c4126071a.png" alt="image" /></p>

<p>지난 시간에 우리는 Loss function으로 <em>‘우리가 만든 스코어가 얼마나 잘 만들어졌는지’</em> 정량화하는 것에 대해 배웠다. 우리는 간단한 모델을 선호하기 때문에 더 나은 모델의 일반화를 위해 Regularization을 추가한다.</p>

<p>그리고 우리는 이 Loss를 낮추기 위해 가장 가파른 내리막 방향으로 내려가는데 이것을 gradient descent method라고 한다. w의 gradient를 찾아서 Loss 풍경의 가장 낮은 지점으로 단계적으로 내려간다.</p>

<p>또한 이 gradient를 계산하는 방법에 대해 배웠다. 오늘은 Computational Graphs를 사용해서 Loss fucntion의 gradient를 계산하는 방법에 대해 배울 것이다.</p>

<hr />

<h2 id="backpropagation">Backpropagation</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/125887845-74520c0b-5298-4a07-8482-d3f2c4ea3c0b.png" alt="image" /></p>

<p>이 Computational Graphs는 반복적으로 chain rule을 사용해서 gradient를 구하는 Backpropagation 방법에 유용한데, 특히 아주 복잡한 함수에 적용하는데에 매우 유용하다. 예를들어 아주 많은 레이어를 거치는 Convolutional Network에 적용하는데에 유용하다.</p>

<p>chain rule을 사용해서 gradient를 구하는데 기본적으로 <strong>[local gradient] X [upstream gradient]</strong> 의 방법으로 gradient를 구한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/126234709-170d0ec8-bcce-4c40-a24e-722e06c64b00.png" alt="image" /></p>

<p>가장 마지막 graph node부터 구하는데, gradient를 구하는 예시를 들겠다. 마지막 gradient는 당연히 1이다.
$$
f(x)=\frac{1}{x}
$$
일때
$$
\frac{df(x)}{dx} = - \frac{1}{x^2}
$$
이므로 
$$
\frac{df(x)}{dx} \frac{dx}{dx} = [LocalGradient][UpstreamGradient] <br />
= (\frac{-1}{1.37^2})(1)=-0.53
$$
 이다.</p>

<p>그 다음 node의 gradient를 구하면
$$
g(x)=x+1
$$</p>

<p>$$
\frac {dg(x)}{dx} = 1
$$</p>

<p>이므로 chain rule을 이용하면
$$
\frac {df(g(x))}{dx} = \frac{dg(x)}{dx} \frac {df(x)}{dg(x)} <br />
= [LocalGradient][UpstreamGradient] <br />
= (1)(-0.53) <br />
= -0.53
$$
이다.</p>

<p>이러한 방법으로 가장 끝에 있는 node(w)에 관한 gradient를 구할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/126234856-6bd41b3e-7f1d-46d3-80f6-5eb3d90ec27c.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/126234888-588b05e2-c482-409e-86f8-e21c83275572.png" alt="image" /></p>

<p>하나하나 다 쪼개서 가장 간단하게 그래프를 나타내고 gradient를 구할 수도 있지만 표현할 수 있다면 묶어서 하나의 node로 나타내도 된다. 예를들어 sigmoid 함수는 특별히 아래처럼 미분이 되기 때문에 한번에 <strong>sigmoid gate</strong>로 표현해서 구할 수 있다.</p>

<p>$$
\frac {d \sigma(x)} {dx} = (1 - \sigma(x)) \sigma(x)
$$
이를 이용해서</p>

<p>$$
\sigma(x)=0.73
$$</p>

<p>의 값을 넣고 구하면
$$
(1-0.73) * (0.73) = 0.2
$$
으로 gradient를 구할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/126235688-b5f2b82b-e4d8-4209-9694-842c901dcffb.png" alt="image" /></p>

<p>덧셈, 최대값, 곱셈 연산이 가지는 패턴은 위와 같다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/126236356-8e61730a-29b8-40f5-861b-46b18eafabe5.png" alt="image" /></p>

<p>$$
\frac {df} {dx} = \sum_{i} \frac {df} {dq_{i}} \frac {dq_{i}} {dx}
$$
특히 덧셈 연산은 x의 각각의 local gradient를 더한 값으로 표현할 수 있다.  이렇게 함수에서 각 변수에 대해서 gradient를 계산하는 방법인 backpropagation에 대해 살펴보았다.</p>

<h2 id="backpropagation---vectorized-gradients">Backpropagation - vectorized gradients</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/126237011-537e0ac0-3b4f-46e4-91e0-69c23cc6e1db.png" alt="image" /></p>

<p>이번에는 vector의 gradient를 살펴볼 것이다. 이때의 유일한 차이점은 <em>gradient가 Jacobian matrix</em>라는 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127001319-580a9843-6dde-4b5d-b766-9aad241e1646.png" alt="image" />
먼저 computational graph를 그리고 시작한다</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127002183-ab73fff6-6738-4e2e-81f1-4c8050863b0c.png" alt="image" /></p>

<p>먼저 마지막 gradient는 당연히 1이다.
그리고 
$$
f(q)=q_1^2+q_2^2
$$
이므로
$$
\frac{df}{dq_{i}} = 2q_{i}
$$
의 벡터이다. 이를 이용해서 W의 gradient를 구하면</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127002874-ffbcef65-862d-49fb-96e0-adc6810249d9.png" alt="image" />
$$
\frac{dq}{dw}=x <br />
\frac{df}{dw}=\frac{df}{dq}<em>\frac{dq}{dw}=2q_{i}</em>x_{j}
$$
<img src="https://user-images.githubusercontent.com/62828866/127091519-5ab80400-f676-47af-815e-c3ef0df0ddbd.png" alt="image" /></p>

<p>예를들어 많은 딥 러닝 프레임워크를 보면 그 안에 backpropagation을 구현해놨다. Caffe의 코드를 보면 레이어가 computational node로 되어있고 안에는 하나씩 forward backward 연산을 해서 쌓는것이다. 시그모이드 레이어를 보면 gradient를 구하는 부분이 위처럼 생겼다</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127091636-7a2136c1-474c-47bd-a195-96cfe9cd3e80.png" alt="image" /></p>

<p>정리하면 위와 같다. 그 중 핵심은 backpropagation을 적용한 forward, backwar API를 사용해서 레이어의 gradient를 구한다!</p>

<hr />

<h2 id="multi-layer-perceptrons">Multi-layer Perceptrons</h2>

<p>신경망은 사실 오래된 기술이다. 1980~1990년대에 연구되었다. 그런데 현대에 여러가지 문제를 해결하기 위해 쓰이는 CNN, LSTM 등의 기법들을 위해하기 위해서는 이 신경망을 먼저 이해해야한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127185959-1f04b066-2414-44a6-8767-e839e41518be.png" alt="image" /></p>

<p>1번째 linear function에 또 linear function을 쌓아서 non linear하게 만든다. 이러한 계층적 다중 레이어 아키텍쳐의 기본이다. DNN(Deep Neural Network)의 뜻은 이러한 Neural network를 깊게 쌓았다는 원초적 의미이다.</p>

<hr />

<h2 id="the-neural-viewpoint">The neural viewpoint</h2>

<p><img src="https://user-images.githubusercontent.com/62828866/127186697-e794c9f0-de13-404b-a28b-51c722ae85c9.png" alt="image" /></p>

<p>인체의 뉴런과 비슷하다. x0이라는 데이터가 들어와서 가중치w0과 시냅스 반응이 나고 이것을 cell body에서 선형 합연산을해서 activation function에 의해 비선형으로 내보낸다. 그리고 이를 다음 뉴런으로 전달해준다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187235-91d68d5b-6856-4ca9-b544-46f17e034da3.png" alt="image" /></p>

<p>그러나 인공신경망이 인체의 뉴런과 아예 동일하다고 볼 수 없으므로 주의해야한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187331-95ea3eca-60b7-46e8-bb27-bc6af229045a.png" alt="image" /></p>

<p>activation function이 여러개 존재한다. 전통적으로 0~1사이로 리턴해주는 sigmoid가 가장 많이 사용됐으며, 2012년에 등장한 <strong>ReLU</strong>가 최근에 가장 많이 쓰인다. Leaky ReLU나 ELU등 좀 더 수정된 function도 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187516-b977a277-7783-476a-9de8-c07cb57de9ee.png" alt="image" /></p>

<p>fully-connected layer는 벡터로 표현이 가능하고 행렬의 곱인 W*x로 계산이 효율적이다. 따라서 fully-connected layer로 많이 쓰고 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187718-98fb296c-be91-41b6-b8d4-83a33fdd46da.png" alt="image" /></p>

<p>또한 fully-connected layer로 쓰면 hidden layer 하나를 한줄로 간단하게 f() function을 통해 표현이 가능하다는 장점이 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/127187823-1851e3c7-a274-4e91-a172-481d944e93a4.png" alt="image" /></p>

<p>여기까지가 Neural Network에 대한 기본 개괄이었다. 다음 시간에는 <strong>드디어</strong> Convolutional Neural Network에 대해서 배울 것이다.</p>]]></content><author><name>danahkim</name></author><category term="Deep-Learning" /><category term="CS231n" /><summary type="html"><![CDATA[[CS231n] Lecture-4-Introduction-to-Neural-Networks 4강 Video 바로가기]]></summary></entry><entry><title type="html">CS231n | 3강 - Loss Functions and Optimization</title><link href="http://localhost:4000/deep-learning/2021/07/22/cs231n-03.html" rel="alternate" type="text/html" title="CS231n | 3강 - Loss Functions and Optimization" /><published>2021-07-22T00:00:00+09:00</published><updated>2021-07-22T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning/2021/07/22/cs231n-03</id><content type="html" xml:base="http://localhost:4000/deep-learning/2021/07/22/cs231n-03.html"><![CDATA[<p>[CS231n] Lecture-3-Loss-Functions-and-Optimization<br />
<a href="https://www.youtube.com/watch?v=h7iBpEHGVNc&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=3">3강 Video 바로가기</a></p>

<ul>
  <li>Contents
    <ul>
      <li>Linear classification II</li>
      <li>Higher-level representations, image features</li>
      <li>Optimization, stochastic gradient descent</li>
    </ul>
  </li>
</ul>

<hr />

<p>지난 강의(Lecture2)에서 이미지 인신의 어려움에 대해 이야기 했다. 사람의 시각 인식과 컴퓨터의 인식에는 Semantic Gap이 존재했고 이를 극복하기 위해서는 여러 Challenge가 존재한다.
또한 이미지 인식을 위한 기법으로 배운 KNN은 Data중심의 접근 방법으로 Hyperparameter인 W가 존재하고 이는 train 과정에서 세팅되어 test데이터를 predict한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125530697-3f5f2c5d-d4f9-41c8-a44e-68b00f8a8e09.png" alt="image" /></p>

<p>그렇다면 어떻게 W를 결정하느냐? W에 대해 좋다 나쁘다를 정량화하는 것이 바로 <strong>Loss Function</strong>이다. 그리고 이 최선의 W를 찾아가는 방법이 <strong>Optimization</strong>이다. 오늘은 이 2가지 방법에 대해 배울 것이다.</p>

<h2 id="linear-classification-ii">Linear classification II</h2>

<h3 id="loss-function">Loss Function</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/125530956-f38d74f7-4431-4235-83c8-a3f5c51e3bf3.png" alt="image" /></p>

<p>Loss function은 현재 분류기가 얼마나 좋은지 말해준다. dataset에 대한 Loss fucntion은 $$f(x_{i},W)$$와 $$y_{i}$$의 차이에 대한 평균이다. 이것은 이미지 뿐만 아니라 매우 general하게 이용된다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125530996-52509d3f-ca05-4960-9d24-53e18ad40ecf.png" alt="image" /></p>

<p>Multiclass SVM Loss가 말하는 것은 true score이 다른 score보다 훨씬 높다면 좋다는 것을 의미한다. 이 Loss는 safety margin보다 true score가 다른 score보다 높다면 loss가 올라가게 된다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125532945-9fc5646f-c547-4223-ab5c-bf888f375eb9.png" alt="image" /></p>

<p>실제 class는 고양이인데 자동차 score(5.1)가 고양이 score(3.2)보다 높기 때문에 손실을 입는걸 볼 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125533260-814e2f28-5cc7-4648-83d9-af1e1688df6c.png" alt="image" /></p>

<p>모든 score에 대한 loss에 대해 평균을 구하면 5.27이 이 함수에 대한 Loss가 된다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125534322-b919b563-2ca9-4df9-ba15-8900b522dcae.png" alt="image" /></p>

<p>$ L=0 $이 되게 하는 W는 unique하지 않다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125534360-90b22274-24a2-4c6f-886f-4f29148a5a3e.png" alt="image" /></p>

<p>Loss에 모델이 simple하게 해주는 Regularization term을 추가해서 train data에 overfitting되는 것을 막을 수 있다. 모델의 복잡성에 페널티를 부여하는 것이다. hyperparameter $ \lambda $를 조절해서 tuning한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125590492-04a91e60-0b73-48ee-a454-cdcf27132d23.png" alt="image" /></p>

<p>L2 Regularization은 w의 복잡성을 줄여준다.
실제로 L2 Regularization은 weight가 분산된 w2를 선호한다.
L1 Regularization은 그러나 변수가 shrinke하는 w1를 선호한다.</p>

<h3 id="softmax-classifiermultinomial-logistic-regression">Softmax Classifier(Multinomial Logistic Regression)</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/125606668-d657092b-da30-477a-b18a-de9149792f43.png" alt="image" /></p>

<p>0~1의 값을 가지도록 exponential을 취해주고 그 값들의 합이 1이 되도록 확률 분포를 따르도록 만들어준다. 이때 Loss function은 맞는 분류의 확률값이 클수록 loss가 작아지도록 $ L = -log P $ 를 최소화한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125609504-d1418f3c-bf6f-45f7-bd34-cef87ef43df3.png" alt="image" /></p>

<p>hinge loss(SVM)과 cross-entropy loss(Softmax)의 비교이다. SVM은 맞게 한번 분류되면 끝이지만 Softmax는 스코어를 더더 좋게 개선하기 위해 노력한다는 차이가 있다. 물론 딥러닝에서 두 loss를 다른거 쓰더라도 크게 결과가 달라지지는 않는다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125610974-2913985b-9ce7-4733-8e96-4b90f158118a.png" alt="image" /></p>

<p>그렇다면 최적의 W를 찾는 방법은? 바로 최적화(Optimazation)이다.</p>

<hr />

<h2 id="optimazation">Optimazation</h2>

<p>한번에 Loss의 minima가 되게하는 명시적인 해답을 찾으면 좋지만, 딥러닝에서는 그렇게 한번에 찾는게 불가능하다. 반복하여 점차 나아지게 하는 방법을 사용한다.</p>

<p><strong>Random Search</strong>
<img src="https://user-images.githubusercontent.com/62828866/125628941-37853001-0ae0-4a8c-bf69-88aa9999552a.png" alt="image" /></p>

<p>15.5%의 정확도이다. 이건 좋은 방법이 아니다.</p>

<p><strong>Follow the slope</strong>
<img src="https://user-images.githubusercontent.com/62828866/125629029-775eb906-93db-4a40-8433-b568c812e69a.png" alt="image" /></p>

<p>현재 서 있는 위치에서 경사를 느끼고 가장 가파른 곳으로 내려가는 방법이다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125629528-f2cc1d1f-6522-43a7-b7c6-9bf8d0e9442e.png" alt="image" /></p>

<p>negative gradient 방향으로가 가장 가파르게 내려간다.</p>

<p><em>Numerical gradient</em>
<img src="https://user-images.githubusercontent.com/62828866/125630952-c2d0a1a9-3050-41ec-801f-55e86d8979ac.png" alt="image" /></p>

<p>gradient를 모두 하나씩 다 구하는 건 계산도 오래 걸릴 뿐더러 효과적인 계산 방법이 아니다. 이러한 Numerical gradient는 디버깅에서 사용하기 좋다.</p>

<p><em>Analytic gradient</em>
<img src="https://user-images.githubusercontent.com/62828866/125635509-3f6cd17b-d109-4462-b134-4eb4d5c9279e.png" alt="image" /></p>

<p>그러나 위처럼 계산할 필요가 없이 <em>미적분학의 요술 망치</em> (the magical hammer of calculus)를 사용하면, 훨씬 더 정확하고 빠르게 gradient를 구할 수 있다. 이 Analytic gradient는 실제적으로 사용되는 방법이다.</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p><img src="https://user-images.githubusercontent.com/62828866/125637842-91930eed-9974-441a-b5d0-0ba1d91a103d.png" alt="image" /></p>

<p>weight를 계속 업데이트해주는데 gradient의 음의 방향으로 step size만큼 움직인다.
step size(learning rate)가 가장 중요한 hyperparameter 중 하나인데, 학습시에 가장 먼저 세팅하려고 한다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125884829-ad796bb9-842b-4686-afb3-cdff16fb5d23.png" alt="image" /></p>

<p>Gradient descent는 각 loss의 선형합이다 따라서 저번에 본 예시처럼 이미지 파일에서 사용한다면 엄청나게 큰 N을 다 계산해야한다 그런데 이건 너무 느리고 오래걸리고 비효율적이다.</p>

<p>따라서 Stochasitic gradient descent를 사용한다.
모든 train set(N)을 사용해서 구하지 않고 통상적으로 2의 제곱수인 32/64/128 크기의 미니배치를 사용해서 Loss를 근사하는 방법이다. 이 외에도 여러 fancy한 방법들이 많이 존재하고 있다.</p>

<p>그 외의 이미지 인식 방법은 Color Histogram으로 이미지를 인식하는 법, Histogram of Oriented Gradients(HoG)으로 Dominant edge orientation으로 인하는 방법, Bag of Words방법으로 NLP처럼 이미지 조각 사전을 만들어서 인식하는 방법들이 있다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125884945-c4e2e7fc-f86c-418d-b6bd-96d3dd343641.png" alt="image" /></p>

<p>그렇다면 이러한 Image feature를 이용하는 방법과 CNN은 어떤 차이가 있을까?
Feature Extraction을 통한 전통적인 방식은 train을 하는 동안에도 Feature block이 fix이다.
이에 반해 CNN은 다른 것은 매우 비슷하지만 <strong>사전에 feature를 정해놓지 않고 data에서 직접 feature를 학습하는 방법이다.</strong></p>

<p>다음시간에는 Neural Network에 대해 다루게 된다!</p>]]></content><author><name>danahkim</name></author><category term="Deep-Learning" /><category term="CS231n" /><summary type="html"><![CDATA[[CS231n] Lecture-3-Loss-Functions-and-Optimization 3강 Video 바로가기]]></summary></entry><entry><title type="html">CS231n | 2강 - Image Classification</title><link href="http://localhost:4000/deep-learning/2021/07/16/cs231n-02.html" rel="alternate" type="text/html" title="CS231n | 2강 - Image Classification" /><published>2021-07-16T00:00:00+09:00</published><updated>2021-07-16T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning/2021/07/16/cs231n-02</id><content type="html" xml:base="http://localhost:4000/deep-learning/2021/07/16/cs231n-02.html"><![CDATA[<p>[CS231n] Lecture-2-Image-Classification<br />
<a href="https://www.youtube.com/watch?v=OoUX-nOEjG0&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=2">2강 Video 바로가기</a></p>

<ul>
  <li>Contents
    <ul>
      <li>The data-driven approach</li>
      <li>K-nearest neighbor</li>
      <li>Linear classification I</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="image-classification-the-data-driven-approach">Image Classification: The data-driven approach</h2>

<p>지난번에 말했던 이미지 분류에 대해서 말해보려 한다. dog, cat, truck, plane으로 fixed categories labeled 작업의 이미지 분류에 대해서이다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125463362-e3573d2b-5966-4001-9988-1c80e68d604f.png" alt="image" /></p>

<p>이미지 인식은 사람에게는 쉬운 일이지만 컴퓨터에게는 굉장히 어려운 일이다. 컴퓨터는 단순히 RGB의0 3개의 layer에 0~255 사이의 숫자로 인식하기 때문에 사람이 인식하는 것과는 의미적 차이(Semantic gap)가 존재한다. Viewpoint variation(각도), Illumination(조명), deformation(자세), occlusion(가려짐), Background Clutter(배경과의 비슷함), Intraclass variation(종의 다양함) 등의 문제를 사람은 쉽게 구분하지만 컴퓨터에게는 어렵다.</p>

<p>이미지 분류는 어떻게 할 것인가?</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125211719-5c5d3d00-e2e3-11eb-863c-d29d9d96a7a1.png" alt="image" /></p>

<p>만약 ‘고양이를 분류하는 알고리즘’을 만든다면, 예전처럼 이미지의 Edge를 찾고, 그 코너를 찾고, 고양이라면 뾰족한 귀가 두 개 있다는 이러한 <em>명시적인 규칙</em>을 찾아서 알고리즘을 만들 수 있다. 그러나 이것은 해보지 않아도 당연히 현실에 잘 작동하지 않을 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125211733-6a12c280-e2e3-11eb-83fc-c372ddc47cd2.png" alt="image" /></p>

<p><strong>Data-Driven Approach</strong>는 많고 다양한 class의 이미지 데이터를 label과 함께 학습해서 각각의 class의 시각적 모습을 학습하여 알고리즘을 만드는 것이다. 따라서 명시적인 규칙(혹은 hard-code algorithm)이 있는게 아니라 주어진 training dataset을 따로 학습하는 train과 이미지를 예측하는 predict의 알고리즘으로 나누어서 만든다.</p>

<hr />

<h2 id="k-nearest-neighbor-classifier">K-Nearest Neighbor classifier</h2>

<p>그 Data-driven approach의 한 기법이 K-Nearest Neighbor Classifier이다. <em>주어진 데이터를 학습해서</em> 분류하는 모형을 만든다.</p>

<pre><code class="language-Python">import numpy as np

class NearestNeighbor(object):
  def __init__(self):
    pass

  def train(self, X, y):
    """ X is N x D where each row is an example. Y is 1-dimension of size N """
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """ X is N x D where each row is an example we wish to predict label for """
    num_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in range(num_test):
      # find the nearest training image to the i'th test image
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example

    return Ypred
</code></pre>

<p><img src="https://user-images.githubusercontent.com/62828866/125298318-fd3f0d00-e362-11eb-83cc-4cdc055d808e.png" alt="image" /></p>

<p>위처럼 K와 distance metric(L1, L2)에 따라 training이 달라진다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125298480-22338000-e363-11eb-84d1-b5633e4eb0e6.png" alt="image" /></p>

<p><img src="https://user-images.githubusercontent.com/62828866/125465042-ba9d9df0-f558-40d3-ac52-7217b5f1b2d5.png" alt="image" /></p>

<p>여기서 K와 distance metric은 <strong>Hyperparameter</strong>라고 한다. data에 존재하는 parameter가 아니라 data를 떠나 문제 의존적인 결정사항이다. 이때 Hyperparameter는 data를 train, validation, test로 나눈 다음에 train data로 훈련한 hyperparameter를 validation data으로 평가해서 고르고, 그것을 test data로 평가한다. K-fold validation을 사용할 수 도 있다. (그러나 K-fold validation은 deep learning에서는 자주 사용하지 않는다.)</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125300021-9de1fc80-e364-11eb-9ade-8c9037c39c22.png" alt="image" /></p>

<p>그러나 knn은 이미지 분류에 잘 안쓴다. 왜냐하면 test time이 오래걸리고, distance metric이 informative하지 않고, 또한 class가 늘어날 수록 차원의 저주가 생기기 때문이다.</p>

<hr />

<h2 id="linear-classification">Linear classification</h2>

<p>Linear classification은 아주 단순하지만 CNN에 매우 중요하다!</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125465416-8358b39e-d545-460d-beec-7b04b5ce9938.png" alt="image" /></p>

<p>Linear classifier을 레고 불록에 비유할 수 있다. 이 레고 블록 하나하나가 linear classifier인 것이고 이것이 모여서 큰 Neural Network가 되는 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125465672-f4975e79-53a2-4193-8517-206b691ad911.png" alt="image" /></p>

<p>선형분류는 가장 단순한 모수적 모델(Parametirc model)이다. Knn은 모수가 필요하지 않았다. 그러나 선형분류는 train을 통해 W를 가지면 test에 W만으로 피팅할 수 있다. b는 bias term인데 만약 unbalanced data라서 만약 cat이 dog보다 많다면 cat의 카테고리에 해당하는 bias term이 클 것이다</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125470065-d7bfe460-787e-4bd2-84f6-e1e2a9cf3fb2.png" alt="image" /></p>

<p>이미지 class에 대한 학습된 Weights를 보면(train 후 역으로 구한 이미지) 그 class의 이미지와 비슷하게 생겼다. 그러나 선형이기 때문에 좌우 대칭이다. (horse를 보면 잘 드러난다.)</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125470142-f5460cc7-1818-4613-9e90-7fb0b8585cd2.png" alt="image" /></p>

<p>Linear classifier의 문제점으로, Odd even 구분, Multimodal case, 그리고 흩어진 공간에 존재하는 경우는 선형으로 구분하지 못한다는 문제가 있다. 그럼에도 불구하고 Linear Classifier는 아주 간단하고 이해하고 해석하기 쉽다는 장점이 파워풀하다.</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125471038-83ba0db8-dc58-4e6f-9737-16381f3e1f84.png" alt="image" /></p>

<p>지금까지 linear score fucntion을 알아보았다. 그렇다면 과연 학습한 Weights가 좋은지 나쁜지는 어떻게 구분할 수 있을까?</p>

<p><img src="https://user-images.githubusercontent.com/62828866/125470452-d003b173-9d6f-4a4b-80a6-e7c3bf9c1fd4.png" alt="image" /></p>

<p>다음 시간에는 좋은 Weight란 어떻게 정량화하는가에 대한 Loss Function, 그리고 그 loss를 줄여주는 Optimazation, 함수 형태를 조절하는 CovNet에 대해서 배워볼 것이다.</p>]]></content><author><name>danahkim</name></author><category term="Deep-Learning" /><category term="CS231n" /><summary type="html"><![CDATA[[CS231n] Lecture-2-Image-Classification 2강 Video 바로가기]]></summary></entry></feed>